{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/clarence/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/clarence/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "from torchvision.transforms import transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=4096)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader.from_folder(r\"./datasets/model1_data\")\n",
    "\n",
    "folder_path = './datasets/model1_data/Moving'\n",
    "image_files = [file for file in os.listdir(folder_path)]\n",
    "augmented_images = []  # List to store augmented images\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "  layers.RandomRotation(0.26),\n",
    "])\n",
    "cnt = 0\n",
    "# Apply data augmentation to each image and store the augmented images\n",
    "for image_file in image_files:\n",
    "    if cnt == 709:\n",
    "        break\n",
    "    image_path = os.path.join(folder_path, image_file)\n",
    "    \n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.io.decode_jpeg(image, channels=3)  # Specify the number of channels (3 for RGB)\n",
    "    image = tf.expand_dims(image, axis=0)\n",
    "    # Apply data augmentation\n",
    "    augmented_image = data_augmentation(image)\n",
    "    \n",
    "    augmented_images.append(augmented_image)\n",
    "    cnt+=1\n",
    "\n",
    "# Specify the path of the folder where you want to save the augmented images\n",
    "save_folder = './datasets/model1_data/Moving'\n",
    "\n",
    "# Iterate through augmented_images and save them to the specified folder\n",
    "for i, augmented_image in enumerate(augmented_images):\n",
    "    # Generate a unique file name for the saved image\n",
    "    image_name = f'augmented_image_{i+736}.jpg'\n",
    "    \n",
    "    # Save the augmented image to the specified folder\n",
    "    save_path = os.path.join(save_folder, image_name)\n",
    "    tf.keras.preprocessing.image.save_img(save_path, augmented_image[0])\n",
    "\n",
    "train_data, test_data = data.split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hub_keras_layer_v1v2 (HubKe  (None, 1280)             3413024   \n",
      " rasLayerV1V2)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,415,586\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 13:46:46.920180: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82/82 [==============================] - 4s 28ms/step - loss: 0.4467 - accuracy: 0.8304\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3599 - accuracy: 0.9017\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.3556 - accuracy: 0.8921\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3352 - accuracy: 0.9181\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3385 - accuracy: 0.9135\n"
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data, use_augmentation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 45ms/step - loss: 0.2942 - accuracy: 0.9521\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 14:30:43.652768: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp62fgqcij/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp62fgqcij/assets\n",
      "/home/clarence/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-31 14:30:47.189791: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-05-31 14:30:47.189821: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmp3ugu9wk9/labels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmp3ugu9wk9/labels.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_data = np.loadtxt('./datasets/model2_data/default.txt')\n",
    "falling_data = np.loadtxt('./datasets/model2_data/falling.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data))))\n",
    "\n",
    "# Convert inputs and outputs to TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "dataset = dataset.shuffle(len(inputs)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.7032 - accuracy: 0.6275\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6862 - accuracy: 0.6438\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6705 - accuracy: 0.6908\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6555 - accuracy: 0.7414\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6409 - accuracy: 0.7703\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6263 - accuracy: 0.8083\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6fa8490700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=6,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    }
   ],
   "source": [
    "# loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "model.save(\"model2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25.0\n"
     ]
    }
   ],
   "source": [
    "# Resize vid\n",
    "video_path = \"s-walk2.mp4\"\n",
    "output_path = \"./posevid.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_dimensions(video_path):\n",
    "    try:\n",
    "        # Open the video file\n",
    "        video = cv2.VideoCapture(video_path)\n",
    "\n",
    "        # Read the first frame\n",
    "        ret, frame = video.read()\n",
    "\n",
    "        # Get the dimensions of the frame\n",
    "        height, width, _ = frame.shape\n",
    "\n",
    "        return width, height\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred: \", e)\n",
    "\n",
    "    finally:\n",
    "        # Release the video capture object\n",
    "        video.release()\n",
    "\n",
    "def crop_frame(frame, target_width, target_height):\n",
    "    # Get the dimensions of the frame\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Calculate the starting point for cropping\n",
    "    start_x = (width - target_width) // 2\n",
    "    start_y = (height - target_height) // 2\n",
    "\n",
    "    # Perform the cropping\n",
    "    cropped_frame = frame[start_y:start_y+target_height, start_x:start_x+target_width, :]\n",
    "\n",
    "    return cropped_frame\n",
    "\n",
    "def pose_predict(image):\n",
    "    OUTPUT_RATIO = 0.008926701731979847\n",
    "    OUTPUT_BIAS = 126\n",
    "    threshold = 0.2/OUTPUT_RATIO+OUTPUT_BIAS\n",
    "    # Example usage\n",
    "    # video_path = \"posevid.mp4\"\n",
    "    # width, height = get_video_dimensions(video_path)\n",
    "    # cropped_frame = crop_frame(image, height, height)\n",
    "    # resized_frame = cv2.resize(image, (368, 368))\n",
    "    # color_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "    # frame = np.expand_dims(color_frame, axis=0)\n",
    "    # frame = frame.astype(np.uint8)\n",
    "    \n",
    "    \n",
    "\n",
    "    # # set input -> invoke -> access output\n",
    "    # interpreter.set_tensor(input_index, frame)\n",
    "    # interpreter.invoke()\n",
    "    # output_data = interpreter.get_tensor(output_index)\n",
    "    # return output_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "OUTPUT_RATIO = 0.008926701731979847\n",
    "OUTPUT_BIAS = 126\n",
    "threshold = 0.2/OUTPUT_RATIO+OUTPUT_BIAS\n",
    "frame_classifications = []\n",
    "moving_prob = []\n",
    "\n",
    "interpreter = tf.lite.Interpreter(model_path=\"./tflite_models/openpose_mobilenetv0.75_quant_1x368x368x3.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "# get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "# keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "# len(input) = len(output) = 1, so access the first element\n",
    "output = interpreter.get_output_details()\n",
    "input = interpreter.get_input_details()\n",
    "output_index = output[0]['index']\n",
    "input_index = input[0]['index']\n",
    "AllPoints = []\n",
    "\n",
    "vid_path = \"posevid.mp4\"\n",
    "cap = cv2.VideoCapture(vid_path)\n",
    "ret, frame = cap.read()\n",
    "for f in range(0,2):\n",
    "    width, height = get_video_dimensions(vid_path)\n",
    "    cropped_frame = crop_frame(frame, height, height)\n",
    "    frame = cropped_frame\n",
    "    resized_frame = cv2.resize(frame, (368, 368))\n",
    "    color_frame = cv2.cvtColor(resized_frame, cv2.COLOR_BGR2RGB)\n",
    "    frame = np.expand_dims(color_frame, axis=0)\n",
    "    frame = frame.astype(np.uint8)\n",
    "    interpreter.set_tensor(input_index, frame)\n",
    "    interpreter.invoke()\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    indexlst = []\n",
    "    for i in range(0,46):\n",
    "        indexrow = []\n",
    "        for j in range(0,46):\n",
    "            list = []\n",
    "            classifications = output_data[0][i][j]\n",
    "            for k in range(0,57):\n",
    "                #float32_value = np.float32(classifications[k]) / 255.0\n",
    "                # if float32_value < 0:\n",
    "                #     float32_value += 256\n",
    "                #list.append((float(classifications[k])- OUTPUT_BIAS) * OUTPUT_RATIO)\n",
    "                list.append(classifications[k])\n",
    "            classifications = list\n",
    "            classifications = [round(num, 2) for num in classifications]\n",
    "            #print(classifications)\n",
    "            indexrow.append(classifications.index(max(classifications)))\n",
    "            newarr = np.array(classifications)\n",
    "                #classifications = [round(num, 4) for num in classifications]\n",
    "        indexlst.append(indexrow)\n",
    "            #print(classifications)\n",
    "    Points = []\n",
    "    for a in range(0,18):\n",
    "        coordinates=(-1,-1)\n",
    "        maxconfidence = -1\n",
    "        for b in range(0,46):\n",
    "            for c in range(0,46):\n",
    "                if indexlst[b][c] == a:\n",
    "                    #print(output_data[0][b][c][a])\n",
    "                    if output_data[0][b][c][a] > maxconfidence and output_data[0][b][c][a] >= threshold:\n",
    "                        maxconfidence = output_data[0][b][c][a]\n",
    "                        coordinates = (b,c)\n",
    "        Points.append(coordinates)\n",
    "    AllPoints.append(Points)\n",
    "    print(f)\n",
    "    ret, frame = cap.read()\n",
    "    # cnt = 0\n",
    "    # for pt in Points:\n",
    "    #     if pt[0] != -1:\n",
    "    #         print(\"x: \"+str(pt[1])+\" \",\"y: \"+str(pt[0])+\" \",\"index: \"+str(cnt))\n",
    "    #     cnt+=1\n",
    "\n",
    "    # print()\n",
    "    # print()\n",
    "    # for i in range(0,len(indexlst)):\n",
    "    #     for j in range(0,len(indexlst[i])):\n",
    "    #         if indexlst[i][j] <= 17:\n",
    "    #             print(\"x: \"+str(i)+\" \",\"y: \"+str(j)+\" \",\"index: \"+str(indexlst[i][j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(46, 46, 57)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming your 1D array is named 'arr'\n",
    "arr = output_data[0]\n",
    "'''array_2d = [[0] * 46 for _ in range(46)]\n",
    "list = []\n",
    "row = 0\n",
    "col = 0\n",
    "for i in range(len(arr)):\n",
    "    list.append(arr[i])\n",
    "    if (i+1) %57 == 0:\n",
    "        array_2d[row][col] = list\n",
    "        row+=1\n",
    "        if row > 45:\n",
    "            row = 0\n",
    "            col+=1\n",
    "        list = []\n",
    "print(np.shape(array_2d))\n",
    "print(array_2d[0][0])\n",
    "print(array_2d[45][45])'''\n",
    "print(np.shape(arr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 18, 2)\n",
      "[(13, 27), (15, 24), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (40, 15), (-1, -1), (-1, -1), (-1, -1), (12, 27), (-1, -1), (12, 25), (-1, -1)]\n",
      "(2, 18, 2)\n",
      "[(254, 528), (293, 469), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (-1, -1), (782, 293), (-1, -1), (-1, -1), (-1, -1), (234, 528), (-1, -1), (234, 489), (-1, -1)]\n"
     ]
    }
   ],
   "source": [
    "#newarr = np.array(AllPoints)\n",
    "print(np.shape(AllPoints))\n",
    "print(AllPoints[0])\n",
    "#print(newarr.shape)\n",
    "# AllPoints = [[1, 2, 3], [4, 5, 6], [7, 8, 9]]\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = \"./pose_output.txt\"\n",
    "\n",
    "# Open the file in write mode\n",
    "with open(file_path, 'w') as file:\n",
    "    # Iterate over each sublist in AllPoints\n",
    "    for sublist in AllPoints:\n",
    "        # Convert the sublist to a string\n",
    "        sublist_str = ' '.join(map(str, sublist))\n",
    "        # Write the sublist string to the file\n",
    "        file.write(sublist_str + '\\n')\n",
    "\n",
    "# Specify the file path and name\n",
    "file_path = \"./pose_output.txt\"\n",
    "\n",
    "# Initialize the larger list\n",
    "larger_list = []\n",
    "\n",
    "# Read the contents of the file\n",
    "with open(file_path, 'r') as file:\n",
    "    # Iterate over each line in the file\n",
    "    for line in file:\n",
    "        # Strip any leading/trailing whitespaces and newline characters\n",
    "        line = line.strip()\n",
    "        line = line.replace(\", \",\",\")\n",
    "        # Split the line by spaces to separate the elements\n",
    "        elements_str = line.split()\n",
    "        \n",
    "        # Ensure the line has exactly 18 elements\n",
    "        if len(elements_str) == 18:\n",
    "            # Convert elements to tuples\n",
    "            elements = [tuple(map(int, elem.strip('()').split(','))) for elem in elements_str]\n",
    "            \n",
    "            # Append the elements list to the larger list\n",
    "            larger_list.append(elements)\n",
    "new_large = []\n",
    "\n",
    "for row in larger_list:\n",
    "    new_row = []\n",
    "    for pt in row:\n",
    "        if pt[0] != -1:\n",
    "            newx = (int)(pt[0]/46*900)\n",
    "            newy = (int)(pt[1]/46*900)\n",
    "            pt = (newx,newy) \n",
    "            new_row.append(pt)\n",
    "        else:\n",
    "            new_row.append((-1,-1))\n",
    "    new_large.append(new_row)\n",
    "\n",
    "print(np.shape(new_large))\n",
    "print(new_large[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_2d = [[0] * 46 for _ in range(46)]\n",
    "list = []\n",
    "row = 0\n",
    "col = 0\n",
    "for i in range(len(arr)):\n",
    "    list.append(arr[i])\n",
    "    if (i+1) %57 == 0:\n",
    "        array_2d[row][col] = list\n",
    "        row+=1\n",
    "        if row > 45:\n",
    "            row = 0\n",
    "            col+=1\n",
    "        list = []\n",
    "print(np.shape(array_2d))\n",
    "print(array_2d[0][0])\n",
    "print(array_2d[45][45])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_probs_trimmed = moving_prob[:-(len(moving_prob)%16)] if len(moving_prob) % 16 != 0 else moving_prob\n",
    "model2_in = np.array(moving_probs_trimmed).reshape((len(moving_prob)//16, 16))\n",
    "# print(model2_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_preds = model2.predict(model2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ True  True False False False False False False False False False]]\n"
     ]
    }
   ],
   "source": [
    "bools = vid_preds.reshape((1, len(vid_preds))) > 0.5\n",
    "print(bools)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
