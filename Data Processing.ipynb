{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader.from_folder(r\"./datasets/tensorflow/photos/\")\n",
    "train_data, test_data = data.split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hub_keras_layer_v1v2_1 (Hub  (None, 1280)             3413024   \n",
      " KerasLayerV1V2)                                                 \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,415,586\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/5\n",
      "82/82 [==============================] - 3s 29ms/step - loss: 0.4151 - accuracy: 0.8624\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.3044 - accuracy: 0.9428\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.2890 - accuracy: 0.9546\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.2799 - accuracy: 0.9596\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 2s 30ms/step - loss: 0.2744 - accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 28ms/step - loss: 0.2613 - accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video resized successfully!\n",
      "shape = (224, 224, 3)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 36\u001b[0m\n\u001b[1;32m     33\u001b[0m     interpreter\u001b[39m.\u001b[39mset_tensor(\u001b[39minput\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mindex\u001b[39m\u001b[39m'\u001b[39m], input_data)\n\u001b[1;32m     34\u001b[0m     interpreter\u001b[39m.\u001b[39minvoke()\n\u001b[0;32m---> 36\u001b[0m     output_data \u001b[39m=\u001b[39m interpreter\u001b[39m.\u001b[39mget_tensor(output[\u001b[39m0\u001b[39;49m][\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     37\u001b[0m     \u001b[39mprint\u001b[39m(output_data)\n\u001b[1;32m     38\u001b[0m \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[39m    # Add the frame classification to the list\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[39m    frame_classifications.append(label)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39m        break\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m        '''\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 0"
     ]
    }
   ],
   "source": [
    "video_path = \"./datasets/vid2.mp4\"\n",
    "output_path = \"resized_video.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load the video using OpenCV\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "\n",
    "# Initialize an empty list to store the frame classifications\n",
    "frame_classifications = []\n",
    "\n",
    "# Loop through the frames of the video\n",
    "while True:\n",
    "    # Read the next frame\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # Break the loop if the video has ended\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    print(f\"shape = {frame_rgb.shape}\")\n",
    "    \n",
    "    # Classify the frame\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "    output = interpreter.get_output_details()  # Get all output details\n",
    "\n",
    "    # Model has single output, so access the first element\n",
    "    output_index = output[0]['index']\n",
    "\n",
    "    input_details = interpreter.get_input_details()[0]  # Model has single input.\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "    input_data = frame_rgb.astype(np.float32)  # Ensure input data type matches the model\n",
    "\n",
    "    interpreter.set_tensor(input_details['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output[0]['index'])\n",
    "    print(output_data)\n",
    "'''\n",
    "    # Add the frame classification to the list\n",
    "    frame_classifications.append(label)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "        '''\n",
    "\n",
    "# Release the video capture and close the OpenCV windows\n",
    "cap.release()\n",
    "#cv2.destroyAllWindows()\n",
    "\n",
    "# Convert the frame classifications to a numpy array\n",
    "frame_classifications = np.array(frame_classifications)\n",
    "\n",
    "# Print the frame classifications\n",
    "\n",
    "\n",
    "print(\"Frame Classifications:\", frame_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 09:14:16.251543: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4umafjyf/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp4umafjyf/assets\n",
      "2023-05-26 09:14:19.503374: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.503486: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-05-26 09:14:19.503538: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-26 09:14:19.503827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.503946: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.504087: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.504222: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.504327: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 09:14:19.504398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6652 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2023-05-26 09:14:19.528266: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 913 nodes (656), 923 edges (664), time = 13.564ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.006ms.\n",
      "\n",
      "/home/sylvia/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-26 09:14:20.075091: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-05-26 09:14:20.075121: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmp2uz_hpi3/labels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmp2uz_hpi3/labels.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
