{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All needed imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, GlobalAveragePooling2D, Dropout\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limit GPU Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 13:38:40.839413: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:40.934816: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:40.934962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:40.973099: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-10 13:38:40.973486: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:40.973637: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:40.973733: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:44.123021: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:44.123636: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:44.124143: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-10 13:38:44.124534: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing Data for Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_paths = [\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall15.mp4\"\n",
    "                 ]\n",
    "\n",
    "default_paths = [\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default15.mp4\"\n",
    "                 ]\n",
    "\n",
    "test_falling_paths = [\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall1.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall2.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall3.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall4.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall5.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall6.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall7.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall8.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall9.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall10.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall11.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall12.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall13.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall14.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall15.mp4\"\n",
    "]\n",
    "\n",
    "test_default_paths = [\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default1.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default2.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default3.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default4.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default5.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default6.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default7.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default8.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default9.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default10.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default11.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default12.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default13.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default14.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default15.mp4\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processing code, returns frame (num_frames, 224,224,3) and frame_diffs (num_frames, 224,224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to apply the same random transformation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_videos(video_paths, label):\n",
    "    frames = []\n",
    "    frame_diffs = []\n",
    "    labels = []\n",
    "    \n",
    "    for path in video_paths:\n",
    "        video_cap = cv2.VideoCapture(path)\n",
    "        \n",
    "        prev_gray_frame = None\n",
    "        \n",
    "        while video_cap.isOpened():\n",
    "            ret, frame = video_cap.read()\n",
    "            \n",
    "            if not ret:\n",
    "                break\n",
    "            \n",
    "            # Resize and convert frame to RGB\n",
    "            frame_resized = cv2.resize(frame, (224, 224))\n",
    "            frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Convert frame to grayscale\n",
    "            gray_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            if prev_gray_frame is None:\n",
    "                frame_diff = np.zeros_like(gray_frame, dtype=np.float32)\n",
    "            else:\n",
    "                frame_diff = cv2.absdiff(prev_gray_frame, gray_frame)\n",
    "                # frame_diff = cv2.absdiff(prev_gray_frame, gray_frame) / 255.0\n",
    "            \n",
    "            prev_gray_frame = gray_frame\n",
    "            \n",
    "            frames.append(frame_rgb)\n",
    "            frame_diffs.append(frame_diff)\n",
    "            labels.append(label)  # 0 for still, 1 for moving\n",
    "        # print(len(labels))\n",
    "        video_cap.release()\n",
    "\n",
    "    return np.array(frames), np.array(frame_diffs), np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping to 224 x 224 x 4 for the convolutional model\n",
    "def combine_frames_and_diffs(frames, frame_diffs):\n",
    "    # frame_diffs dimension\n",
    "    # print(frame_diffs.shape)\n",
    "    frame_diffs_expanded = np.expand_dims(frame_diffs, axis=-1) # Add an extra dimension\n",
    "    # frame_diffs_expanded dimension\n",
    "    # print(frame_diffs_expanded.shape)\n",
    "    combined_input = np.concatenate([frames, frame_diffs_expanded], axis=-1) # Concatenate along the last axis\n",
    "    # combined_input dimension\n",
    "    # print(combined_input.shape)\n",
    "    \n",
    "    # Visualize the difference\n",
    "    # cv2.imwrite('img_still.png', combined_input[10])\n",
    "    return combined_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 224, 224, 4)\n",
      "(1015, 224, 224, 4)\n"
     ]
    }
   ],
   "source": [
    "# Generate the frames and frame_diffs array for still\n",
    "still_frames, still_diff, still_labels = process_videos(default_paths, 0)\n",
    "test_still_frames, test_still_diff, test_still_labels = process_videos(test_default_paths, 0)\n",
    "# print(still_frames.shape)\n",
    "falling_frames, falling_diff, falling_labels = process_videos(falling_paths, 1)\n",
    "test_falling_frames, test_falling_diff, test_falling_labels = process_videos(test_falling_paths, 1)\n",
    "# print(falling_frames.shape)\n",
    "\n",
    "# Combine them\n",
    "concatenate_frames = np.concatenate([still_frames, falling_frames], axis = 0)\n",
    "# print(concatenate_frames.shape)\n",
    "concatenate_diff =  np.concatenate([still_diff, falling_diff], axis = 0)\n",
    "# print(concatenate_diff.shape)\n",
    "concatenate_labels =  np.concatenate([still_labels, falling_labels], axis = 0)\n",
    "# print(concatenate_labels.shape)\n",
    "\n",
    "# Create 224x224x4 shape for the model\n",
    "combined_input = combine_frames_and_diffs(concatenate_frames, concatenate_diff)\n",
    "# print(combined_input)\n",
    "\n",
    "# Combine them\n",
    "test_concatenate_frames = np.concatenate([test_still_frames, test_falling_frames], axis = 0)\n",
    "test_concatenate_diff =  np.concatenate([test_still_diff, test_falling_diff], axis = 0)\n",
    "test_concatenate_labels =  np.concatenate([test_still_labels, test_falling_labels], axis = 0)\n",
    "# print(test_concatenate_frames.shape)\n",
    "# Create 224x224x4 shape for the model\n",
    "test_combined_input = combine_frames_and_diffs(test_concatenate_frames, test_concatenate_diff)\n",
    "print(combined_input.shape)\n",
    "print(test_combined_input.shape)\n",
    "# Shuffle the data if needed\n",
    "indices = np.arange(combined_input.shape[0])\n",
    "np.random.shuffle(indices)\n",
    "combined_input = combined_input[indices]\n",
    "concatenate_labels = concatenate_labels[indices]\n",
    "\n",
    "#data augmentation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    # layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomContrast(factor=0.2)\n",
    "\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TensorFlow dataset\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((combined_input, concatenate_labels))\n",
    "    dataset = dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    test_dataset = tf.data.Dataset.from_tensor_slices((test_combined_input, test_concatenate_labels))\n",
    "    test_dataset = test_dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "# Split the dataset into training and validation\n",
    "train_dataset = dataset.take(int(0.8 * len(dataset)))\n",
    "val_dataset = dataset.skip(int(0.8 * len(dataset)))\n",
    "\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((combined_input, concatenate_labels))\n",
    "# Batch and prefetch if needed\n",
    "# dataset = dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " combined_input (InputLayer)  [(None, 224, 224, 4)]    0         \n",
      "                                                                 \n",
      " sequential_1 (Sequential)   (None, 224, 224, 4)       0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 222, 222, 32)      1184      \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 111, 111, 32)     0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling2D)                                        \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,369\n",
      "Trainable params: 118,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    combined_input = Input(shape=(224, 224, 4), name='combined_input')\n",
    "\n",
    "    # Apply data augmentation here\n",
    "    x = data_augmentation(combined_input)\n",
    "\n",
    "    # Convolutional layers for image processing\n",
    "    # Convolutional layers are useful for learning spatial hierarchies and detecting features \n",
    "    x = Conv2D(32, (3, 3), activation='relu')(x) #  Convolutional layers extract spatial features from the image\n",
    "    x = MaxPooling2D((2, 2))(x) #  Max-pooling layers reduce the size of the feature maps.\n",
    "    x = Conv2D(64, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    x = Conv2D(128, (3, 3), activation='relu')(x)\n",
    "    x = MaxPooling2D((2, 2))(x)\n",
    "    \n",
    "    # Fully connected layers: These layers learn to make decisions based on the features extracted by the convolutional layers\n",
    "    # x = Flatten()(x) # This line flattens the 2D feature maps into a 1D vector\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    output = Dense(1, activation='sigmoid')(x)  # Single output with sigmoid activation\n",
    "\n",
    "    model = Model(inputs=combined_input, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])  # Binary cross-entropy loss\n",
    "    \n",
    "    return model\n",
    "    \n",
    "model = create_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "54/54 [==============================] - 3s 40ms/step - loss: 0.7906 - accuracy: 0.7141 - val_loss: 0.4825 - val_accuracy: 0.8333\n",
      "Epoch 2/5\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.3707 - accuracy: 0.8507 - val_loss: 0.2282 - val_accuracy: 0.9306\n",
      "Epoch 3/5\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.3426 - accuracy: 0.8738 - val_loss: 0.2141 - val_accuracy: 0.9398\n",
      "Epoch 4/5\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.2935 - accuracy: 0.8854 - val_loss: 0.2182 - val_accuracy: 0.9491\n",
      "Epoch 5/5\n",
      "54/54 [==============================] - 2s 39ms/step - loss: 0.2982 - accuracy: 0.8889 - val_loss: 0.2227 - val_accuracy: 0.9398\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f46e0470820>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_dataset, validation_data=val_dataset, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 1s 11ms/step - loss: 0.4543 - accuracy: 0.7714\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.4542892277240753, 0.7714285850524902]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(test_dataset)\n",
    "# model.save(\"m1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantize and save as tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmprhk8dmwg/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sylvia/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-08-10 10:32:08.581405: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-08-10 10:32:08.581426: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-08-10 10:32:08.581972: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmprhk8dmwg\n",
      "2023-08-10 10:32:08.584459: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-08-10 10:32:08.584471: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmprhk8dmwg\n",
      "2023-08-10 10:32:08.593697: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-08-10 10:32:08.641366: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmprhk8dmwg\n",
      "2023-08-10 10:32:08.658135: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 76166 microseconds.\n",
      "2023-08-10 10:32:08.685283: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for data, label in tf.data.Dataset.from_tensor_slices((combined_input, concatenate_labels)).batch(1).take(100):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_m1 = converter.convert()\n",
    "\n",
    "# Save the quantized model 1\n",
    "with open('m1_quant.tflite', 'wb') as f: f.write(tflite_quant_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass in video for model 1 to predict to generate data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [  1 224 224   4]\n",
      "Output shape: [1 1]\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter = tf.lite.Interpreter(model_path=\"m1_quant.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "print('Input shape:', input_details[0]['shape'])\n",
    "output_details = interpreter.get_output_details()\n",
    "print('Output shape:', output_details[0]['shape'])\n",
    "\n",
    "# [1,1] u get [[1]], instead of [1]\n",
    "# idk why it wraps an extra bracket but it just does"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict with model 1 and save the results into an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_single_vid(path):\n",
    "    frames = []\n",
    "    frame_diffs = []\n",
    "    video_cap = cv2.VideoCapture(path)\n",
    "    \n",
    "    prev_gray_frame = None\n",
    "    \n",
    "    while video_cap.isOpened():\n",
    "        ret, frame = video_cap.read()\n",
    "        \n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # Resize and convert frame to RGB\n",
    "        frame_resized = cv2.resize(frame, (224, 224))\n",
    "        frame_rgb = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Convert frame to grayscale\n",
    "        gray_frame = cv2.cvtColor(frame_resized, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        if prev_gray_frame is None:\n",
    "            frame_diff = np.zeros_like(gray_frame, dtype=np.float32)\n",
    "        else:\n",
    "            frame_diff = cv2.absdiff(prev_gray_frame, gray_frame)\n",
    "            # frame_diff = cv2.absdiff(prev_gray_frame, gray_frame) / 255.0\n",
    "        \n",
    "        prev_gray_frame = gray_frame\n",
    "        \n",
    "        frames.append(frame_rgb)\n",
    "        frame_diffs.append(frame_diff)\n",
    "\n",
    "    video_cap.release()\n",
    "    return np.array(frames), np.array(frame_diffs)\n",
    "\n",
    "def predict_single_vid(interpreter, input_data):\n",
    "    # Get the input and output details\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    # Initialize an empty list to store the results\n",
    "    results = []\n",
    "\n",
    "    # Iterate over each instance in the input data\n",
    "    for instance in input_data:\n",
    "        # Set the tensor to point to the input data to be inferred\n",
    "        interpreter.set_tensor(input_details['index'], np.expand_dims(instance, axis=0))\n",
    "\n",
    "        # Run the computation\n",
    "        interpreter.invoke()\n",
    "\n",
    "        # Get the output tensor\n",
    "        output = interpreter.get_tensor(output_details['index'])\n",
    "\n",
    "        # Append the output to the results list\n",
    "        results.append(output)\n",
    "\n",
    "    # Convert the results list to an array and return it\n",
    "    return np.array(results)\n",
    "\n",
    "def add_noise(data, noise_level=5):\n",
    "    # Convert to float\n",
    "    float_data = data.astype(np.float32)\n",
    "    \n",
    "    # Generate random noise\n",
    "    noise = np.random.uniform(-noise_level, noise_level, data.shape)\n",
    "    \n",
    "    # Add the noise to the data\n",
    "    noisy_data = float_data + noise\n",
    "    \n",
    "    # Clip values to uint8 range and convert back to uint8\n",
    "    noisy_data = np.clip(noisy_data, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return noisy_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_8 = []\n",
    "labels = []\n",
    "for vid in falling_paths:\n",
    "    vid_frames, vid_diffs = process_single_vid(vid)\n",
    "    # reshape and combine inputs\n",
    "    combined = combine_frames_and_diffs(vid_frames, vid_diffs)\n",
    "    # convert datatype from float 32 to uint8\n",
    "    combined = combined.astype(np.uint8)\n",
    "    # print(combined)\n",
    "    # predict_single_vid\n",
    "    res = predict_single_vid(interpreter, combined)\n",
    "    # save results to array in sliding window fashion 8 at a time\n",
    "    for i in range(len(res) - 8 + 1):\n",
    "        window = res[i:i+8]\n",
    "        result_8.append(window)\n",
    "        # print(result_8)\n",
    "        labels.append(1)\n",
    "\n",
    "for vid in default_paths:\n",
    "    vid_frames, vid_diffs = process_single_vid(vid)\n",
    "    # reshape and combine inputs\n",
    "    combined = combine_frames_and_diffs(vid_frames, vid_diffs)\n",
    "    # convert datatype from float 32 to uint8\n",
    "    combined = combined.astype(np.uint8)\n",
    "    # predict_single_vid\n",
    "    res = predict_single_vid(interpreter, combined)\n",
    "    # save results to array in sliding window fashion 8 at a time\n",
    "    for i in range(len(res) - 8 + 1):\n",
    "        window = res[i:i+8]\n",
    "        result_8.append(window)\n",
    "        labels.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1740, 8)\n",
      "(1740,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "result_8 = np.array(result_8)\n",
    "labels = np.array(labels)\n",
    "\n",
    "result_8 = result_8.reshape(len(result_8), 8)\n",
    "noisy_data = add_noise(result_8)\n",
    "doubled_data = np.vstack((result_8, noisy_data))\n",
    "augmented_labels = labels\n",
    "doubled_labels = np.hstack((labels, augmented_labels))\n",
    "\n",
    "print(doubled_data.shape)\n",
    "print(doubled_labels.shape)\n",
    "\n",
    "np.savetxt('model2data_augmented.txt', doubled_data, fmt='%d', delimiter=' ')\n",
    "np.savetxt('model2labels_augmented.txt', doubled_labels, fmt='%d', delimiter=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1740, 8)\n"
     ]
    }
   ],
   "source": [
    "X = np.loadtxt('model2data_augmented.txt', dtype=np.float32, delimiter=' ')\n",
    "X = X/255.0\n",
    "\n",
    "print(X.shape)\n",
    "Y = np.loadtxt('model2labels_augmented.txt', dtype=np.float32, delimiter=' ')\n",
    "# print(Y.shape)\n",
    "\n",
    "# fall_X = np.loadtxt('./../K-optimized1-training/model1_output_fall.txt', dtype=np.uint8, delimiter=' ')\n",
    "# default_X = np.loadtxt('./../K-optimized1-training/model1_output_default.txt', dtype=np.uint8, delimiter=' ')\n",
    "# X = np.concatenate((fall_X, default_X))\n",
    "\n",
    "# fall_Y = np.ones(len(fall_X))\n",
    "# default_Y = np.zeros(len(default_X))\n",
    "\n",
    "# Y = np.concatenate((fall_Y, default_Y))\n",
    "# print(Y.shape)\n",
    "\n",
    "# Create the dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "dataset = dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_8 = []\n",
    "test_labels = []\n",
    "\n",
    "for vid in test_falling_paths:\n",
    "    test_vid_frames, test_vid_diffs = process_single_vid(vid)\n",
    "    # reshape and combine inputs\n",
    "    test_combined = combine_frames_and_diffs(test_vid_frames, test_vid_diffs)\n",
    "    # convert datatype from float 32 to uint8\n",
    "    test_combined = test_combined.astype(np.uint8)\n",
    "    # print(combined)\n",
    "    # predict_single_vid\n",
    "    test_res = predict_single_vid(interpreter, test_combined)\n",
    "    # save results to array in sliding window fashion 8 at a time\n",
    "    for i in range(len(test_res) - 8 + 1):\n",
    "        window = test_res[i:i+8]\n",
    "        test_result_8.append(window)\n",
    "        test_labels.append(1)\n",
    "\n",
    "for vid in test_default_paths:\n",
    "    test_vid_frames, test_vid_diffs = process_single_vid(vid)\n",
    "    # reshape and combine inputs\n",
    "    test_combined = combine_frames_and_diffs(test_vid_frames, test_vid_diffs)\n",
    "    # convert datatype from float 32 to uint8\n",
    "    test_combined = test_combined.astype(np.uint8)\n",
    "    # predict_single_vid\n",
    "    test_res = predict_single_vid(interpreter, test_combined)\n",
    "    # save results to array in sliding window fashion 8 at a time\n",
    "    for i in range(len(test_res) - 8 + 1):\n",
    "        window = test_res[i:i+8]\n",
    "        test_result_8.append(window)\n",
    "        test_labels.append(0)\n",
    "\n",
    "test_result_8 = np.array(test_result_8)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805, 8, 1, 1)\n",
      "(805,)\n",
      "(805, 8)\n"
     ]
    }
   ],
   "source": [
    "print(test_result_8.shape)\n",
    "print(test_labels.shape)\n",
    "test_result_8 = test_result_8.reshape(len(test_result_8), 8)\n",
    "np.savetxt('model2data_test.txt', test_result_8, fmt='%d', delimiter=' ')\n",
    "np.savetxt('model2labels_test.txt', test_labels, fmt='%d', delimiter=' ')\n",
    "print(test_result_8.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805, 8)\n",
      "(805,)\n"
     ]
    }
   ],
   "source": [
    "test_X = np.loadtxt('model2data_test.txt', dtype=np.float32, delimiter=' ')\n",
    "test_X = test_X/255.0\n",
    "print(test_X.shape)\n",
    "test_Y = np.loadtxt('model2labels_test.txt', dtype=np.float32, delimiter=' ')\n",
    "print(test_Y.shape)\n",
    "\n",
    "# test_fall_X = np.loadtxt('./../K-optimized1-training/model1_output_fall_test.txt', dtype=np.uint8, delimiter=' ')\n",
    "# test_default_X = np.loadtxt('./../K-optimized1-training/model1_output_default_test.txt', dtype=np.uint8, delimiter=' ')\n",
    "# test_X = np.concatenate((test_fall_X, test_default_X))\n",
    "\n",
    "# test_fall_Y = np.ones(len(test_fall_X))\n",
    "# test_default_Y = np.zeros(len(test_default_X))\n",
    "\n",
    "# test_Y = np.concatenate((test_fall_Y, test_default_Y))\n",
    "# print(test_Y.shape)\n",
    "\n",
    "# Create the dataset\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_X, test_Y))\n",
    "test_dataset = test_dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_layer (InputLayer)    [(None, 8)]               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 64)                576       \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 16)                528       \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,201\n",
      "Trainable params: 3,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model_2():\n",
    "    input_shape = (8, )  # Input shape for the sequence of 8 uint8 values\n",
    "    \n",
    "    # Define the model\n",
    "    input_layer = Input(shape=input_shape, name='input_layer')\n",
    "    \n",
    "    # Several dense layers to extract features and relationships\n",
    "    x = Dense(64, activation='relu')(input_layer)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(16, activation='relu')(x)\n",
    "    \n",
    "    # Output layer\n",
    "    output = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "    # Create and compile the model\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "model_2 = create_model_2()\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare Tensorflow dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.5715 - accuracy: 0.5333\n",
      "Epoch 2/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.4346 - accuracy: 0.8770\n",
      "Epoch 3/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.3165 - accuracy: 0.9529\n",
      "Epoch 4/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1935 - accuracy: 0.9621\n",
      "Epoch 5/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1433 - accuracy: 0.9672\n",
      "Epoch 6/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1165 - accuracy: 0.9684\n",
      "Epoch 7/15\n",
      "109/109 [==============================] - 1s 5ms/step - loss: 0.1155 - accuracy: 0.9724\n",
      "Epoch 8/15\n",
      "109/109 [==============================] - 0s 4ms/step - loss: 0.1148 - accuracy: 0.9736\n",
      "Epoch 9/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1043 - accuracy: 0.9776\n",
      "Epoch 10/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9759\n",
      "Epoch 11/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.1090 - accuracy: 0.9759\n",
      "Epoch 12/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0979 - accuracy: 0.9724\n",
      "Epoch 13/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0975 - accuracy: 0.9753\n",
      "Epoch 14/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0980 - accuracy: 0.9753\n",
      "Epoch 15/15\n",
      "109/109 [==============================] - 0s 2ms/step - loss: 0.0969 - accuracy: 0.9782\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fa7d8208fd0>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(dataset, epochs=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51/51 [==============================] - 0s 996us/step - loss: 0.8359 - accuracy: 0.8932\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.8359243273735046, 0.8931676745414734]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float32\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp0q_s911v/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp0q_s911v/assets\n",
      "/home/sylvia/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-08-10 13:58:37.414938: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-08-10 13:58:37.414960: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-08-10 13:58:37.415137: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmp0q_s911v\n",
      "2023-08-10 13:58:37.416346: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-08-10 13:58:37.416358: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmp0q_s911v\n",
      "2023-08-10 13:58:37.420355: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-08-10 13:58:37.455065: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmp0q_s911v\n",
      "2023-08-10 13:58:37.466654: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 51518 microseconds.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_layer:0 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 14\u001b[0m\n\u001b[1;32m     11\u001b[0m converter\u001b[39m.\u001b[39minference_input_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mint8\n\u001b[1;32m     12\u001b[0m converter\u001b[39m.\u001b[39minference_output_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mint8\n\u001b[0;32m---> 14\u001b[0m tflite_model \u001b[39m=\u001b[39m converter\u001b[39m.\u001b[39;49mconvert()\n\u001b[1;32m     16\u001b[0m \u001b[39m# Save the model to disk\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mm2_quant.tflite\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f: f\u001b[39m.\u001b[39mwrite(tflite_model)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:803\u001b[0m, in \u001b[0;36m_export_metrics.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(convert_func)\n\u001b[1;32m    801\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    802\u001b[0m   \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[0;32m--> 803\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_and_export_metrics(convert_func, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:789\u001b[0m, in \u001b[0;36mTFLiteConverterBase._convert_and_export_metrics\u001b[0;34m(self, convert_func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save_conversion_params_metric()\n\u001b[1;32m    788\u001b[0m start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mprocess_time()\n\u001b[0;32m--> 789\u001b[0m result \u001b[39m=\u001b[39m convert_func(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    790\u001b[0m elapsed_time_ms \u001b[39m=\u001b[39m (time\u001b[39m.\u001b[39mprocess_time() \u001b[39m-\u001b[39m start_time) \u001b[39m*\u001b[39m \u001b[39m1000\u001b[39m\n\u001b[1;32m    791\u001b[0m \u001b[39mif\u001b[39;00m result:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:1210\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2.convert\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1197\u001b[0m \u001b[39m@_export_metrics\u001b[39m\n\u001b[1;32m   1198\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mconvert\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1199\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Converts a keras model based on instance variables.\u001b[39;00m\n\u001b[1;32m   1200\u001b[0m \n\u001b[1;32m   1201\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1208\u001b[0m \u001b[39m      Invalid quantization parameters.\u001b[39;00m\n\u001b[1;32m   1209\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1210\u001b[0m   saved_model_convert_result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_convert_as_saved_model()\n\u001b[1;32m   1211\u001b[0m   \u001b[39mif\u001b[39;00m saved_model_convert_result:\n\u001b[1;32m   1212\u001b[0m     \u001b[39mreturn\u001b[39;00m saved_model_convert_result\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:1192\u001b[0m, in \u001b[0;36mTFLiteKerasModelConverterV2._convert_as_saved_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1189\u001b[0m   graph_def, input_tensors, output_tensors \u001b[39m=\u001b[39m (\n\u001b[1;32m   1190\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_keras_to_saved_model(temp_dir))\n\u001b[1;32m   1191\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msaved_model_dir:\n\u001b[0;32m-> 1192\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(TFLiteKerasModelConverterV2,\n\u001b[1;32m   1193\u001b[0m                  \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mconvert(graph_def, input_tensors, output_tensors)\n\u001b[1;32m   1194\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   1195\u001b[0m   shutil\u001b[39m.\u001b[39mrmtree(temp_dir, \u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:1009\u001b[0m, in \u001b[0;36mTFLiteConverterBaseV2.convert\u001b[0;34m(self, graph_def, input_tensors, output_tensors)\u001b[0m\n\u001b[1;32m   1002\u001b[0m \u001b[39m# Converts model.\u001b[39;00m\n\u001b[1;32m   1003\u001b[0m result \u001b[39m=\u001b[39m _convert_graphdef(\n\u001b[1;32m   1004\u001b[0m     input_data\u001b[39m=\u001b[39mgraph_def,\n\u001b[1;32m   1005\u001b[0m     input_tensors\u001b[39m=\u001b[39minput_tensors,\n\u001b[1;32m   1006\u001b[0m     output_tensors\u001b[39m=\u001b[39moutput_tensors,\n\u001b[1;32m   1007\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mconverter_kwargs)\n\u001b[0;32m-> 1009\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_optimize_tflite_model(\n\u001b[1;32m   1010\u001b[0m     result, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quant_mode, quant_io\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexperimental_new_quantizer)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py:216\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    215\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 216\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py:206\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    205\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:749\u001b[0m, in \u001b[0;36mTFLiteConverterBase._optimize_tflite_model\u001b[0;34m(self, model, quant_mode, quant_io)\u001b[0m\n\u001b[1;32m    747\u001b[0m   q_activations_type \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39mactivations_type()\n\u001b[1;32m    748\u001b[0m   q_allow_float \u001b[39m=\u001b[39m quant_mode\u001b[39m.\u001b[39mis_allow_float()\n\u001b[0;32m--> 749\u001b[0m   model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_quantize(\n\u001b[1;32m    750\u001b[0m       model, q_in_type, q_out_type, q_activations_type, q_allow_float)\n\u001b[1;32m    752\u001b[0m m_in_type \u001b[39m=\u001b[39m in_type \u001b[39mif\u001b[39;00m in_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n\u001b[1;32m    753\u001b[0m m_out_type \u001b[39m=\u001b[39m out_type \u001b[39mif\u001b[39;00m out_type \u001b[39melse\u001b[39;00m _dtypes\u001b[39m.\u001b[39mfloat32\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/lite.py:555\u001b[0m, in \u001b[0;36mTFLiteConverterBase._quantize\u001b[0;34m(self, result, input_type, output_type, activations_type, allow_float)\u001b[0m\n\u001b[1;32m    551\u001b[0m calibrate_quantize \u001b[39m=\u001b[39m _calibrator\u001b[39m.\u001b[39mCalibrator(result,\n\u001b[1;32m    552\u001b[0m                                             custom_op_registerers_by_name,\n\u001b[1;32m    553\u001b[0m                                             custom_op_registerers_by_func)\n\u001b[1;32m    554\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexperimental_new_quantizer:\n\u001b[0;32m--> 555\u001b[0m   calibrated \u001b[39m=\u001b[39m calibrate_quantize\u001b[39m.\u001b[39;49mcalibrate(\n\u001b[1;32m    556\u001b[0m       \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrepresentative_dataset\u001b[39m.\u001b[39;49minput_gen)\n\u001b[1;32m    558\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_experimental_calibrate_only:\n\u001b[1;32m    559\u001b[0m   \u001b[39mreturn\u001b[39;00m calibrated\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py:216\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m error:\n\u001b[1;32m    215\u001b[0m   report_error_message(\u001b[39mstr\u001b[39m(error))\n\u001b[0;32m--> 216\u001b[0m   \u001b[39mraise\u001b[39;00m error \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert_phase.py:206\u001b[0m, in \u001b[0;36mconvert_phase.<locals>.actual_decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    204\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    205\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 206\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    207\u001b[0m   \u001b[39mexcept\u001b[39;00m ConverterError \u001b[39mas\u001b[39;00m converter_error:\n\u001b[1;32m    208\u001b[0m     \u001b[39mif\u001b[39;00m converter_error\u001b[39m.\u001b[39merrors:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py:224\u001b[0m, in \u001b[0;36mCalibrator.calibrate\u001b[0;34m(self, dataset_gen)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[39m@convert_phase\u001b[39m(Component\u001b[39m.\u001b[39mOPTIMIZE_TFLITE_MODEL, SubComponent\u001b[39m.\u001b[39mCALIBRATE)\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcalibrate\u001b[39m(\u001b[39mself\u001b[39m, dataset_gen):\n\u001b[1;32m    216\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Calibrates the model with specified generator.\u001b[39;00m\n\u001b[1;32m    217\u001b[0m \n\u001b[1;32m    218\u001b[0m \u001b[39m  Returns:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m    dataset_gen: A generator that generates calibration samples.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_feed_tensors(dataset_gen, resize_input\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    225\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mCalibrate()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/optimize/calibrator.py:138\u001b[0m, in \u001b[0;36mCalibrator._feed_tensors\u001b[0;34m(self, dataset_gen, resize_input)\u001b[0m\n\u001b[1;32m    136\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_calibrator\u001b[39m.\u001b[39mFeedTensor(input_array, signature_key)\n\u001b[1;32m    137\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 138\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_calibrator\u001b[39m.\u001b[39;49mFeedTensor(input_array)\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot set tensor: Got value of type INT8 but expected type FLOAT32 for input 0, name: serving_default_input_layer:0 "
     ]
    }
   ],
   "source": [
    "print(X.dtype)\n",
    "def representative_dataset_2():\n",
    "    for i in range(len(X)):\n",
    "        sample = X[i:i+1].astype(np.float32)  # Extract and cast in one line\n",
    "        yield [sample]\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model_2)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset_2\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.int8\n",
    "converter.inference_output_type = tf.int8\n",
    "\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the model to disk\n",
    "with open('m2_quant.tflite', 'wb') as f: f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking input / output of model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: [1 8]\n",
      "Input type: <class 'numpy.uint8'>\n",
      "Output shape: [1 1]\n",
      "Output type: <class 'numpy.uint8'>\n"
     ]
    }
   ],
   "source": [
    "# Load TFLite model and allocate tensors.\n",
    "interpreter_2 = tf.lite.Interpreter(model_path=\"m2_quant.tflite\")\n",
    "interpreter_2.allocate_tensors()\n",
    "\n",
    "input_details = interpreter_2.get_input_details()\n",
    "print('Input shape:', input_details[0]['shape'])\n",
    "print('Input type:', input_details[0]['dtype'])\n",
    "output_details = interpreter_2.get_output_details()\n",
    "print('Output shape:', output_details[0]['shape'])\n",
    "print('Output type:', output_details[0]['dtype'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
