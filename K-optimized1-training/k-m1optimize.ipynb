{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 16:59:40.142685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.158697: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.158856: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.159549: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-14 16:59:40.159808: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.159938: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.160051: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.533298: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.533468: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.533576: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-14 16:59:40.533662: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864\n",
      "1127\n"
     ]
    }
   ],
   "source": [
    "#cut a video into different frames\n",
    "def cut_video(filename):\n",
    "    list_of_frames = []\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count  = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        list_of_frames.append(frame)\n",
    "   \n",
    "    cap.release()\n",
    "    return np.array(list_of_frames)\n",
    "\n",
    "\n",
    "# loop through each frame in the array of images, take two frames at a time, \n",
    "# # loop through each pixel in the 2 frames, find the differences in values \n",
    "# in grey scale for the corresponding pixels, append the diff to the rgb array \n",
    "# representing each pixel =>>>>>>>> RGBX. Notei if first frame append 0 to rgb array\n",
    "def load_images_from_folders(data_path,total): \n",
    "    images = np.zeros((total,224,224,4),dtype=float)\n",
    "    counter = 0\n",
    "\n",
    "    #loop through each vid in the folder \n",
    "    for index in range(len(data_path)): #looping through the entire folder array\n",
    "        \n",
    "        #variable to track whether it is the first image/frame in the video\n",
    "        first_image = True\n",
    "\n",
    "        #cut the video into different frames and save them all into an array\n",
    "        array_images = cut_video(data_path[index]) # array_images[0][0][0] = single pixel\n",
    "\n",
    "        #array to store modified pixel of each image, later on append it to images[]\n",
    "        \n",
    "  \n",
    "        for i in range(len(array_images)-1):\n",
    "            if first_image == True:\n",
    "                #process for 0th and 1st frame\n",
    "                images[counter] = first_frame_pixel_modification(array_images[i])\n",
    "                first_image = False\n",
    "                counter+=1\n",
    "            \n",
    "            images[counter] = two_frame_pixel_modification(array_images[i],array_images[i+1])\n",
    "            counter+=1\n",
    "        \n",
    "    #         print(\"finished frame \" + str(i)+ \" of video \" + str(index))\n",
    "        \n",
    "    #     print(str(counter) +\" matches \" + str(len(array_images)))\n",
    "    #     print(\"finished video \" + str(index))\n",
    "        \n",
    "    #     #after finish loop through a frame, append the frame into the images[] array\n",
    "    # print(\"final count: \" + str(counter) + \"; expected count = \" + str(total))\n",
    "    print(counter)\n",
    "    # print(images.shape)\n",
    "    return images\n",
    "\n",
    "def two_frame_pixel_modification(frame1, frame2): \n",
    "    row_frame = np.zeros((224,224,4),dtype=float)\n",
    "    np.array(row_frame) \n",
    "    for r in range(len(frame1)):\n",
    "        column_frame = np.zeros((224,4),dtype=float)\n",
    "        for c in range(len(frame1[0])):\n",
    "            #calcualte each pixel's difference \n",
    "            grey1 =  find_greyscale(frame1[r][c][2],frame1[r][c][1],frame1[r][c][0])\n",
    "            grey2 =  find_greyscale(frame2[r][c][2],frame2[r][c][1],frame2[r][c][0])\n",
    "            diff_grey = abs(grey1-grey2)\n",
    "            new_rgbx = [frame2[r][c][2],frame2[r][c][1],frame2[r][c][0],diff_grey] #add greyscale diff to the end of the pixel array, and append it to the individual frame array\n",
    "            new_rgbx = np.array(new_rgbx)\n",
    "            column_frame[c] = np.array(new_rgbx)\n",
    "        \n",
    "        row_frame[r] = column_frame\n",
    " \n",
    "        \n",
    "    return row_frame \n",
    "\n",
    "def first_frame_pixel_modification(frame0):\n",
    "    row_frame = np.zeros((224,224,4),dtype=float)\n",
    "    np.array(row_frame)\n",
    "    for r in range(len(frame0)):\n",
    "\n",
    "        column_frame = np.zeros((224,4),dtype=float)\n",
    "        for c in range(len(frame0[0])): \n",
    "\n",
    "            new_rgbx = [frame0[r][c][2],frame0[r][c][1],frame0[r][c][0],0]\n",
    "            column_frame[c] = np.array(new_rgbx)\n",
    "\n",
    "        row_frame[r] = column_frame\n",
    "\n",
    "    return row_frame\n",
    "    \n",
    "\n",
    "def find_greyscale(r,g,b):\n",
    "    grey = 0.299*r + 0.587*g + 0.114*b\n",
    "    return grey\n",
    "\n",
    "def cal_totalframes(array_of_vids):\n",
    "    total_frames = 0\n",
    "    for i in range(len(array_of_vids)):\n",
    "        list_of_cut_frames = cut_video(array_of_vids[i])\n",
    "        total_frames+= len(list_of_cut_frames)\n",
    "    return total_frames\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "moving_path = [\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall15.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall16.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall17.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall18.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall19.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall20.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall21.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall22.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall23.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall24.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall25.mp4\"\n",
    "                 ]\n",
    "\n",
    "still_path = [\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default15.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default16.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default17.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default18.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default19.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default20.mp4\"\n",
    "                 ]\n",
    "\n",
    "moving_data = load_images_from_folders(moving_path,cal_totalframes(moving_path))\n",
    "still_data = load_images_from_folders(still_path,cal_totalframes(still_path))\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1991, 224, 224, 4)\n",
      "(1991,)\n",
      "63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:14:16.808995: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3196813312 exceeds 10% of free system memory.\n"
     ]
    }
   ],
   "source": [
    "#concatenate still and movind data to create inputs and outputs\n",
    "inputs1 = np.concatenate((still_data, moving_data))\n",
    "\n",
    "outputs1 = np.concatenate((np.zeros(len(still_data)),np.ones(len(moving_data))))\n",
    "\n",
    "\n",
    "#shuffle the data to prepare for training \n",
    "shuffled_indices = np.random.permutation(len(inputs1))\n",
    "inputs1, outputs1 = inputs1[shuffled_indices],outputs1[shuffled_indices]\n",
    "\n",
    "#inputs = np.concatenate((inputs,inputs))\n",
    "#outputs = np.concatenate((outputs,outputs))\n",
    "\n",
    "# inputs = inputs.astype(np.float32)\n",
    "# outputs = outputs.astype(np.float32)\n",
    "inputs1 = inputs1 / 255.0\n",
    "outputs1 = outputs1.astype(np.float32)\n",
    "\n",
    "#create a tensorflow dataset\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs1,outputs1))\n",
    "    dataset = dataset.batch(32).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(inputs1.shape)\n",
    "print(outputs1.shape)\n",
    "print(len(dataset))\n",
    "\n",
    "#determine train and test \n",
    "train_data = dataset.take(int(1.0*len(dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 224, 224, 4)       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 222, 222, 32)      1184      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 111, 111, 32)     0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 54, 54, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 26, 26, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 128)              0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               16512     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 118,369\n",
      "Trainable params: 118,369\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input_shape = (2160,224, 224, 4)\n",
    "# input_shape = (1, 224, 224, 4)\n",
    "input_shape = (None, 224, 224, 4)\n",
    "\n",
    "#data augmentation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomContrast(factor=0.1),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomTranslation(0.2, 0.2)\n",
    "\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    # Add Convolutional and Pooling layers\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output of the Convolutional layers before passing to Dense layers\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    # Flatten(),\n",
    "    \n",
    "    # Add Dense layers for classification\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),  # Using sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Print the model summary to verify the input shape\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:21:16.036437: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3196813312 exceeds 10% of free system memory.\n",
      "2023-08-14 17:21:16.774970: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3196813312 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:21:18.760216: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8600\n",
      "2023-08-14 17:21:19.148196: W tensorflow/stream_executor/gpu/asm_compiler.cc:111] *** WARNING *** You are using ptxas 10.1.243, which is older than 11.1. ptxas before 11.1 is known to miscompile XLA code, leading to incorrect results or invalid-address errors.\n",
      "\n",
      "You may not need to update to CUDA 11.1; cherry-picking the ptxas binary is often sufficient.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 7s 83ms/step - loss: 0.5603 - accuracy: 0.7268 - val_loss: 0.6871 - val_accuracy: 0.6394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:21:24.735039: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3196813312 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10\n",
      "63/63 [==============================] - 5s 77ms/step - loss: 0.4656 - accuracy: 0.7951 - val_loss: 0.6455 - val_accuracy: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:21:30.275886: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 3196813312 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10\n",
      "63/63 [==============================] - 5s 78ms/step - loss: 0.4505 - accuracy: 0.8112 - val_loss: 0.8661 - val_accuracy: 0.6049\n",
      "Epoch 4/10\n",
      "63/63 [==============================] - 5s 77ms/step - loss: 0.4364 - accuracy: 0.8172 - val_loss: 0.8043 - val_accuracy: 0.6502\n",
      "Epoch 5/10\n",
      "63/63 [==============================] - 5s 79ms/step - loss: 0.3958 - accuracy: 0.8393 - val_loss: 0.5069 - val_accuracy: 0.7576\n",
      "Epoch 6/10\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.3448 - accuracy: 0.8679 - val_loss: 0.4069 - val_accuracy: 0.8187\n",
      "Epoch 7/10\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.3601 - accuracy: 0.8629 - val_loss: 0.4265 - val_accuracy: 0.7931\n",
      "Epoch 8/10\n",
      "63/63 [==============================] - 5s 76ms/step - loss: 0.3409 - accuracy: 0.8709 - val_loss: 0.4847 - val_accuracy: 0.7862\n",
      "Epoch 9/10\n",
      "63/63 [==============================] - 5s 77ms/step - loss: 0.3328 - accuracy: 0.8724 - val_loss: 0.4177 - val_accuracy: 0.8049\n",
      "Epoch 10/10\n",
      "63/63 [==============================] - 5s 77ms/step - loss: 0.3191 - accuracy: 0.8764 - val_loss: 0.4602 - val_accuracy: 0.7842\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7faed0049b50>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint_filepath = 'tmp/checkpoints'\n",
    "model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "#model.fit(train_data, epochs=10, validation_data=val_data, callbacks=[model_checkpoint_callback])\n",
    "model.fit(train_data, epochs=10, validation_data=tdataset, callbacks=[model_checkpoint_callback])\n",
    "#model.fit(train_data, epochs=5)\n",
    "\n",
    "model.load_weights(checkpoint_filepath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "864 1127\n",
      "<TakeDataset element_spec=(TensorSpec(shape=(None, 224, 224, 4), dtype=tf.float64, name=None), TensorSpec(shape=(None,), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "pos = 0\n",
    "neg = 0\n",
    "for x, y in train_data.unbatch():\n",
    "    #print(x[0][0],y)\n",
    "    if y > 0.5:\n",
    "        pos += 1\n",
    "    else:\n",
    "        neg += 1\n",
    "\n",
    "print(pos, neg)\n",
    "\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "501\n",
      "514\n"
     ]
    }
   ],
   "source": [
    "###TESTING \n",
    "test_moving = [ \"./../datasets/vids/testdata/moving/resized-test-fall1.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall2.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall3.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall4.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall5.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall6.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall7.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall8.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall9.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall10.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall11.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall12.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall13.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall14.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall15.mp4\"\n",
    "]\n",
    "\n",
    "test_still = [\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default1.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default2.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default3.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default4.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default5.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default6.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default7.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default8.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default9.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default10.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default11.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default12.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default13.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default14.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default15.mp4\"]\n",
    "\n",
    "still_tdata = load_images_from_folders(test_still,cal_totalframes(test_still))\n",
    "moving_tdata = load_images_from_folders(test_moving,cal_totalframes(test_moving))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91764706 1.         0.98823529 0.06491765]\n",
      "   ...\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]]\n",
      "\n",
      "  [[0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91764706 1.         0.98823529 0.06491765]\n",
      "   ...\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]]\n",
      "\n",
      "  [[0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91372549 1.         0.98431373 0.06329804]\n",
      "   [0.91764706 1.         0.98823529 0.06491765]\n",
      "   ...\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]\n",
      "   [0.96470588 0.98431373 0.98039216 0.14509804]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.49019608 0.50196078 0.42352941 0.30239608]\n",
      "   [0.46666667 0.47843137 0.4        0.27886667]\n",
      "   [0.41960784 0.43137255 0.35294118 0.23180784]\n",
      "   ...\n",
      "   [0.57254902 0.56862745 0.54117647 0.02717255]\n",
      "   [0.57254902 0.56862745 0.54117647 0.02717255]\n",
      "   [0.57254902 0.56862745 0.54117647 0.02717255]]\n",
      "\n",
      "  [[0.54117647 0.55294118 0.4745098  0.35729804]\n",
      "   [0.5372549  0.54901961 0.47058824 0.35337647]\n",
      "   [0.52941176 0.54117647 0.4627451  0.34553333]\n",
      "   ...\n",
      "   [0.56470588 0.56078431 0.53333333 0.03893725]\n",
      "   [0.56862745 0.56470588 0.5372549  0.03501569]\n",
      "   [0.56862745 0.56470588 0.5372549  0.03501569]]\n",
      "\n",
      "  [[0.52156863 0.53333333 0.45490196 0.3376902 ]\n",
      "   [0.52941176 0.54117647 0.4627451  0.34553333]\n",
      "   [0.52941176 0.54117647 0.4627451  0.34553333]\n",
      "   ...\n",
      "   [0.56078431 0.55686275 0.52941176 0.04678039]\n",
      "   [0.56470588 0.56078431 0.53333333 0.04285882]\n",
      "   [0.56470588 0.56078431 0.53333333 0.04285882]]]\n",
      "\n",
      "\n",
      " [[[0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   ...\n",
      "   [0.80392157 0.76862745 0.74509804 0.        ]\n",
      "   [0.90588235 0.87058824 0.84705882 0.        ]\n",
      "   [0.96862745 0.93333333 0.90980392 0.        ]]\n",
      "\n",
      "  [[0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   ...\n",
      "   [0.69411765 0.65882353 0.63529412 0.        ]\n",
      "   [0.70588235 0.67058824 0.64705882 0.        ]\n",
      "   [0.72156863 0.68627451 0.6627451  0.        ]]\n",
      "\n",
      "  [[0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   [0.75686275 0.75686275 0.75686275 0.        ]\n",
      "   ...\n",
      "   [0.70196078 0.66666667 0.64313725 0.        ]\n",
      "   [0.69019608 0.65490196 0.63137255 0.        ]\n",
      "   [0.68235294 0.64705882 0.62352941 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.40392157 0.40784314 0.37647059 0.        ]\n",
      "   [0.40392157 0.40784314 0.37647059 0.        ]\n",
      "   [0.4        0.40392157 0.37254902 0.        ]\n",
      "   ...\n",
      "   [0.43137255 0.43137255 0.43137255 0.        ]\n",
      "   [0.38431373 0.38431373 0.38431373 0.        ]\n",
      "   [0.18431373 0.18431373 0.18431373 0.        ]]\n",
      "\n",
      "  [[0.40392157 0.40784314 0.37647059 0.        ]\n",
      "   [0.4        0.40392157 0.37254902 0.        ]\n",
      "   [0.4        0.40392157 0.37254902 0.        ]\n",
      "   ...\n",
      "   [0.43529412 0.43529412 0.43529412 0.        ]\n",
      "   [0.38039216 0.38039216 0.38039216 0.        ]\n",
      "   [0.17254902 0.17254902 0.17254902 0.        ]]\n",
      "\n",
      "  [[0.4        0.40392157 0.37254902 0.        ]\n",
      "   [0.4        0.40392157 0.37254902 0.        ]\n",
      "   [0.4        0.40392157 0.37254902 0.        ]\n",
      "   ...\n",
      "   [0.43529412 0.43529412 0.43529412 0.        ]\n",
      "   [0.38039216 0.38039216 0.38039216 0.        ]\n",
      "   [0.17254902 0.17254902 0.17254902 0.        ]]]\n",
      "\n",
      "\n",
      " [[[0.32941176 0.4        0.39607843 0.01035686]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   ...\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]]\n",
      "\n",
      "  [[0.32941176 0.4        0.39607843 0.01035686]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   ...\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]]\n",
      "\n",
      "  [[0.32941176 0.4        0.39607843 0.01035686]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   [0.3254902  0.39607843 0.39215686 0.00643529]\n",
      "   ...\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]\n",
      "   [0.56470588 0.56470588 0.54509804 0.        ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.00392157]\n",
      "   ...\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]]\n",
      "\n",
      "  [[0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.00392157]\n",
      "   ...\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]]\n",
      "\n",
      "  [[0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.        ]\n",
      "   [0.3372549  0.38039216 0.38431373 0.00392157]\n",
      "   ...\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]\n",
      "   [0.09019608 0.13333333 0.12941176 0.00145098]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   ...\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]]\n",
      "\n",
      "  [[0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   ...\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]]\n",
      "\n",
      "  [[0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   [0.98823529 0.98823529 0.98823529 0.00392157]\n",
      "   ...\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]\n",
      "   [0.62352941 0.63137255 0.63137255 0.05724706]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.54117647 0.54117647 0.52156863 0.00784314]\n",
      "   [0.54117647 0.54117647 0.52156863 0.00784314]\n",
      "   [0.54117647 0.54117647 0.52156863 0.00784314]\n",
      "   ...\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]]\n",
      "\n",
      "  [[0.54117647 0.54117647 0.52156863 0.00392157]\n",
      "   [0.54117647 0.54117647 0.52156863 0.00392157]\n",
      "   [0.54117647 0.54117647 0.52156863 0.00392157]\n",
      "   ...\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]]\n",
      "\n",
      "  [[0.5372549  0.5372549  0.51764706 0.00784314]\n",
      "   [0.5372549  0.5372549  0.51764706 0.00784314]\n",
      "   [0.5372549  0.5372549  0.51764706 0.00784314]\n",
      "   ...\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]\n",
      "   [0.56078431 0.56078431 0.54117647 0.04313725]]]\n",
      "\n",
      "\n",
      " [[[0.65882353 0.65098039 0.63529412 0.05238824]\n",
      "   [0.65882353 0.65098039 0.63529412 0.05238824]\n",
      "   [0.65882353 0.65098039 0.63529412 0.05238824]\n",
      "   ...\n",
      "   [0.8        0.79215686 0.82352941 0.18623529]\n",
      "   [0.78431373 0.77647059 0.80784314 0.20192157]\n",
      "   [0.77254902 0.76470588 0.79607843 0.21368627]]\n",
      "\n",
      "  [[0.65882353 0.65098039 0.63529412 0.04846667]\n",
      "   [0.65882353 0.65098039 0.63529412 0.04846667]\n",
      "   [0.65882353 0.65098039 0.63529412 0.04846667]\n",
      "   ...\n",
      "   [0.8        0.79215686 0.82352941 0.18623529]\n",
      "   [0.78431373 0.77647059 0.80784314 0.20192157]\n",
      "   [0.76862745 0.76078431 0.79215686 0.21760784]]\n",
      "\n",
      "  [[0.65882353 0.65098039 0.63529412 0.0485098 ]\n",
      "   [0.65882353 0.65098039 0.63529412 0.0485098 ]\n",
      "   [0.65882353 0.65098039 0.63529412 0.0485098 ]\n",
      "   ...\n",
      "   [0.79215686 0.78431373 0.81568627 0.19407843]\n",
      "   [0.77254902 0.76470588 0.79607843 0.21368627]\n",
      "   [0.76470588 0.75686275 0.78823529 0.22152941]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.27058824 0.27058824 0.30980392 0.06713725]\n",
      "   [0.27058824 0.27058824 0.30980392 0.06321569]\n",
      "   [0.2627451  0.2745098  0.30980392 0.05933725]\n",
      "   ...\n",
      "   [0.56078431 0.56470588 0.53333333 0.0627451 ]\n",
      "   [0.54901961 0.55294118 0.52156863 0.08235294]\n",
      "   [0.54509804 0.54901961 0.51764706 0.08627451]]\n",
      "\n",
      "  [[0.2745098  0.2745098  0.31372549 0.06321569]\n",
      "   [0.2745098  0.2745098  0.31372549 0.05929412]\n",
      "   [0.27058824 0.28235294 0.31764706 0.05149412]\n",
      "   ...\n",
      "   [0.55686275 0.56078431 0.52941176 0.08235294]\n",
      "   [0.54901961 0.55294118 0.52156863 0.09019608]\n",
      "   [0.54509804 0.54901961 0.51764706 0.09411765]]\n",
      "\n",
      "  [[0.27843137 0.27843137 0.31764706 0.05537255]\n",
      "   [0.28235294 0.28235294 0.32156863 0.04752941]\n",
      "   [0.2745098  0.28627451 0.32156863 0.04757255]\n",
      "   ...\n",
      "   [0.56470588 0.56862745 0.5372549  0.0745098 ]\n",
      "   [0.56470588 0.56862745 0.5372549  0.0745098 ]\n",
      "   [0.56470588 0.56862745 0.5372549  0.0745098 ]]]\n",
      "\n",
      "\n",
      " [[[0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   ...\n",
      "   [0.22352941 0.16078431 0.22745098 0.17930196]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]]\n",
      "\n",
      "  [[0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   ...\n",
      "   [0.22352941 0.16078431 0.22745098 0.17930196]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]]\n",
      "\n",
      "  [[0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   [0.25882353 0.17647059 0.24313725 0.20085098]\n",
      "   ...\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]\n",
      "   [0.23137255 0.16862745 0.23529412 0.1871451 ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[0.40784314 0.34117647 0.41960784 0.35828627]\n",
      "   [0.4        0.33333333 0.41176471 0.35044314]\n",
      "   [0.38039216 0.31372549 0.39215686 0.33083529]\n",
      "   ...\n",
      "   [0.24313725 0.18039216 0.23921569 0.19017255]\n",
      "   [0.24313725 0.18039216 0.23921569 0.19017255]\n",
      "   [0.24705882 0.18431373 0.24313725 0.20193725]]\n",
      "\n",
      "  [[0.43529412 0.36470588 0.4627451  0.38522353]\n",
      "   [0.42745098 0.35686275 0.45490196 0.37738039]\n",
      "   [0.41176471 0.34117647 0.43921569 0.36169412]\n",
      "   ...\n",
      "   [0.24313725 0.18039216 0.23137255 0.18927843]\n",
      "   [0.24705882 0.18431373 0.23529412 0.1932    ]\n",
      "   [0.24705882 0.18431373 0.23529412 0.20104314]]\n",
      "\n",
      "  [[0.44705882 0.37647059 0.4745098  0.39698824]\n",
      "   [0.43921569 0.36862745 0.46666667 0.3891451 ]\n",
      "   [0.42745098 0.35686275 0.45490196 0.37738039]\n",
      "   ...\n",
      "   [0.24705882 0.18431373 0.23529412 0.1932    ]\n",
      "   [0.25098039 0.18823529 0.23921569 0.19712157]\n",
      "   [0.25098039 0.18823529 0.23921569 0.20496471]]]]\n",
      "[1. 0. 0. ... 1. 0. 1.]\n"
     ]
    }
   ],
   "source": [
    "#concatenate still and movind data to create inputs and outputs\n",
    "tinputs = np.concatenate((still_tdata, moving_tdata))\n",
    "toutputs = np.concatenate((np.zeros(len(still_tdata)),np.ones(len(moving_tdata))))\n",
    "\n",
    "# tinputs = tinputs.astype(np.float32)\n",
    "# toutputs = toutputs.astype(np.float32)\n",
    "\n",
    "tinputs = tinputs / 255.0\n",
    "toutputs = toutputs.astype(np.float32)\n",
    "#shuffle the data to prepare for training \n",
    "tshuffled_indices = np.random.permutation(len(tinputs))\n",
    "tinputs, toutputs = tinputs[tshuffled_indices],toutputs[tshuffled_indices]\n",
    "\n",
    "print(tinputs)\n",
    "print(toutputs)\n",
    "\n",
    "#create a tensorflow dataset\n",
    "with tf.device('/cpu:0'):\n",
    "    tdataset = tf.data.Dataset.from_tensor_slices((tinputs,toutputs))\n",
    "    tdataset = tdataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64/64 [==============================] - 2s 16ms/step - loss: 0.4069 - accuracy: 0.8187\n"
     ]
    }
   ],
   "source": [
    "# model.load_weights(checkpoint_filepath)\n",
    "loss, accuracy = model.evaluate(tdataset)\n",
    "#loss, accuracy = model.evaluate(train_data.take(100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### save and quantize model, turn into tflite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-14 17:22:17.125206: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ktest_model1/assets\n"
     ]
    }
   ],
   "source": [
    "#save model1 \n",
    "model.save(\"ktest_model1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpqvh9ir_4/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-08-14 17:22:20.930092: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-08-14 17:22:20.930115: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-08-14 17:22:20.930638: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpqvh9ir_4\n",
      "2023-08-14 17:22:20.934909: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-08-14 17:22:20.934921: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpqvh9ir_4\n",
      "2023-08-14 17:22:20.952388: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-08-14 17:22:21.007052: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpqvh9ir_4\n",
      "2023-08-14 17:22:21.030305: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 99669 microseconds.\n",
      "2023-08-14 17:22:21.079895: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "def representative_dataset():\n",
    "    for data, label in tf.data.Dataset.from_tensor_slices((inputs1, outputs1)).batch(1).take(100):\n",
    "        yield [tf.dtypes.cast(data, tf.float32)]\n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    converter.representative_dataset = representative_dataset\n",
    "    converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "    converter.inference_input_type = tf.uint8\n",
    "    converter.inference_output_type = tf.uint8\n",
    "    tflite_quant_m1 = converter.convert()\n",
    "\n",
    "    # Save the quantized model 1\n",
    "    with open('k_m1_quant_v6.tflite', 'wb') as f: f.write(tflite_quant_m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code to separate inputs of each video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_path = [\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall15.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall16.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall17.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall18.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall19.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall20.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall21.mp4\"\n",
    "                 ]\n",
    "\n",
    "still_path = [\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default15.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default16.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default17.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default18.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default19.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default20.mp4\"]\n",
    "\n",
    "\n",
    "test_moving = [ \"./../datasets/vids/testdata/moving/resized-test-fall1.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall2.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall3.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall4.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall5.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall6.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall7.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall8.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall9.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall10.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall11.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall12.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall13.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall14.mp4\",\n",
    "                \"./../datasets/vids/testdata/moving/resized-test-fall15.mp4\"\n",
    "]\n",
    "\n",
    "test_still = [\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default1.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default2.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default3.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default4.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default5.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default6.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default7.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default8.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default9.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default10.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default11.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default12.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default13.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default14.mp4\",\n",
    "                \"./../datasets/vids/testdata/still/resized-test-default15.mp4\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_774241/3015909937.py:15: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(separated_arrays)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(21,)\n",
      "(20,)\n",
      "(15,)\n",
      "(15,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cal_total_idvFrames(vid):\n",
    "    total_frame = len(cut_video(vid))\n",
    "    return total_frame\n",
    "\n",
    "def separate_aggregated_images(aggregated_images, num_images_per_video):\n",
    "    separated_arrays = []\n",
    "    start_idx = 0\n",
    "\n",
    "    for num_images in num_images_per_video:\n",
    "        end_idx = start_idx + num_images\n",
    "        video_images = aggregated_images[start_idx:end_idx]\n",
    "        separated_arrays.append(video_images)\n",
    "        start_idx = end_idx\n",
    "\n",
    "    return np.array(separated_arrays)\n",
    "\n",
    "def separate_inputs(inputs,path):\n",
    "    list_total_of_each_vid = []\n",
    "\n",
    "    for i in range(len(path)):\n",
    "        list_total_of_each_vid.append(cal_total_idvFrames(path[i]))\n",
    "    \n",
    "    #once obtained total of each vid, separate each\n",
    "    updated_inputs = separate_aggregated_images(inputs,list_total_of_each_vid)\n",
    "    return np.array(updated_inputs)\n",
    "\n",
    "\n",
    "separated_moving_inputs = separate_inputs(moving_data,moving_path)\n",
    "separated_still_inputs = separate_inputs(still_data,still_path)\n",
    "\n",
    "\n",
    "separated_moving_test = separate_inputs(moving_tdata, test_moving)\n",
    "separated_still_test = separate_inputs(still_tdata, test_still)\n",
    "\n",
    "print(separated_moving_inputs.shape)\n",
    "print(separated_still_inputs.shape)\n",
    "print(separated_moving_test.shape)\n",
    "print(separated_still_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use model 1 to perform prediction and generate dataset for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 52\u001b[0m\n\u001b[1;32m     48\u001b[0m falling_frames \u001b[39m=\u001b[39m []\n\u001b[1;32m     49\u001b[0m default_frames \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 52\u001b[0m falling_frames_test \u001b[39m=\u001b[39m batchPredictMultVids(separated_moving_test)\n\u001b[1;32m     53\u001b[0m default_frames_test \u001b[39m=\u001b[39m batchPredictMultVids(separated_still_test)\n\u001b[1;32m     54\u001b[0m falling_frames \u001b[39m=\u001b[39m batchPredictMultVids(separated_moving_inputs)\n",
      "Cell \u001b[0;32mIn[19], line 38\u001b[0m, in \u001b[0;36mbatchPredictMultVids\u001b[0;34m(list_of_images_of_all_vids)\u001b[0m\n\u001b[1;32m     36\u001b[0m record \u001b[39m=\u001b[39m []\n\u001b[1;32m     37\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(list_of_images_of_all_vids)):\n\u001b[0;32m---> 38\u001b[0m     vid_probs \u001b[39m=\u001b[39m batchPredict(list_of_images_of_all_vids[i])\n\u001b[1;32m     39\u001b[0m     \u001b[39m#append sliding frames of 8 into the record\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[39mfor\u001b[39;00m f \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(vid_probs)\u001b[39m-\u001b[39m\u001b[39m7\u001b[39m):\n",
      "Cell \u001b[0;32mIn[19], line 25\u001b[0m, in \u001b[0;36mbatchPredict\u001b[0;34m(list_of_images_of_a_singlevid)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[39mfor\u001b[39;00m instance \u001b[39min\u001b[39;00m list_of_images_of_a_singlevid:\n\u001b[1;32m     24\u001b[0m     intepreter\u001b[39m.\u001b[39mset_tensor(input_index, np\u001b[39m.\u001b[39mexpand_dims(instance,axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m))\n\u001b[0;32m---> 25\u001b[0m     intepreter\u001b[39m.\u001b[39;49minvoke()\n\u001b[1;32m     26\u001b[0m     output_data \u001b[39m=\u001b[39m intepreter\u001b[39m.\u001b[39mget_tensor(output_index)\n\u001b[1;32m     27\u001b[0m     output_data \u001b[39m=\u001b[39m output_data[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/lite/python/interpreter.py:916\u001b[0m, in \u001b[0;36mInterpreter.invoke\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    904\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Invoke the interpreter.\u001b[39;00m\n\u001b[1;32m    905\u001b[0m \n\u001b[1;32m    906\u001b[0m \u001b[39mBe sure to set the input sizes, allocate tensors and fill values before\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[39m  ValueError: When the underlying interpreter fails raise ValueError.\u001b[39;00m\n\u001b[1;32m    914\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    915\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_safe()\n\u001b[0;32m--> 916\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_interpreter\u001b[39m.\u001b[39;49mInvoke()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.device('/cpu:0'):\n",
    "\n",
    "    def batchPredict(list_of_images_of_a_singlevid):\n",
    "        probs = []\n",
    "\n",
    "        # define intepreter for quantized modell\n",
    "        intepreter = tf.lite.Interpreter(model_path=\"k_m1_quant_v5.tflite\")\n",
    "        intepreter.allocate_tensors()\n",
    "        #get outputs and inputs details\n",
    "        output = intepreter.get_output_details()\n",
    "        input = intepreter.get_input_details()\n",
    "        output_index = output[0]['index']\n",
    "        input_index = input[0]['index']\n",
    "        \n",
    "     \n",
    "        # output_index = output[0]['index']\n",
    "        # input_index = input[0]['index']\n",
    "        \n",
    "        #set input ->invoke -> access outputs\n",
    "        list_of_images_of_a_singlevid = np.array(list_of_images_of_a_singlevid, dtype=np.uint8)\n",
    "\n",
    "        #run through model and predict the output\n",
    "        for instance in list_of_images_of_a_singlevid:\n",
    "            intepreter.set_tensor(input_index, np.expand_dims(instance,axis=0))\n",
    "            intepreter.invoke()\n",
    "            output_data = intepreter.get_tensor(output_index)\n",
    "            output_data = output_data[0][0]\n",
    "            # print(output_data)\n",
    "            probs.append(output_data)\n",
    "        \n",
    "        return probs\n",
    "            \n",
    "        \n",
    "\n",
    "    def batchPredictMultVids(list_of_images_of_all_vids):\n",
    "        record = []\n",
    "        for i in range(len(list_of_images_of_all_vids)):\n",
    "            vid_probs = batchPredict(list_of_images_of_all_vids[i])\n",
    "            #append sliding frames of 8 into the record\n",
    "            for f in range(len(vid_probs)-7):\n",
    "                record.append(vid_probs[f:f+8])\n",
    "            print(len(record))\n",
    "        return np.array(record)\n",
    "            \n",
    "    \n",
    "\n",
    "with tf.device('/cpu:0'):\n",
    "    falling_frames = []\n",
    "    default_frames = []\n",
    "\n",
    "    \n",
    "    falling_frames_test = batchPredictMultVids(separated_moving_test)\n",
    "    default_frames_test = batchPredictMultVids(separated_still_test)\n",
    "    falling_frames = batchPredictMultVids(separated_moving_inputs)\n",
    "    default_frames = batchPredictMultVids(separated_still_inputs)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save output from m1 and later load in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "np.savetxt('model1_output_default.txt', default_frames)\n",
    "np.savetxt('model1_output_fall.txt', falling_frames)\n",
    "np.savetxt('model1_output_default_test.txt', default_frames_test)\n",
    "np.savetxt('model1_output_fall_test.txt', falling_frames_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 49.  49.  18. ...  42. 252. 251.]\n",
      " [ 49.  18.  49. ... 252. 251.  76.]\n",
      " [ 18.  49.  17. ... 251.  76. 255.]\n",
      " ...\n",
      " [255. 255. 255. ... 254. 254. 254.]\n",
      " [255. 255. 255. ... 254. 254. 236.]\n",
      " [255. 255. 255. ... 254. 236. 128.]]\n",
      "[[29. 29. 10. ... 14. 12. 12.]\n",
      " [29. 10. 31. ... 12. 12. 17.]\n",
      " [10. 31.  9. ... 12. 17. 18.]\n",
      " ...\n",
      " [96. 31. 25. ... 29. 34. 42.]\n",
      " [31. 25. 36. ... 34. 42. 27.]\n",
      " [25. 36. 25. ... 42. 27. 86.]]\n"
     ]
    }
   ],
   "source": [
    "default_frames = np.loadtxt('model1_output_default.txt')\n",
    "falling_frames = np.loadtxt('model1_output_fall.txt')\n",
    "default_frames_test = np.loadtxt('model1_output_default_test.txt')\n",
    "falling_frames_test = np.loadtxt('model1_output_fall_test.txt')\n",
    "\n",
    "print(falling_frames_test)\n",
    "print(default_frames_test)\n",
    "\n",
    "#dont uncomment this\n",
    "# for i in range(len(default_frames_test)):\n",
    "#     print(falling_frames_test[i])\n",
    "\n",
    "\n",
    "default_frames = default_frames.astype(np.uint8)\n",
    "falling_frames = falling_frames.astype(np.uint8)\n",
    "default_frames_test = default_frames_test.astype(np.uint8)\n",
    "falling_frames_test = falling_frames_test.astype(np.uint8)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1723, 8) (1723,)\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "inputs = np.concatenate((default_frames, default_frames_test,falling_frames_test,falling_frames))\n",
    "outputs = np.concatenate((np.zeros(len(default_frames)),np.zeros(len(default_frames_test)),np.ones(len(falling_frames_test)), np.ones(len(falling_frames)))) #ones are falling , zeros are default\n",
    "dataset_size = len(inputs)\n",
    "new_indices = np.random.permutation(dataset_size) # shuffle indices to shuffle X and y at the same time\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices]\n",
    "\n",
    "train_size = int(1*dataset_size)\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_train, y_train = inputs[:train_size], outputs[:train_size] #x = images, y = label \n",
    "\n",
    "print(X_train.shape,y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(805, 8) (805,)\n"
     ]
    }
   ],
   "source": [
    "#test data\n",
    "inputs = np.concatenate((default_frames_test, falling_frames_test))\n",
    "unshuffled_frames = inputs\n",
    "outputs = np.concatenate((np.zeros(len(default_frames_test)), np.ones(len(falling_frames_test)))) #ones are falling , zeros are default\n",
    "dataset_size = len(inputs)\n",
    "new_indices = np.random.permutation(dataset_size) # shuffle indices to shuffle X and y at the same time\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices]\n",
    "\n",
    "train_size = int(1*dataset_size)\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_test, y_test = inputs[:train_size], outputs[:train_size] #x = images, y = label \n",
    "\n",
    "print(X_test.shape,y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train M2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(8,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54/54 [==============================] - 0s 3ms/step - loss: 15.9393 - accuracy: 0.5038 - val_loss: 2.6129 - val_accuracy: 0.6348\n",
      "Epoch 2/15\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 1.7696 - accuracy: 0.6030 - val_loss: 1.0539 - val_accuracy: 0.6894\n",
      "Epoch 3/15\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.8448 - accuracy: 0.6901 - val_loss: 0.6568 - val_accuracy: 0.7205\n",
      "Epoch 4/15\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.6736 - accuracy: 0.7609 - val_loss: 0.4505 - val_accuracy: 0.8745\n",
      "Epoch 5/15\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.4800 - accuracy: 0.8154 - val_loss: 0.3562 - val_accuracy: 0.8683\n",
      "Epoch 6/15\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4229 - accuracy: 0.8543 - val_loss: 0.4450 - val_accuracy: 0.8050\n",
      "Epoch 7/15\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.4071 - accuracy: 0.8584 - val_loss: 0.3568 - val_accuracy: 0.9217\n",
      "Epoch 8/15\n",
      "54/54 [==============================] - 0s 5ms/step - loss: 0.3550 - accuracy: 0.8764 - val_loss: 0.2489 - val_accuracy: 0.9081\n",
      "Epoch 9/15\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.3126 - accuracy: 0.8961 - val_loss: 0.2416 - val_accuracy: 0.9217\n",
      "Epoch 10/15\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3227 - accuracy: 0.8810 - val_loss: 0.3918 - val_accuracy: 0.8646\n",
      "Epoch 11/15\n",
      "54/54 [==============================] - 0s 2ms/step - loss: 0.3421 - accuracy: 0.8723 - val_loss: 0.2295 - val_accuracy: 0.9180\n",
      "Epoch 12/15\n",
      "54/54 [==============================] - 0s 3ms/step - loss: 0.2658 - accuracy: 0.9095 - val_loss: 0.1809 - val_accuracy: 0.9516\n",
      "Epoch 13/15\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2744 - accuracy: 0.8984 - val_loss: 0.2073 - val_accuracy: 0.9255\n",
      "Epoch 14/15\n",
      "54/54 [==============================] - 0s 4ms/step - loss: 0.2349 - accuracy: 0.9240 - val_loss: 0.2319 - val_accuracy: 0.9516\n",
      "Epoch 15/15\n",
      "54/54 [==============================] - 0s 6ms/step - loss: 0.2275 - accuracy: 0.9205 - val_loss: 0.1779 - val_accuracy: 0.9354\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f034cd17280>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=15,callbacks=[tensorboard_callback], validation_data=(X_test,y_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26/26 [==============================] - 0s 953us/step - loss: 0.1779 - accuracy: 0.9354\n",
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxn46en05/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpxn46en05/assets\n",
      "2023-08-14 11:03:07.382892: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-08-14 11:03:07.382911: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-08-14 11:03:07.383051: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpxn46en05\n",
      "2023-08-14 11:03:07.383692: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-08-14 11:03:07.383703: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpxn46en05\n",
      "2023-08-14 11:03:07.385584: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-08-14 11:03:07.407334: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpxn46en05\n",
      "2023-08-14 11:03:07.414219: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 31155 microseconds.\n"
     ]
    }
   ],
   "source": [
    "### CONVERT TO TFLITE\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model2_tflite = converter.convert()\n",
    "with open('updated_kmodel2.tflite', 'wb') as f: f.write(model2_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo4l0ulw1/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpo4l0ulw1/assets\n",
      "/home/kevin/.local/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-08-14 11:03:17.997773: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-08-14 11:03:17.997792: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-08-14 11:03:17.997958: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpo4l0ulw1\n",
      "2023-08-14 11:03:17.998563: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-08-14 11:03:17.998573: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpo4l0ulw1\n",
      "2023-08-14 11:03:18.000368: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-08-14 11:03:18.022526: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpo4l0ulw1\n",
      "2023-08-14 11:03:18.029388: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 31431 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "### QUANTIZE\n",
    "def representative_dataset():\n",
    "  for d in inputs:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('k_m2_quant_v5.tflite', 'wb') as f: f.write(tflite_quant_model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define intepreter for quantized modell\n",
    "intepreter = tf.lite.Interpreter(model_path=\"k_m2_quant_v5.tflite\")\n",
    "intepreter.allocate_tensors()\n",
    "#get outputs and inputs details\n",
    "output = intepreter.get_output_details()\n",
    "input = intepreter.get_input_details()\n",
    "output_index = output[0]['index']\n",
    "input_index = input[0]['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[255 255 255 ... 254 236 128]\n",
      " [133 133 150 ... 144 217 254]\n",
      " [112  96  36 ...  59 200  96]\n",
      " ...\n",
      " [ 59  52  59 ...  68  56  91]\n",
      " [239  86  96 ... 117 150  96]\n",
      " [217 255 255 ... 255 255 255]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[29 29 10 31  9 14 12 12]\n",
      "[29 10 31  9 14 12 12 17]\n",
      "[10 31  9 14 12 12 17 18]\n",
      "[31  9 14 12 12 17 18 17]\n",
      "[ 9 14 12 12 17 18 17 21]\n",
      "[14 12 12 17 18 17 21 36]\n",
      "[12 12 17 18 17 21 36 18]\n",
      "[12 17 18 17 21 36 18 14]\n",
      "[17 18 17 21 36 18 14 20]\n",
      "[18 17 21 36 18 14 20 72]\n",
      "[17 21 36 18 14 20 72 63]\n",
      "[21 36 18 14 20 72 63 36]\n",
      "[36 18 14 20 72 63 36 11]\n",
      "[18 14 20 72 63 36 11 39]\n",
      "[14 20 72 63 36 11 39 18]\n",
      "[20 72 63 36 11 39 18 11]\n",
      "[72 63 36 11 39 18 11 31]\n",
      "[63 36 11 39 18 11 31 12]\n",
      "[36 11 39 18 11 31 12 14]\n",
      "[11 39 18 11 31 12 14 20]\n",
      "[39 18 11 31 12 14 20 16]\n",
      "[18 11 31 12 14 20 16 14]\n",
      "[11 31 12 14 20 16 14 10]\n",
      "[31 12 14 20 16 14 10 10]\n",
      "[12 14 20 16 14 10 10 10]\n",
      "[14 20 16 14 10 10 10 16]\n",
      "[45 49 45 86 86 31 42 36]\n",
      "[49 45 86 86 31 42 36 45]\n",
      "[45 86 86 31 42 36 45 36]\n",
      "[86 86 31 42 36 45 36 39]\n",
      "[86 31 42 36 45 36 39 36]\n",
      "[31 42 36 45 36 39 36 34]\n",
      "[ 42  36  45  36  39  36  34 204]\n",
      "[ 36  45  36  39  36  34 204 144]\n",
      "[ 45  36  39  36  34 204 144  56]\n",
      "[ 36  39  36  34 204 144  56  27]\n",
      "[ 39  36  34 204 144  56  27  27]\n",
      "[ 36  34 204 144  56  27  27  25]\n",
      "[ 34 204 144  56  27  27  25  23]\n",
      "[204 144  56  27  27  25  23  36]\n",
      "[144  56  27  27  25  23  36  49]\n",
      "[56 27 27 25 23 36 49 36]\n",
      "[27 27 25 23 36 49 36 52]\n",
      "[27 25 23 36 49 36 52 49]\n",
      "[25 23 36 49 36 52 49 45]\n",
      "[23 36 49 36 52 49 45 27]\n",
      "[36 49 36 52 49 45 27 31]\n",
      "[49 36 52 49 45 27 31 29]\n",
      "[36 52 49 45 27 31 29 27]\n",
      "[52 49 45 27 31 29 27 45]\n",
      "[49 45 27 31 29 27 45 39]\n",
      "[45 27 31 29 27 45 39 31]\n",
      "[36 36 39 39 25 31 31 31]\n",
      "[36 39 39 25 31 31 31 31]\n",
      "[39 39 25 31 31 31 31 34]\n",
      "[39 25 31 31 31 31 34 34]\n",
      "[25 31 31 31 31 34 34 36]\n",
      "[31 31 31 31 34 34 36 34]\n",
      "[31 31 31 34 34 36 34 34]\n",
      "[31 31 34 34 36 34 34 39]\n",
      "[31 34 34 36 34 34 39 68]\n",
      "[34 34 36 34 34 39 68 36]\n",
      "[34 36 34 34 39 68 36 36]\n",
      "[36 34 34 39 68 36 36 34]\n",
      "[34 34 39 68 36 36 34 42]\n",
      "[34 39 68 36 36 34 42 56]\n",
      "[39 68 36 36 34 42 56 76]\n",
      "[68 36 36 34 42 56 76 68]\n",
      "[36 36 34 42 56 76 68 56]\n",
      "[ 36  34  42  56  76  68  56 112]\n",
      "[ 34  42  56  76  68  56 112 180]\n",
      "[ 42  56  76  68  56 112 180 165]\n",
      "[ 56  76  68  56 112 180 165 160]\n",
      "[ 76  68  56 112 180 165 160  96]\n",
      "[ 68  56 112 180 165 160  96  68]\n",
      "[ 56 112 180 165 160  96  68 101]\n",
      "[112 180 165 160  96  68 101  63]\n",
      "[123 117  81 150  91  91 117  81]\n",
      "[117  81 150  91  91 117  81 180]\n",
      "[ 81 150  91  91 117  81 180  72]\n",
      "[150  91  91 117  81 180  72 101]\n",
      "[ 91  91 117  81 180  72 101  52]\n",
      "[ 91 117  81 180  72 101  52 101]\n",
      "[117  81 180  72 101  52 101  68]\n",
      "[ 81 180  72 101  52 101  68 112]\n",
      "[180  72 101  52 101  68 112  72]\n",
      "[ 72 101  52 101  68 112  72  76]\n",
      "[101  52 101  68 112  72  76 139]\n",
      "[ 52 101  68 112  72  76 139 155]\n",
      "[101  68 112  72  76 139 155 247]\n",
      "[ 68 112  72  76 139 155 247 239]\n",
      "[112  72  76 139 155 247 239  86]\n",
      "[ 72  76 139 155 247 239  86  96]\n",
      "[ 76 139 155 247 239  86  96 106]\n",
      "[139 155 247 239  86  96 106 133]\n",
      "[155 247 239  86  96 106 133 117]\n",
      "[247 239  86  96 106 133 117 150]\n",
      "[239  86  96 106 133 117 150  96]\n",
      "[ 86  96 106 133 117 150  96 117]\n",
      "[ 96 106 133 117 150  96 117 238]\n",
      "[106 133 117 150  96 117 238 252]\n",
      "[133 117 150  96 117 238 252 247]\n",
      "[117 150  96 117 238 252 247 251]\n",
      "[56 52 16 34 21 21 20 18]\n",
      "[52 16 34 21 21 20 18 17]\n",
      "[16 34 21 21 20 18 17 27]\n",
      "[34 21 21 20 18 17 27 42]\n",
      "[21 21 20 18 17 27 42 42]\n",
      "[21 20 18 17 27 42 42 63]\n",
      "[20 18 17 27 42 42 63 68]\n",
      "[18 17 27 42 42 63 68 20]\n",
      "[17 27 42 42 63 68 20 39]\n",
      "[27 42 42 63 68 20 39 63]\n",
      "[42 42 63 68 20 39 63 34]\n",
      "[42 63 68 20 39 63 34 25]\n",
      "[63 68 20 39 63 34 25 17]\n",
      "[68 20 39 63 34 25 17 27]\n",
      "[20 39 63 34 25 17 27 25]\n",
      "[39 63 34 25 17 27 25 23]\n",
      "[63 34 25 17 27 25 23 17]\n",
      "[34 25 17 27 25 23 17 21]\n",
      "[25 17 27 25 23 17 21 14]\n",
      "[17 27 25 23 17 21 14 17]\n",
      "[27 25 23 17 21 14 17 17]\n",
      "[25 23 17 21 14 17 17 29]\n",
      "[23 17 21 14 17 17 29 21]\n",
      "[17 21 14 17 17 29 21 29]\n",
      "[21 14 17 17 29 21 29 36]\n",
      "[14 17 17 29 21 29 36 23]\n",
      "[42 42 25 59 29 23 20 27]\n",
      "[42 25 59 29 23 20 27 20]\n",
      "[25 59 29 23 20 27 20 29]\n",
      "[59 29 23 20 27 20 29 23]\n",
      "[29 23 20 27 20 29 23 23]\n",
      "[23 20 27 20 29 23 23 23]\n",
      "[20 27 20 29 23 23 23 18]\n",
      "[27 20 29 23 23 23 18 20]\n",
      "[20 29 23 23 23 18 20 45]\n",
      "[29 23 23 23 18 20 45 96]\n",
      "[23 23 23 18 20 45 96 56]\n",
      "[23 23 18 20 45 96 56 29]\n",
      "[ 23  18  20  45  96  56  29 144]\n",
      "[ 18  20  45  96  56  29 144  52]\n",
      "[ 20  45  96  56  29 144  52  31]\n",
      "[ 45  96  56  29 144  52  31  27]\n",
      "[ 96  56  29 144  52  31  27  27]\n",
      "[ 56  29 144  52  31  27  27  45]\n",
      "[ 29 144  52  31  27  27  45  31]\n",
      "[144  52  31  27  27  45  31  27]\n",
      "[52 31 27 27 45 31 27 31]\n",
      "[31 27 27 45 31 27 31 56]\n",
      "[27 27 45 31 27 31 56 42]\n",
      "[27 45 31 27 31 56 42 34]\n",
      "[45 31 27 31 56 42 34 81]\n",
      "[31 27 31 56 42 34 81 42]\n",
      "[63 63 34 68 31 31 36 29]\n",
      "[63 34 68 31 31 36 29 29]\n",
      "[34 68 31 31 36 29 29 31]\n",
      "[68 31 31 36 29 29 31 42]\n",
      "[31 31 36 29 29 31 42 31]\n",
      "[31 36 29 29 31 42 31 34]\n",
      "[36 29 29 31 42 31 34 34]\n",
      "[29 29 31 42 31 34 34 34]\n",
      "[29 31 42 31 34 34 34 39]\n",
      "[31 42 31 34 34 34 39 31]\n",
      "[42 31 34 34 34 39 31 34]\n",
      "[31 34 34 34 39 31 34 31]\n",
      "[34 34 34 39 31 34 31 36]\n",
      "[34 34 39 31 34 31 36 34]\n",
      "[34 39 31 34 31 36 34 42]\n",
      "[39 31 34 31 36 34 42 45]\n",
      "[31 34 31 36 34 42 45 45]\n",
      "[34 31 36 34 42 45 45 59]\n",
      "[ 31  36  34  42  45  45  59 106]\n",
      "[ 36  34  42  45  45  59 106  45]\n",
      "[ 34  42  45  45  59 106  45  49]\n",
      "[ 42  45  45  59 106  45  49  42]\n",
      "[ 45  45  59 106  45  49  42 123]\n",
      "[ 45  59 106  45  49  42 123  45]\n",
      "[ 59 106  45  49  42 123  45  39]\n",
      "[106  45  49  42 123  45  39  42]\n",
      "[ 49  49  45  63 160 144  36  42]\n",
      "[ 49  45  63 160 144  36  42  59]\n",
      "[ 45  63 160 144  36  42  59  36]\n",
      "[ 63 160 144  36  42  59  36  56]\n",
      "[160 144  36  42  59  36  56  31]\n",
      "[144  36  42  59  36  56  31  34]\n",
      "[36 42 59 36 56 31 34 42]\n",
      "[42 59 36 56 31 34 42 39]\n",
      "[59 36 56 31 34 42 39 68]\n",
      "[36 56 31 34 42 39 68 56]\n",
      "[56 31 34 42 39 68 56 42]\n",
      "[31 34 42 39 68 56 42 45]\n",
      "[34 42 39 68 56 42 45 42]\n",
      "[42 39 68 56 42 45 42 52]\n",
      "[ 39  68  56  42  45  42  52 133]\n",
      "[ 68  56  42  45  42  52 133  81]\n",
      "[ 56  42  45  42  52 133  81  49]\n",
      "[ 42  45  42  52 133  81  49  42]\n",
      "[ 45  42  52 133  81  49  42  39]\n",
      "[ 42  52 133  81  49  42  39  42]\n",
      "[ 52 133  81  49  42  39  42  42]\n",
      "[133  81  49  42  39  42  42  36]\n",
      "[81 49 42 39 42 42 36 42]\n",
      "[49 42 39 42 42 36 42 45]\n",
      "[42 39 42 42 36 42 45 86]\n",
      "[31 31 23 31 23 21 49 36]\n",
      "[ 31  23  31  23  21  49  36 112]\n",
      "[ 23  31  23  21  49  36 112  96]\n",
      "[ 31  23  21  49  36 112  96  36]\n",
      "[ 23  21  49  36 112  96  36  63]\n",
      "[ 21  49  36 112  96  36  63  39]\n",
      "[ 49  36 112  96  36  63  39  59]\n",
      "[ 36 112  96  36  63  39  59 200]\n",
      "[112  96  36  63  39  59 200  96]\n",
      "[ 96  36  63  39  59 200  96  42]\n",
      "[ 36  63  39  59 200  96  42  56]\n",
      "[ 63  39  59 200  96  42  56  59]\n",
      "[ 39  59 200  96  42  56  59  68]\n",
      "[ 59 200  96  42  56  59  68  49]\n",
      "[200  96  42  56  59  68  49 133]\n",
      "[ 96  42  56  59  68  49 133  68]\n",
      "[ 42  56  59  68  49 133  68  81]\n",
      "[ 56  59  68  49 133  68  81  59]\n",
      "[ 59  68  49 133  68  81  59  56]\n",
      "[ 68  49 133  68  81  59  56  72]\n",
      "[ 49 133  68  81  59  56  72  86]\n",
      "[133  68  81  59  56  72  86 150]\n",
      "[ 68  81  59  56  72  86 150 247]\n",
      "[ 81  59  56  72  86 150 247 231]\n",
      "[ 59  56  72  86 150 247 231 252]\n",
      "[155 155 197 155 128 123 123 144]\n",
      "[155 197 155 128 123 123 144 160]\n",
      "[197 155 128 123 123 144 160 144]\n",
      "[155 128 123 123 144 160 144 133]\n",
      "[128 123 123 144 160 144 133 133]\n",
      "[123 123 144 160 144 133 133 133]\n",
      "[123 144 160 144 133 133 133 133]\n",
      "[144 160 144 133 133 133 133 133]\n",
      "[160 144 133 133 133 133 133 139]\n",
      "[144 133 133 133 133 133 139 133]\n",
      "[133 133 133 133 133 139 133 133]\n",
      "[133 133 133 133 139 133 133 128]\n",
      "[133 133 133 139 133 133 128 128]\n",
      "[133 133 139 133 133 128 128 123]\n",
      "[133 139 133 133 128 128 123 123]\n",
      "[139 133 133 128 128 123 123 123]\n",
      "[133 133 128 128 123 123 123 123]\n",
      "[133 128 128 123 123 123 123 117]\n",
      "[128 128 123 123 123 123 117 117]\n",
      "[128 123 123 123 123 117 117 117]\n",
      "[123 123 123 123 117 117 117 117]\n",
      "[123 123 123 117 117 117 117 117]\n",
      "[123 123 117 117 117 117 117 117]\n",
      "[123 117 117 117 117 117 117 227]\n",
      "[117 117 117 117 117 117 227 128]\n",
      "[39 39 96 49 42 42 34 36]\n",
      "[39 96 49 42 42 34 36 45]\n",
      "[96 49 42 42 34 36 45 42]\n",
      "[49 42 42 34 36 45 42 36]\n",
      "[42 42 34 36 45 42 36 45]\n",
      "[42 34 36 45 42 36 45 31]\n",
      "[34 36 45 42 36 45 31 52]\n",
      "[36 45 42 36 45 31 52 36]\n",
      "[45 42 36 45 31 52 36 36]\n",
      "[42 36 45 31 52 36 36 49]\n",
      "[36 45 31 52 36 36 49 52]\n",
      "[45 31 52 36 36 49 52 59]\n",
      "[31 52 36 36 49 52 59 52]\n",
      "[52 36 36 49 52 59 52 59]\n",
      "[ 36  36  49  52  59  52  59 123]\n",
      "[ 36  49  52  59  52  59 123 112]\n",
      "[ 49  52  59  52  59 123 112  68]\n",
      "[ 52  59  52  59 123 112  68  56]\n",
      "[ 59  52  59 123 112  68  56  91]\n",
      "[ 52  59 123 112  68  56  91  68]\n",
      "[ 59 123 112  68  56  91  68  72]\n",
      "[123 112  68  56  91  68  72  86]\n",
      "[112  68  56  91  68  72  86  45]\n",
      "[68 56 91 68 72 86 45 45]\n",
      "[56 91 68 72 86 45 45 76]\n",
      "[91 68 72 86 45 45 76 49]\n",
      "[68 72 86 45 45 76 49 45]\n",
      "[72 86 45 45 76 49 45 42]\n",
      "[86 45 45 76 49 45 42 45]\n",
      "[45 45 76 49 45 42 45 49]\n",
      "[ 34  34 128  34  14  31  18  14]\n",
      "[ 34 128  34  14  31  18  14  20]\n",
      "[128  34  14  31  18  14  20  14]\n",
      "[34 14 31 18 14 20 14 16]\n",
      "[14 31 18 14 20 14 16 13]\n",
      "[31 18 14 20 14 16 13 17]\n",
      "[18 14 20 14 16 13 17 29]\n",
      "[ 14  20  14  16  13  17  29 239]\n",
      "[ 20  14  16  13  17  29 239  56]\n",
      "[ 14  16  13  17  29 239  56 214]\n",
      "[ 16  13  17  29 239  56 214 160]\n",
      "[ 13  17  29 239  56 214 160  49]\n",
      "[ 17  29 239  56 214 160  49  13]\n",
      "[ 29 239  56 214 160  49  13  20]\n",
      "[239  56 214 160  49  13  20  21]\n",
      "[ 56 214 160  49  13  20  21  18]\n",
      "[214 160  49  13  20  21  18  52]\n",
      "[160  49  13  20  21  18  52  18]\n",
      "[49 13 20 21 18 52 18 25]\n",
      "[13 20 21 18 52 18 25 23]\n",
      "[20 21 18 52 18 25 23 20]\n",
      "[21 18 52 18 25 23 20 21]\n",
      "[18 52 18 25 23 20 21 18]\n",
      "[52 18 25 23 20 21 18 21]\n",
      "[18 25 23 20 21 18 21 17]\n",
      "[25 23 20 21 18 21 17 16]\n",
      "[23 20 21 18 21 17 16 17]\n",
      "[20 21 18 21 17 16 17 68]\n",
      "[21 18 21 17 16 17 68 86]\n",
      "[18 21 17 16 17 68 86 20]\n",
      "[106 106  81 112  56  63  76  72]\n",
      "[106  81 112  56  63  76  72 112]\n",
      "[ 81 112  56  63  76  72 112  96]\n",
      "[112  56  63  76  72 112  96  68]\n",
      "[ 56  63  76  72 112  96  68  72]\n",
      "[ 63  76  72 112  96  68  72  86]\n",
      "[ 76  72 112  96  68  72  86  91]\n",
      "[ 72 112  96  68  72  86  91  96]\n",
      "[112  96  68  72  86  91  96  63]\n",
      "[96 68 72 86 91 96 63 63]\n",
      "[68 72 86 91 96 63 63 52]\n",
      "[72 86 91 96 63 63 52 52]\n",
      "[86 91 96 63 63 52 52 49]\n",
      "[91 96 63 63 52 52 49 45]\n",
      "[96 63 63 52 52 49 45 56]\n",
      "[63 63 52 52 49 45 56 45]\n",
      "[63 52 52 49 45 56 45 49]\n",
      "[52 52 49 45 56 45 49 56]\n",
      "[52 49 45 56 45 49 56 72]\n",
      "[49 45 56 45 49 56 72 72]\n",
      "[45 56 45 49 56 72 72 63]\n",
      "[56 45 49 56 72 72 63 56]\n",
      "[45 49 56 72 72 63 56 56]\n",
      "[49 56 72 72 63 56 56 72]\n",
      "[56 72 72 63 56 56 72 59]\n",
      "[72 72 39 76 68 63 29 29]\n",
      "[72 39 76 68 63 29 29 49]\n",
      "[ 39  76  68  63  29  29  49 155]\n",
      "[ 76  68  63  29  29  49 155 106]\n",
      "[ 68  63  29  29  49 155 106  96]\n",
      "[ 63  29  29  49 155 106  96  96]\n",
      "[ 29  29  49 155 106  96  96  86]\n",
      "[ 29  49 155 106  96  96  86  72]\n",
      "[ 49 155 106  96  96  86  72 112]\n",
      "[155 106  96  96  86  72 112 106]\n",
      "[106  96  96  86  72 112 106  59]\n",
      "[ 96  96  86  72 112 106  59  36]\n",
      "[ 96  86  72 112 106  59  36  42]\n",
      "[ 86  72 112 106  59  36  42  42]\n",
      "[ 72 112 106  59  36  42  42  49]\n",
      "[112 106  59  36  42  42  49  42]\n",
      "[106  59  36  42  42  49  42  45]\n",
      "[59 36 42 42 49 42 45 49]\n",
      "[36 42 42 49 42 45 49 39]\n",
      "[42 42 49 42 45 49 39 42]\n",
      "[42 49 42 45 49 39 42 42]\n",
      "[49 42 45 49 39 42 42 45]\n",
      "[42 45 49 39 42 42 45 56]\n",
      "[45 49 39 42 42 45 56 63]\n",
      "[49 39 42 42 45 56 63 91]\n",
      "[ 39  42  42  45  56  63  91 128]\n",
      "[ 42  42  45  56  63  91 128 227]\n",
      "[ 42  45  56  63  91 128 227 197]\n",
      "[ 45  56  63  91 128 227 197 123]\n",
      "[ 49  49  34  59 106  96 255 247]\n",
      "[ 49  34  59 106  96 255 247  56]\n",
      "[ 34  59 106  96 255 247  56  72]\n",
      "[ 59 106  96 255 247  56  72 255]\n",
      "[106  96 255 247  56  72 255  52]\n",
      "[ 96 255 247  56  72 255  52  81]\n",
      "[255 247  56  72 255  52  81 233]\n",
      "[247  56  72 255  52  81 233  49]\n",
      "[ 56  72 255  52  81 233  49  27]\n",
      "[ 72 255  52  81 233  49  27  31]\n",
      "[255  52  81 233  49  27  31  39]\n",
      "[ 52  81 233  49  27  31  39  36]\n",
      "[ 81 233  49  27  31  39  36  21]\n",
      "[233  49  27  31  39  36  21  49]\n",
      "[49 27 31 39 36 21 49 36]\n",
      "[ 27  31  39  36  21  49  36 123]\n",
      "[ 31  39  36  21  49  36 123  96]\n",
      "[ 39  36  21  49  36 123  96  31]\n",
      "[ 36  21  49  36 123  96  31  25]\n",
      "[ 21  49  36 123  96  31  25  36]\n",
      "[ 49  36 123  96  31  25  36  25]\n",
      "[ 36 123  96  31  25  36  25  29]\n",
      "[123  96  31  25  36  25  29  34]\n",
      "[96 31 25 36 25 29 34 42]\n",
      "[31 25 36 25 29 34 42 27]\n",
      "[25 36 25 29 34 42 27 86]\n",
      "[ 49  49  18  49  17  42 252 251]\n",
      "[ 49  18  49  17  42 252 251  76]\n",
      "[ 18  49  17  42 252 251  76 255]\n",
      "[ 49  17  42 252 251  76 255 255]\n",
      "[ 17  42 252 251  76 255 255 255]\n",
      "[ 42 252 251  76 255 255 255 255]\n",
      "[252 251  76 255 255 255 255 231]\n",
      "[251  76 255 255 255 255 231 235]\n",
      "[ 76 255 255 255 255 231 235 255]\n",
      "[255 255 255 255 231 235 255 255]\n",
      "[255 255 255 231 235 255 255 255]\n",
      "[255 255 231 235 255 255 255 160]\n",
      "[255 231 235 255 255 255 160 255]\n",
      "[231 235 255 255 255 160 255 101]\n",
      "[235 255 255 255 160 255 101 255]\n",
      "[255 255 255 160 255 101 255 255]\n",
      "[255 255 160 255 101 255 255 255]\n",
      "[255 160 255 101 255 255 255 255]\n",
      "[160 255 101 255 255 255 255 255]\n",
      "[255 101 255 255 255 255 255 255]\n",
      "[101 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[ 68  68  23  72  20  68  45 255]\n",
      "[ 68  23  72  20  68  45 255  96]\n",
      "[ 23  72  20  68  45 255  96 255]\n",
      "[ 72  20  68  45 255  96 255 255]\n",
      "[ 20  68  45 255  96 255 255 255]\n",
      "[ 68  45 255  96 255 255 255 255]\n",
      "[ 45 255  96 255 255 255 255 255]\n",
      "[255  96 255 255 255 255 255 255]\n",
      "[ 96 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 150]\n",
      "[255 255 255 255 255 255 150 254]\n",
      "[255 255 255 255 255 150 254 255]\n",
      "[255 255 255 255 150 254 255 255]\n",
      "[255 255 255 150 254 255 255 255]\n",
      "[255 255 150 254 255 255 255 255]\n",
      "[255 150 254 255 255 255 255 255]\n",
      "[150 254 255 255 255 255 255 255]\n",
      "[254 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 244]\n",
      "[255 255 255 255 255 255 244 255]\n",
      "[255 255 255 255 255 244 255 255]\n",
      "[238 238 238 233 180 197 207 193]\n",
      "[238 238 233 180 197 207 193 222]\n",
      "[238 233 180 197 207 193 222 255]\n",
      "[233 180 197 207 193 222 255 255]\n",
      "[180 197 207 193 222 255 255 255]\n",
      "[197 207 193 222 255 255 255 255]\n",
      "[207 193 222 255 255 255 255 255]\n",
      "[193 222 255 255 255 255 255 255]\n",
      "[222 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 231]\n",
      "[255 255 255 255 255 255 231 254]\n",
      "[255 255 255 255 255 231 254 225]\n",
      "[255 255 255 255 231 254 225 255]\n",
      "[255 255 255 231 254 225 255 255]\n",
      "[255 255 231 254 225 255 255 255]\n",
      "[255 231 254 225 255 255 255 255]\n",
      "[231 254 225 255 255 255 255 255]\n",
      "[254 225 255 255 255 255 255 254]\n",
      "[225 255 255 255 255 255 254 255]\n",
      "[255 255 255 255 255 254 255 255]\n",
      "[160 160 255 239 255 255 255 255]\n",
      "[160 255 239 255 255 255 255 255]\n",
      "[255 239 255 255 255 255 255 255]\n",
      "[239 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 165]\n",
      "[255 255 255 255 255 255 165 255]\n",
      "[255 255 255 255 255 165 255 255]\n",
      "[255 255 255 255 165 255 255 255]\n",
      "[255 255 255 165 255 255 255 255]\n",
      "[255 255 165 255 255 255 255 220]\n",
      "[255 165 255 255 255 255 220 255]\n",
      "[165 255 255 255 255 220 255 255]\n",
      "[255 255 255 255 220 255 255 255]\n",
      "[255 255 255 220 255 255 255 255]\n",
      "[255 255 220 255 255 255 255 231]\n",
      "[255 220 255 255 255 255 231 211]\n",
      "[220 255 255 255 255 231 211 235]\n",
      "[255 255 255 255 231 211 235 255]\n",
      "[255 255 255 231 211 235 255 207]\n",
      "[255 255 231 211 235 255 207 255]\n",
      "[255 231 211 235 255 207 255 170]\n",
      "[231 211 235 255 207 255 170 255]\n",
      "[211 235 255 207 255 170 255 184]\n",
      "[235 255 207 255 170 255 184 255]\n",
      "[255 207 255 170 255 184 255 123]\n",
      "[207 255 170 255 184 255 123 255]\n",
      "[255 170 255 184 255 123 255  96]\n",
      "[170 255 184 255 123 255  96  96]\n",
      "[255 184 255 123 255  96  96  96]\n",
      "[184 255 123 255  96  96  96 255]\n",
      "[255 123 255  96  96  96 255 117]\n",
      "[123 255  96  96  96 255 117 255]\n",
      "[255  96  96  96 255 117 255 175]\n",
      "[ 96  96  96 255 117 255 175 250]\n",
      "[ 96  96 255 117 255 175 250 160]\n",
      "[ 96 255 117 255 175 250 160 251]\n",
      "[255 117 255 175 250 160 251 150]\n",
      "[117 255 175 250 160 251 150 255]\n",
      "[255 175 250 160 251 150 255 150]\n",
      "[133 133 255 188 255 255 255 255]\n",
      "[133 255 188 255 255 255 255 255]\n",
      "[255 188 255 255 255 255 255 255]\n",
      "[188 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 225]\n",
      "[255 255 255 255 255 255 225 255]\n",
      "[255 255 255 255 255 225 255 180]\n",
      "[255 255 255 255 225 255 180 255]\n",
      "[255 255 255 225 255 180 255 165]\n",
      "[255 255 225 255 180 255 165 254]\n",
      "[255 225 255 180 255 165 254 170]\n",
      "[225 255 180 255 165 254 170 255]\n",
      "[255 180 255 165 254 170 255 193]\n",
      "[180 255 165 254 170 255 193 255]\n",
      "[255 165 254 170 255 193 255 193]\n",
      "[49 49 23 49 27 23 59 25]\n",
      "[49 23 49 27 23 59 25 25]\n",
      "[23 49 27 23 59 25 25 23]\n",
      "[49 27 23 59 25 25 23 27]\n",
      "[27 23 59 25 25 23 27 31]\n",
      "[ 23  59  25  25  23  27  31 255]\n",
      "[ 59  25  25  23  27  31 255 255]\n",
      "[ 25  25  23  27  31 255 255 217]\n",
      "[ 25  23  27  31 255 255 217 255]\n",
      "[ 23  27  31 255 255 217 255 255]\n",
      "[ 27  31 255 255 217 255 255 254]\n",
      "[ 31 255 255 217 255 255 254 255]\n",
      "[255 255 217 255 255 254 255 255]\n",
      "[255 217 255 255 254 255 255 255]\n",
      "[217 255 255 254 255 255 255 255]\n",
      "[255 255 254 255 255 255 255 255]\n",
      "[255 254 255 255 255 255 255 255]\n",
      "[254 255 255 255 255 255 255 139]\n",
      "[255 255 255 255 255 255 139 255]\n",
      "[255 255 255 255 255 139 255 255]\n",
      "[255 255 255 255 139 255 255 255]\n",
      "[255 255 255 139 255 255 255 255]\n",
      "[255 255 139 255 255 255 255 255]\n",
      "[255 139 255 255 255 255 255 255]\n",
      "[139 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[170 170 207 170 222 231 253 255]\n",
      "[170 207 170 222 231 253 255 255]\n",
      "[207 170 222 231 253 255 255 255]\n",
      "[170 222 231 253 255 255 255 254]\n",
      "[222 231 253 255 255 255 254 255]\n",
      "[231 253 255 255 255 254 255 255]\n",
      "[253 255 255 255 254 255 255 197]\n",
      "[255 255 255 254 255 255 197 255]\n",
      "[255 255 254 255 255 197 255 255]\n",
      "[255 254 255 255 197 255 255 255]\n",
      "[254 255 255 197 255 255 255 231]\n",
      "[255 255 197 255 255 255 231 255]\n",
      "[255 197 255 255 255 231 255 255]\n",
      "[197 255 255 255 231 255 255 255]\n",
      "[255 255 255 231 255 255 255 255]\n",
      "[255 255 231 255 255 255 255 255]\n",
      "[255 231 255 255 255 255 255 255]\n",
      "[231 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[214 214 255 227 255 255 255 254]\n",
      "[214 255 227 255 255 255 254 235]\n",
      "[255 227 255 255 255 254 235 211]\n",
      "[227 255 255 255 254 235 211 133]\n",
      "[255 255 255 254 235 211 133 255]\n",
      "[255 255 254 235 211 133 255 255]\n",
      "[255 254 235 211 133 255 255 255]\n",
      "[254 235 211 133 255 255 255 255]\n",
      "[235 211 133 255 255 255 255 207]\n",
      "[211 133 255 255 255 255 207 255]\n",
      "[133 255 255 255 255 207 255 255]\n",
      "[255 255 255 255 207 255 255 255]\n",
      "[255 255 255 207 255 255 255 255]\n",
      "[255 255 207 255 255 255 255 255]\n",
      "[255 207 255 255 255 255 255 255]\n",
      "[207 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[123 123 240 133 133 150 150 133]\n",
      "[123 240 133 133 150 150 133 144]\n",
      "[240 133 133 150 150 133 144 217]\n",
      "[133 133 150 150 133 144 217 254]\n",
      "[133 150 150 133 144 217 254 255]\n",
      "[150 150 133 144 217 254 255 255]\n",
      "[150 133 144 217 254 255 255 255]\n",
      "[133 144 217 254 255 255 255 255]\n",
      "[144 217 254 255 255 255 255 255]\n",
      "[217 254 255 255 255 255 255 255]\n",
      "[254 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 240]\n",
      "[255 255 255 255 255 255 240 255]\n",
      "[255 255 255 255 255 240 255 246]\n",
      "[255 255 255 255 240 255 246 255]\n",
      "[255 255 255 240 255 246 255 180]\n",
      "[255 255 240 255 246 255 180 255]\n",
      "[255 240 255 246 255 180 255 123]\n",
      "[240 255 246 255 180 255 123 255]\n",
      "[255 246 255 180 255 123 255 239]\n",
      "[246 255 180 255 123 255 239 255]\n",
      "[255 180 255 123 255 239 255 144]\n",
      "[180 255 123 255 239 255 144 255]\n",
      "[255 123 255 239 255 144 255 155]\n",
      "[144 144 180 128 225 255 255 255]\n",
      "[144 180 128 225 255 255 255 227]\n",
      "[180 128 225 255 255 255 227 255]\n",
      "[128 225 255 255 255 227 255 175]\n",
      "[225 255 255 255 227 255 175 255]\n",
      "[255 255 255 227 255 175 255 255]\n",
      "[255 255 227 255 175 255 255 255]\n",
      "[255 227 255 175 255 255 255 255]\n",
      "[227 255 175 255 255 255 255 255]\n",
      "[255 175 255 255 255 255 255 255]\n",
      "[175 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 252]\n",
      "[255 255 255 255 255 255 252 255]\n",
      "[255 255 255 255 255 252 255 255]\n",
      "[255 255 255 255 252 255 255 255]\n",
      "[255 255 255 252 255 255 255 220]\n",
      "[255 255 252 255 255 255 220 243]\n",
      "[255 252 255 255 255 220 243 238]\n",
      "[252 255 255 255 220 243 238 165]\n",
      "[255 255 255 220 243 238 165 106]\n",
      "[160 160 170 160 160 160 155 225]\n",
      "[160 170 160 160 160 155 225 251]\n",
      "[170 160 160 160 155 225 251 248]\n",
      "[160 160 160 155 225 251 248 227]\n",
      "[160 160 155 225 251 248 227 254]\n",
      "[160 155 225 251 248 227 254 255]\n",
      "[155 225 251 248 227 254 255 255]\n",
      "[225 251 248 227 254 255 255 139]\n",
      "[251 248 227 254 255 255 139 255]\n",
      "[248 227 254 255 255 139 255 255]\n",
      "[227 254 255 255 139 255 255 255]\n",
      "[254 255 255 139 255 255 255 255]\n",
      "[255 255 139 255 255 255 255 255]\n",
      "[255 139 255 255 255 255 255 255]\n",
      "[139 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 239]\n",
      "[255 255 255 255 255 255 239  96]\n",
      "[ 68  68  27  68  36  68  96 255]\n",
      "[ 68  27  68  36  68  96 255 255]\n",
      "[ 27  68  36  68  96 255 255 112]\n",
      "[ 68  36  68  96 255 255 112 255]\n",
      "[ 36  68  96 255 255 112 255 255]\n",
      "[ 68  96 255 255 112 255 255 255]\n",
      "[ 96 255 255 112 255 255 255 255]\n",
      "[255 255 112 255 255 255 255 255]\n",
      "[255 112 255 255 255 255 255 255]\n",
      "[112 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 253]\n",
      "[255 255 255 255 255 255 253 255]\n",
      "[255 255 255 255 255 253 255 255]\n",
      "[255 255 255 255 253 255 255 255]\n",
      "[255 255 255 253 255 255 255 255]\n",
      "[255 255 253 255 255 255 255 255]\n",
      "[255 253 255 255 255 255 255 255]\n",
      "[253 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 150]\n",
      "[255 255 255 255 255 255 150 144]\n",
      "[255 255 255 255 255 150 144 255]\n",
      "[255 255 255 255 150 144 255 255]\n",
      "[255 255 255 150 144 255 255 255]\n",
      "[255 255 150 144 255 255 255 255]\n",
      "[155 155 204 165 200 249 255 254]\n",
      "[155 204 165 200 249 255 254 254]\n",
      "[204 165 200 249 255 254 254 255]\n",
      "[165 200 249 255 254 254 255 247]\n",
      "[200 249 255 254 254 255 247 246]\n",
      "[249 255 254 254 255 247 246 139]\n",
      "[255 254 254 255 247 246 139 231]\n",
      "[254 254 255 247 246 139 231 255]\n",
      "[254 255 247 246 139 231 255 255]\n",
      "[255 247 246 139 231 255 255 255]\n",
      "[247 246 139 231 255 255 255 255]\n",
      "[246 139 231 255 255 255 255 255]\n",
      "[139 231 255 255 255 255 255 255]\n",
      "[231 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 236]\n",
      "[255 255 255 255 255 255 236 253]\n",
      "[255 255 255 255 255 236 253 243]\n",
      "[255 255 255 255 236 253 243 214]\n",
      "[184 184 247 184 254 184 252 255]\n",
      "[184 247 184 254 184 252 255 255]\n",
      "[247 184 254 184 252 255 255 255]\n",
      "[184 254 184 252 255 255 255 255]\n",
      "[254 184 252 255 255 255 255 255]\n",
      "[184 252 255 255 255 255 255 255]\n",
      "[252 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 101]\n",
      "[255 255 255 255 255 255 101 255]\n",
      "[255 255 255 255 255 101 255  96]\n",
      "[255 255 255 255 101 255  96  96]\n",
      "[255 255 255 101 255  96  96 255]\n",
      "[255 255 101 255  96  96 255 255]\n",
      "[255 101 255  96  96 255 255 255]\n",
      "[101 255  96  96 255 255 255 254]\n",
      "[255  96  96 255 255 255 254 249]\n",
      "[ 96  96 255 255 255 254 249 180]\n",
      "[ 96 255 255 255 254 249 180 204]\n",
      "[255 255 255 254 249 180 204 133]\n",
      "[255 255 254 249 180 204 133 133]\n",
      "[255 254 249 180 204 133 133 133]\n",
      "[254 249 180 204 133 133 133 144]\n",
      "[106 106 243  81 155 144 220 255]\n",
      "[106 243  81 155 144 220 255 255]\n",
      "[243  81 155 144 220 255 255 254]\n",
      "[ 81 155 144 220 255 255 254 255]\n",
      "[155 144 220 255 255 254 255 255]\n",
      "[144 220 255 255 254 255 255 255]\n",
      "[220 255 255 254 255 255 255 255]\n",
      "[255 255 254 255 255 255 255 255]\n",
      "[255 254 255 255 255 255 255 255]\n",
      "[254 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 255]\n",
      "[255 255 255 255 255 255 255 254]\n",
      "[255 255 255 255 255 255 254 254]\n",
      "[255 255 255 255 255 254 254 254]\n",
      "[255 255 255 255 254 254 254 236]\n",
      "[255 255 255 254 254 254 236 128]\n",
      "805\n"
     ]
    }
   ],
   "source": [
    "probs = []\n",
    "for instance in unshuffled_frames:\n",
    "            print(instance)\n",
    "            intepreter.set_tensor(input_index, np.expand_dims(instance,axis=0))\n",
    "            intepreter.invoke()\n",
    "            output_data = intepreter.get_tensor(output_index)\n",
    "            output_data = output_data[0][0]\n",
    "            probs.append(output_data/255)\n",
    "        \n",
    "print(len(probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "396 409\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "default_num = default_frames_test.shape[0] #396 \n",
    "falling_num = falling_frames_test.shape[0] #409\n",
    "\n",
    "print(default_num,falling_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[62, 26]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#testing on test data\n",
    "def find_prediction_given_thresh(thresh,probs):\n",
    "    prediction = []\n",
    "    for i in range(len(probs)):\n",
    "        if probs[i] > thresh:\n",
    "            prediction.append([probs[i],True])\n",
    "        else:\n",
    "            prediction.append([probs[i],False])\n",
    "    return prediction\n",
    "\n",
    "\n",
    "def threshold_optimizer(probs):\n",
    "    global_best = [-1,32434343]\n",
    "    for f in range(30,95):\n",
    "        threshold = f/100\n",
    "        prediction = find_prediction_given_thresh(threshold,probs)\n",
    "        fake_fall = 0\n",
    "        fake_still = 0\n",
    "\n",
    "        for i in range(len(prediction)):\n",
    "            if i < 396:\n",
    "                if prediction[i][1] == True:\n",
    "                    fake_fall +=1\n",
    "            else: \n",
    "                if prediction[i][1] == False:\n",
    "                    fake_still +=1\n",
    "\n",
    "        total_fakes = fake_fall + fake_still\n",
    "        if total_fakes < global_best[1]:\n",
    "            global_best = [f,total_fakes]\n",
    "    print(global_best)\n",
    "    return global_best[0]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "final_prediction = find_prediction_given_thresh(threshold_optimizer(probs)/100,probs)\n",
    "with open('test_optimized_m2_output.txt', 'w') as f:\n",
    "    for i in range(len(final_prediction)):\n",
    "        f.write(str(final_prediction[i][1])+\"\\n\")\n",
    "f.close()\n",
    "\n",
    "        \n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
