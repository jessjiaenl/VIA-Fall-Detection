{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kevin/.local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/kevin/.local/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/kevin/.local/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "import cv2\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, RandomFlip, RandomRotation\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-04 17:16:55.529962: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.546477: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.546632: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.547279: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-04 17:16:55.547568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.547689: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.547794: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.935269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.935439: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.935547: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-08-04 17:16:55.935632: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "567\n",
      "513\n"
     ]
    }
   ],
   "source": [
    "#cut a video into different frames\n",
    "def cut_video(filename):\n",
    "    list_of_frames = []\n",
    "    cap = cv2.VideoCapture(filename)\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    frame_count  = 0\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        list_of_frames.append(frame)\n",
    "   \n",
    "    cap.release()\n",
    "    return np.array(list_of_frames)\n",
    "\n",
    "\n",
    "# loop through each frame in the array of images, take two frames at a time, \n",
    "# # loop through each pixel in the 2 frames, find the differences in values \n",
    "# in grey scale for the corresponding pixels, append the diff to the rgb array \n",
    "# representing each pixel =>>>>>>>> RGBX. Notei if first frame append 0 to rgb array\n",
    "def load_images_from_folders(data_path,total): \n",
    "    images = np.zeros((total,224,224,4),dtype=int)\n",
    "    counter = 0\n",
    "\n",
    "    #loop through each vid in the folder \n",
    "    for index in range(len(data_path)): #looping through the entire folder array\n",
    "        \n",
    "        #variable to track whether it is the first image/frame in the video\n",
    "        first_image = True\n",
    "\n",
    "        #cut the video into different frames and save them all into an array\n",
    "        array_images = cut_video(data_path[index]) # array_images[0][0][0] = single pixel\n",
    "\n",
    "        #array to store modified pixel of each image, later on append it to images[]\n",
    "        \n",
    "  \n",
    "        for i in range(len(array_images)-1):\n",
    "            if first_image == True:\n",
    "                #process for 0th and 1st frame\n",
    "                images[counter] = first_frame_pixel_modification(array_images[i])\n",
    "                first_image = False\n",
    "                counter+=1\n",
    "            \n",
    "            images[counter] = two_frame_pixel_modification(array_images[i],array_images[i+1])\n",
    "            counter+=1\n",
    "        \n",
    "    #         print(\"finished frame \" + str(i)+ \" of video \" + str(index))\n",
    "        \n",
    "    #     print(str(counter) +\" matches \" + str(len(array_images)))\n",
    "    #     print(\"finished video \" + str(index))\n",
    "        \n",
    "    #     #after finish loop through a frame, append the frame into the images[] array\n",
    "    # print(\"final count: \" + str(counter) + \"; expected count = \" + str(total))\n",
    "    print(counter)\n",
    "    # print(images.shape)\n",
    "    return images\n",
    "\n",
    "def two_frame_pixel_modification(frame1, frame2): \n",
    "    row_frame = np.zeros((224,224,4),dtype=int)\n",
    "    np.array(row_frame) \n",
    "    for r in range(len(frame1)):\n",
    "        column_frame = np.zeros((224,4),dtype=int)\n",
    "        for c in range(len(frame1[0])):\n",
    "            #calcualte each pixel's difference \n",
    "            grey1 =  find_greyscale(frame1[r][c][2],frame1[r][c][1],frame1[r][c][0])\n",
    "            grey2 =  find_greyscale(frame2[r][c][2],frame2[r][c][1],frame2[r][c][0])\n",
    "            diff_grey = abs(grey1-grey2)\n",
    "            new_rgbx = [frame2[r][c][0],frame2[r][c][1],frame2[r][c][2],diff_grey] #add greyscale diff to the end of the pixel array, and append it to the individual frame array\n",
    "            new_rgbx = np.array(new_rgbx)\n",
    "            column_frame[c] = np.array(new_rgbx)\n",
    "        \n",
    "        row_frame[r] = column_frame\n",
    " \n",
    "        \n",
    "    return row_frame \n",
    "\n",
    "def first_frame_pixel_modification(frame0):\n",
    "    row_frame = np.zeros((224,224,4),dtype=int)\n",
    "    np.array(row_frame)\n",
    "    for r in range(len(frame0)):\n",
    "\n",
    "        column_frame = np.zeros((224,4),dtype=int)\n",
    "        for c in range(len(frame0[0])): \n",
    "\n",
    "            new_rgbx = [frame0[r][c][0],frame0[r][c][1],frame0[r][c][2],0]\n",
    "            column_frame[c] = np.array(new_rgbx)\n",
    "\n",
    "        row_frame[r] = column_frame\n",
    "\n",
    "    return row_frame\n",
    "    \n",
    "\n",
    "def find_greyscale(r,g,b):\n",
    "    grey = 0.299*r + 0.587*g + 0.114*b\n",
    "    return grey\n",
    "\n",
    "moving_path = [\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_moving/resized_logitech-fall15.mp4\"\n",
    "                 ]\n",
    "\n",
    "still_path = [\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default1.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default2.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default3.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default4.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default5.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default6.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default7.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default8.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default9.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default10.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default11.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default12.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default13.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default14.mp4\",\n",
    "                 \"./../datasets/vids/splitted/new_still/resized_logitech-default15.mp4\"\n",
    "                 ]\n",
    "\n",
    "still_data = load_images_from_folders(still_path,567)\n",
    "moving_data = load_images_from_folders(moving_path,513)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(still_data)\n",
    "print(moving_data)\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   ...\n",
      "   [114 119 117   0]\n",
      "   [114 119 117   0]\n",
      "   [116 121 119   0]]\n",
      "\n",
      "  [[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   ...\n",
      "   [114 119 117   0]\n",
      "   [116 121 119   0]\n",
      "   [116 121 119   0]]\n",
      "\n",
      "  [[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [132 135 135   0]\n",
      "   ...\n",
      "   [116 121 119   0]\n",
      "   [117 122 120   0]\n",
      "   [117 122 120   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 182 186   0]\n",
      "   [183 191 195   0]\n",
      "   [186 194 198   0]\n",
      "   ...\n",
      "   [134 142 146   0]\n",
      "   [134 142 146   0]\n",
      "   [134 142 146   0]]\n",
      "\n",
      "  [[179 187 191   0]\n",
      "   [187 195 199   0]\n",
      "   [190 198 202   0]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]\n",
      "\n",
      "  [[183 191 195   0]\n",
      "   [187 195 199   0]\n",
      "   [190 198 202   0]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]]\n",
      "\n",
      "\n",
      " [[[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   ...\n",
      "   [114 119 117   0]\n",
      "   [114 119 117   0]\n",
      "   [116 121 119   0]]\n",
      "\n",
      "  [[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   ...\n",
      "   [114 119 117   0]\n",
      "   [116 121 119   0]\n",
      "   [116 121 119   0]]\n",
      "\n",
      "  [[130 133 133   0]\n",
      "   [130 133 133   0]\n",
      "   [132 135 135   0]\n",
      "   ...\n",
      "   [116 121 119   0]\n",
      "   [117 122 120   0]\n",
      "   [117 122 120   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[174 182 186   0]\n",
      "   [183 191 195   0]\n",
      "   [186 194 198   0]\n",
      "   ...\n",
      "   [134 142 146   0]\n",
      "   [134 142 146   0]\n",
      "   [134 142 146   0]]\n",
      "\n",
      "  [[179 187 191   0]\n",
      "   [187 195 199   0]\n",
      "   [190 198 202   0]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]\n",
      "\n",
      "  [[183 191 195   0]\n",
      "   [187 195 199   0]\n",
      "   [190 198 202   0]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]]\n",
      "\n",
      "\n",
      " [[[122 125 125   7]\n",
      "   [125 128 128   5]\n",
      "   [128 131 131   2]\n",
      "   ...\n",
      "   [120 126 121   5]\n",
      "   [121 127 122   6]\n",
      "   [121 127 122   4]]\n",
      "\n",
      "  [[139 142 142   9]\n",
      "   [148 151 151  18]\n",
      "   [146 149 149  16]\n",
      "   ...\n",
      "   [120 126 121   5]\n",
      "   [121 127 122   4]\n",
      "   [121 127 122   4]]\n",
      "\n",
      "  [[184 187 187  54]\n",
      "   [217 220 220  87]\n",
      "   [221 224 224  89]\n",
      "   ...\n",
      "   [120 126 121   3]\n",
      "   [121 127 122   3]\n",
      "   [121 127 122   3]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[197 205 209  23]\n",
      "   [201 209 213  18]\n",
      "   [205 213 217  19]\n",
      "   ...\n",
      "   [133 141 145   1]\n",
      "   [133 141 145   1]\n",
      "   [133 141 145   1]]\n",
      "\n",
      "  [[204 212 216  25]\n",
      "   [207 215 219  20]\n",
      "   [209 217 221  18]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]\n",
      "\n",
      "  [[208 216 220  25]\n",
      "   [209 217 221  21]\n",
      "   [211 219 223  21]\n",
      "   ...\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]\n",
      "   [133 141 145   0]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[ 83  78  79  13]\n",
      "   [ 83  78  79  14]\n",
      "   [ 82  77  78  14]\n",
      "   ...\n",
      "   [253 236 252   8]\n",
      "   [250 235 251   9]\n",
      "   [249 234 250  10]]\n",
      "\n",
      "  [[ 83  78  79  12]\n",
      "   [ 83  78  79  13]\n",
      "   [ 82  77  78  13]\n",
      "   ...\n",
      "   [253 236 252   8]\n",
      "   [250 235 251   9]\n",
      "   [249 234 250  10]]\n",
      "\n",
      "  [[ 84  79  80  14]\n",
      "   [ 84  79  80  15]\n",
      "   [ 83  78  79  15]\n",
      "   ...\n",
      "   [255 235 252   8]\n",
      "   [252 235 251   9]\n",
      "   [251 234 250  10]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  82  83  54]\n",
      "   [ 86  81  82  53]\n",
      "   [ 85  80  81  50]\n",
      "   ...\n",
      "   [246 248 232   2]\n",
      "   [245 246 228   5]\n",
      "   [243 244 226  12]]\n",
      "\n",
      "  [[ 86  81  82  54]\n",
      "   [ 85  80  81  53]\n",
      "   [ 84  79  80  49]\n",
      "   ...\n",
      "   [245 247 231   1]\n",
      "   [243 245 224  10]\n",
      "   [242 244 223  12]]\n",
      "\n",
      "  [[ 85  80  81  54]\n",
      "   [ 84  79  80  51]\n",
      "   [ 86  81  82  46]\n",
      "   ...\n",
      "   [244 246 230   4]\n",
      "   [243 245 224  11]\n",
      "   [242 244 223  12]]]\n",
      "\n",
      "\n",
      " [[[ 83  78  79   0]\n",
      "   [ 83  78  79   0]\n",
      "   [ 82  77  78   0]\n",
      "   ...\n",
      "   [253 236 252   0]\n",
      "   [250 235 251   0]\n",
      "   [249 234 250   0]]\n",
      "\n",
      "  [[ 83  78  79   0]\n",
      "   [ 83  78  79   0]\n",
      "   [ 82  77  78   0]\n",
      "   ...\n",
      "   [253 236 252   0]\n",
      "   [250 235 251   0]\n",
      "   [249 234 250   0]]\n",
      "\n",
      "  [[ 84  79  80   0]\n",
      "   [ 84  79  80   0]\n",
      "   [ 83  78  79   0]\n",
      "   ...\n",
      "   [255 235 252   0]\n",
      "   [252 235 251   0]\n",
      "   [251 234 250   0]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 87  82  83   0]\n",
      "   [ 86  81  82   0]\n",
      "   [ 85  80  81   0]\n",
      "   ...\n",
      "   [246 248 232   0]\n",
      "   [245 246 228   0]\n",
      "   [243 244 226   0]]\n",
      "\n",
      "  [[ 86  81  82   0]\n",
      "   [ 85  80  81   0]\n",
      "   [ 84  79  80   0]\n",
      "   ...\n",
      "   [245 247 231   0]\n",
      "   [243 245 224   0]\n",
      "   [242 244 223   0]]\n",
      "\n",
      "  [[ 85  80  81   0]\n",
      "   [ 84  79  80   0]\n",
      "   [ 86  81  82   0]\n",
      "   ...\n",
      "   [244 246 230   0]\n",
      "   [243 245 224   0]\n",
      "   [242 244 223   0]]]\n",
      "\n",
      "\n",
      " [[[ 85  80  81   1]\n",
      "   [ 85  80  81   1]\n",
      "   [ 84  79  80   2]\n",
      "   ...\n",
      "   [255 221 239  12]\n",
      "   [255 221 239  11]\n",
      "   [255 223 241   8]]\n",
      "\n",
      "  [[ 87  82  83   4]\n",
      "   [ 86  81  82   3]\n",
      "   [ 85  80  81   2]\n",
      "   ...\n",
      "   [255 220 238  13]\n",
      "   [255 221 239  11]\n",
      "   [255 221 239  10]]\n",
      "\n",
      "  [[ 87  82  83   3]\n",
      "   [ 87  82  83   3]\n",
      "   [ 86  81  82   3]\n",
      "   ...\n",
      "   [255 220 238  12]\n",
      "   [255 220 238  12]\n",
      "   [255 221 239  10]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[149 142 148  61]\n",
      "   [149 142 148  62]\n",
      "   [149 142 148  63]\n",
      "   ...\n",
      "   [251 251 251   8]\n",
      "   [251 251 251  10]\n",
      "   [251 251 251  12]]\n",
      "\n",
      "  [[151 144 150  64]\n",
      "   [151 144 150  65]\n",
      "   [151 144 150  66]\n",
      "   ...\n",
      "   [251 251 251   9]\n",
      "   [251 251 251  12]\n",
      "   [251 251 251  13]]\n",
      "\n",
      "  [[151 144 150  65]\n",
      "   [151 144 150  66]\n",
      "   [151 144 150  64]\n",
      "   ...\n",
      "   [251 251 251  10]\n",
      "   [251 251 251  12]\n",
      "   [251 251 251  13]]]]\n"
     ]
    }
   ],
   "source": [
    "#concatenate still and movind data to create inputs and outputs\n",
    "inputs = np.concatenate((still_data,moving_data))\n",
    "print(inputs)\n",
    "outputs = np.concatenate((np.zeros(len(still_data)),np.ones(len(moving_data))))\n",
    "\n",
    "\n",
    "#shuffle the data to prepare for training \n",
    "shuffled_indices = np.random.permutation(len(inputs))\n",
    "inputs, ouputs = inputs[shuffled_indices],outputs[shuffled_indices]\n",
    "\n",
    "#create a tensorflow dataset\n",
    "with tf.device('/cpu:0'):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((inputs,outputs))\n",
    "    dataset = dataset.batch(16).prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "\n",
    "#determine train and test \n",
    "train_data = dataset.take(int(0.6*len(dataset)))\n",
    "val_data = dataset.skip(int(0.6*len(dataset)))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1080, 224, 224, 4)\n",
      "(1080,)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_27 (Conv2D)          (None, 222, 222, 32)      1184      \n",
      "                                                                 \n",
      " max_pooling2d_27 (MaxPoolin  (None, 111, 111, 32)     0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_28 (Conv2D)          (None, 109, 109, 64)      18496     \n",
      "                                                                 \n",
      " max_pooling2d_28 (MaxPoolin  (None, 54, 54, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_29 (Conv2D)          (None, 52, 52, 128)       73856     \n",
      "                                                                 \n",
      " max_pooling2d_29 (MaxPoolin  (None, 26, 26, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " flatten_9 (Flatten)         (None, 86528)             0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 128)               11075712  \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 64)                8256      \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,177,569\n",
      "Trainable params: 11,177,569\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_shape = (224, 224, 4)\n",
    "\n",
    "#data augmentation \n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    layers.RandomRotation(0.2),\n",
    "    layers.RandomContrast(factor=0.8)\n",
    "\n",
    "])\n",
    "\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "\n",
    "    # Add Convolutional and Pooling layers\n",
    "    data_augmentation,\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=input_shape),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    \n",
    "    # Flatten the output of the Convolutional layers before passing to Dense layers\n",
    "    Flatten(),\n",
    "    \n",
    "    # Add Dense layers for classification\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid'),  # Using sigmoid activation for binary classification\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "# Print the model summary to verify the input shape\n",
    "model.build(input_shape)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generator class -> save memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DataGenerator(Sequence):\n",
    "#     def __init__(self, x_set, y_set, batch_size):\n",
    "#         self.x, self.y = x_set, y_set\n",
    "#         self.batch_size = batch_size\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(np.ceil(len(self.x) / float(self.batch_size)))\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         batch_x = self.x[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         batch_y = self.y[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "#         return batch_x, batch_y\n",
    "\n",
    "# train_gen = DataGenerator(x_train, y_train, 32)\n",
    "# test_gen = DataGenerator(x_val, y_val, 32)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1100.1805 - accuracy: 0.8859 - val_loss: 2.6006e-06 - val_accuracy: 1.0000\n",
      "Epoch 2/5\n",
      "40/40 [==============================] - 3s 66ms/step - loss: 86.8901 - accuracy: 0.8359 - val_loss: 1.1512e-05 - val_accuracy: 1.0000\n",
      "Epoch 3/5\n",
      "40/40 [==============================] - 3s 65ms/step - loss: 55.9969 - accuracy: 0.7859 - val_loss: 0.0041 - val_accuracy: 1.0000\n",
      "Epoch 4/5\n",
      "40/40 [==============================] - 3s 68ms/step - loss: 1.3988 - accuracy: 0.9078 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 5/5\n",
      "40/40 [==============================] - 3s 67ms/step - loss: 5.3323 - accuracy: 0.7609 - val_loss: 2.0148 - val_accuracy: 0.2341\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fca61de2b20>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model.fit(train_data,epochs=5,validation_data=val_data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUANTIZE\n",
    "# method 1: save as savedmodel then load and quantize like model2 did\n",
    "# method 2: change datatype before saving\n",
    "'''\n",
    "def representative_dataset():\n",
    "  for d in test_data:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('model2_quant', 'wb') as f: f.write(tflite_quant_model2)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predicting multiple vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_falling.mp4\"]\n",
    "\n",
    "default_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_default.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_jump.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_pickup.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run4.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk4.MOV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchPredict(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    probs = []\n",
    "    while True:\n",
    "        ret, frame = cap.read() \n",
    "        if not ret: break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        output = interpreter.get_output_details()\n",
    "        input = interpreter.get_input_details()\n",
    "        output_index = output[0]['index']\n",
    "        input_index = input[0]['index']\n",
    "\n",
    "        interpreter.set_tensor(input_index, frame_rgb)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        output_data = output_data[0]\n",
    "        output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "        predicted_index = np.argmax(output_data)\n",
    "        class_labels = [\"Moving\", \"Still\"]\n",
    "        predicted_class = class_labels[predicted_index]        \n",
    "        prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "\n",
    "        if predicted_class == \"Still\": probs.append(np.subtract(1, prob))\n",
    "        else: probs.append(prob)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchPredictMultVids(vid_path_lst):\n",
    "    record = []\n",
    "    for vid in vid_path_lst:\n",
    "        vid_probs = batchPredict(vid)\n",
    "        print(vid)\n",
    "        for i in range(len(vid_probs)-15):\n",
    "            record.append(vid_probs[i:i+16])\n",
    "            for j in range(i, i+16):\n",
    "                print(vid_probs[j], end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling = batchPredictMultVids(falling_paths)\n",
    "default = batchPredictMultVids(default_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2344, 16) (6270, 16)\n"
     ]
    }
   ],
   "source": [
    "default_data = np.loadtxt('./datasets/model2_data/default.txt')\n",
    "falling_data = np.loadtxt('./datasets/model2_data/falling.txt')\n",
    "\n",
    "print(falling_data.shape, default_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6270, 16) (6270, 16)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#augment the falling data\n",
    "\n",
    "mu, sig = 0, 0.001\n",
    "copy_count = len(default_data)-len(falling_data)\n",
    "oldLen = len(falling_data)\n",
    "rowi = 0\n",
    "\n",
    "# limit = lambda x : max(min(x,1), 0), basically 0 <= x < 1\n",
    "\n",
    "def limit(x):\n",
    "    if x > 1: return 1 - (x-1) # if x larger than 1 -> take the difference between x and 1 and subtract from 1\n",
    "    elif x < 0: return abs(x) # if x is smaller than 0 -> abs val it \n",
    "    return x\n",
    "\n",
    "newLimit = np.vectorize(limit) #Vectorize to loop through the entire array at once\n",
    "\n",
    "for i in range(copy_count):\n",
    "    row = falling_data[rowi] + np.random.normal(mu, sig, 16)\n",
    "    print(row)\n",
    "    row = newLimit(row)\n",
    "\n",
    "    falling_data = np.vstack([falling_data, row])\n",
    "    rowi += 1\n",
    "    if rowi >= oldLen: rowi = 0\n",
    "\n",
    "print(falling_data.shape, default_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2484 3621 7926 ... 5541 2568 7662]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'Sequential' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m X_test, y_test \u001b[39m=\u001b[39m inputs[train_size:], outputs[train_size:]\n\u001b[1;32m     22\u001b[0m \u001b[39m# Define the model architecture\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m model \u001b[39m=\u001b[39m Sequential([\n\u001b[1;32m     24\u001b[0m     Flatten(input_shape\u001b[39m=\u001b[39m(\u001b[39m28\u001b[39m, \u001b[39m28\u001b[39m)),\n\u001b[1;32m     25\u001b[0m     Dense(\u001b[39m100\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     26\u001b[0m     Dense(\u001b[39m100\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m),\n\u001b[1;32m     27\u001b[0m     Dense(\u001b[39m10\u001b[39m, activation\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msigmoid\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m ])\n\u001b[1;32m     30\u001b[0m \u001b[39m# Compile the model\u001b[39;00m\n\u001b[1;32m     31\u001b[0m model\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39madam\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     32\u001b[0m               loss\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_crossentropy\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     33\u001b[0m               metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msparse_categorical_accuracy\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Sequential' is not defined"
     ]
    }
   ],
   "source": [
    "# loading data, output is used to validate input\n",
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data)))) #ones are falling , zeros are default\n",
    "\n",
    "\n",
    "dataset_size = len(inputs) #length of the entire dataset, including both outputs and inputs\n",
    "new_indices = np.random.permutation(dataset_size) # using dataset size, shufflen the indices to shuffle X and y at the same time\n",
    "print(new_indices)\n",
    "\n",
    "#new_indices is an array that contains all indices of the dataset in a shuffled order, when passed into inputs/outputs, other np arrays, inputs and outputs will be rearranged to match the index of the new_indices array\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices] \n",
    "\n",
    "\n",
    "train_size = int(0.8*dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_train, y_train = inputs[:train_size], outputs[:train_size] #x = images, y = label \n",
    "X_test, y_test = inputs[train_size:], outputs[train_size:]\n",
    "\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential([\n",
    "    Flatten(input_shape=(28, 28)),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(100, activation=\"relu\"),\n",
    "    Dense(10, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"sparse_categorical_accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=32, epochs=50,\n",
    "                    validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(\"Test Loss:\", test_loss)\n",
    "print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "# Plot the validation accuracy over epochs\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=6,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "# model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CONVERT TO TFLITE\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model2_tflite = converter.convert()\n",
    "with open('model2.tflite', 'wb') as f: f.write(model2_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUANTIZE\n",
    "def representative_dataset():\n",
    "  for d in inputs:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('model2_quant', 'wb') as f: f.write(tflite_quant_model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resize vid\n",
    "video_path = \"compilation_cut.mp4\"\n",
    "output_path = \"compilation_cut_resized.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "#resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store the frame classifications\n",
    "frame_classifications = []\n",
    "moving_prob = []\n",
    "# Loop through the frames of the video (need to change to 30 fps)\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    #just need to figure out if this is 30 fps\n",
    "\n",
    "    if not ret: # Break the loop if the video has ended\n",
    "        break\n",
    "\n",
    "    ''' Formulate Input Data (frame_rgb) '''\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Make frame input data and ensure its type matches the model\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    ''' Classify the Frame '''\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "    # keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "    # len(input) = len(output) = 1, so access the first element\n",
    "    output = interpreter.get_output_details()\n",
    "    input = interpreter.get_input_details()\n",
    "    output_index = output[0]['index']\n",
    "    input_index = input[0]['index']\n",
    "\n",
    "    # set input -> invoke -> access output\n",
    "    interpreter.set_tensor(input_index, frame_rgb)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    # If the output_data shape is (batch_size, num_classes), select the first frame\n",
    "    output_data = output_data[0]\n",
    "\n",
    "    # Convert each entry into probability\n",
    "    output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    predicted_index = np.argmax(output_data)\n",
    "\n",
    "    # Assuming you have a list of class labels corresponding to the model's output classes\n",
    "    class_labels = [\"Moving\", \"Still\"]\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = class_labels[predicted_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    # print(\"Predicted Class:\", predicted_class)\n",
    "    frame_classifications.append((predicted_class, max(output_probs.numpy())))\n",
    "    \n",
    "    prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "    if predicted_class == \"Still\":\n",
    "        \n",
    "        moving_prob.append(np.subtract(1, prob))\n",
    "    else:\n",
    "        moving_prob.append(prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_probs_trimmed = moving_prob[:-(len(moving_prob)%16)]\n",
    "model2_in = np.array(moving_probs_trimmed).reshape((len(moving_prob)//16, 16))\n",
    "print(model2_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_preds = model2.predict(model2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.88\n",
    "bools = vid_preds.reshape((1, len(vid_preds))) > threshold\n",
    "print(bools)\n",
    "print(vid_preds.reshape((1, len(vid_preds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
