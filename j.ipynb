{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.8.4 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-01 11:35:30.877396: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:30.914784: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:30.914983: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:30.916119: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-01 11:35:30.916472: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:30.916644: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:30.916793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:31.306358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:31.306525: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:31.306631: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-06-01 11:35:31.306717: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader.from_folder(r\"./datasets/model1_data\")\n",
    "train_data, test_data = data.split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hub_keras_layer_v1v2_6 (Hub  (None, 1280)             3413024   \n",
      " KerasLayerV1V2)                                                 \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,415,586\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "82/82 [==============================] - 3s 28ms/step - loss: 0.4442 - accuracy: 0.8266\n",
      "Epoch 2/5\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3626 - accuracy: 0.8944\n",
      "Epoch 3/5\n",
      "82/82 [==============================] - 2s 28ms/step - loss: 0.3414 - accuracy: 0.9093\n",
      "Epoch 4/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.3384 - accuracy: 0.9135\n",
      "Epoch 5/5\n",
      "82/82 [==============================] - 2s 29ms/step - loss: 0.3344 - accuracy: 0.9165\n"
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data, use_augmentation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 29ms/step - loss: 0.2689 - accuracy: 0.9623\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_data = np.loadtxt('./datasets/model2_data/default.txt')\n",
    "falling_data = np.loadtxt('./datasets/model2_data/falling.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data))))\n",
    "\n",
    "# Convert inputs and outputs to TensorFlow Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "dataset = dataset.shuffle(len(inputs)).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6934 - accuracy: 0.6962\n",
      "Epoch 2/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6736 - accuracy: 0.6998\n",
      "Epoch 3/6\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6582 - accuracy: 0.7288\n",
      "Epoch 4/6\n",
      "5/5 [==============================] - 0s 1ms/step - loss: 0.6426 - accuracy: 0.7722\n",
      "Epoch 5/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6273 - accuracy: 0.7975\n",
      "Epoch 6/6\n",
      "5/5 [==============================] - 0s 2ms/step - loss: 0.6126 - accuracy: 0.8156\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fd83ddc7f70>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(dataset, epochs=6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-31 11:17:06.978328: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "INFO:tensorflow:Assets written to: model2/assets\n"
     ]
    }
   ],
   "source": [
    "# loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "model.save(\"model2\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# Resize vid\n",
    "video_path = \"s-walk2.mp4\"\n",
    "output_path = \"datasets/model1_vids/resized_IMG_0484.MOV\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "# resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store the frame classifications\n",
    "frame_classifications = []\n",
    "moving_prob = []\n",
    "# Loop through the frames of the video (need to change to 30 fps)\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    #just need to figure out if this is 30 fps\n",
    "\n",
    "    if not ret: # Break the loop if the video has ended\n",
    "        break\n",
    "\n",
    "    ''' Formulate Input Data (frame_rgb) '''\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Make frame input data and ensure its type matches the model\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    ''' Classify the Frame '''\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "    # keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "    # len(input) = len(output) = 1, so access the first element\n",
    "    output = interpreter.get_output_details()\n",
    "    input = interpreter.get_input_details()\n",
    "    output_index = output[0]['index']\n",
    "    input_index = input[0]['index']\n",
    "\n",
    "    # set input -> invoke -> access output\n",
    "    interpreter.set_tensor(input_index, frame_rgb)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    # If the output_data shape is (batch_size, num_classes), select the first frame\n",
    "    output_data = output_data[0]\n",
    "\n",
    "    # Convert each entry into probability\n",
    "    output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    predicted_index = np.argmax(output_data)\n",
    "\n",
    "    # Assuming you have a list of class labels corresponding to the model's output classes\n",
    "    class_labels = [\"Moving\", \"Still\"]\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = class_labels[predicted_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    # print(\"Predicted Class:\", predicted_class)\n",
    "    frame_classifications.append((predicted_class, max(output_probs.numpy())))\n",
    "    \n",
    "    prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "    if predicted_class == \"Still\":\n",
    "        \n",
    "        moving_prob.append(np.subtract(1, prob))\n",
    "    else:\n",
    "        moving_prob.append(prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_probs_trimmed = moving_prob[:-(len(moving_prob)%16)]\n",
    "model2_in = np.array(moving_probs_trimmed).reshape((len(moving_prob)//16, 16))\n",
    "# print(model2_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_preds = model2.predict(model2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False  True  True  True]]\n",
      "[[0.37859526 0.69188964 0.50663507 0.5051739 ]]\n"
     ]
    }
   ],
   "source": [
    "bools = vid_preds.reshape((1, len(vid_preds))) > 0.5\n",
    "print(bools)\n",
    "print(vid_preds.reshape((1, len(vid_preds))))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predicting multiple vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_falling.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_falling.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_falling.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_falling.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_falling.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_falling.MOV\"]\n",
    "\n",
    "default_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_default.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_jump.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_pickup.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run4.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk4.MOV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchPredict(path, probs):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    # Loop through the frames of the video (need to change to 30 fps)\n",
    "    while True:\n",
    "        ret, frame = cap.read() \n",
    "        if not ret: break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        output = interpreter.get_output_details()\n",
    "        input = interpreter.get_input_details()\n",
    "        output_index = output[0]['index']\n",
    "        input_index = input[0]['index']\n",
    "\n",
    "        interpreter.set_tensor(input_index, frame_rgb)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        output_data = output_data[0]\n",
    "        output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "        predicted_index = np.argmax(output_data)\n",
    "        class_labels = [\"Moving\", \"Still\"]\n",
    "        predicted_class = class_labels[predicted_index]        \n",
    "        prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "\n",
    "        if predicted_class == \"Still\": probs.append(np.subtract(1, prob))\n",
    "        else: probs.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in falling_paths:\n",
    "    vid_probs = batchPredict(vid)\n",
    "    print(vid)\n",
    "    for i in range(len(moving_prob)-15):\n",
    "        for j in range(i, i+16):\n",
    "            print(moving_prob[j], end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vid in default_paths:\n",
    "    vid_probs = batchPredict(vid)\n",
    "    print(vid)\n",
    "    for i in range(len(moving_prob)-15):\n",
    "        for j in range(i, i+16):\n",
    "            print(moving_prob[j], end=\" \")\n",
    "        print()\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
