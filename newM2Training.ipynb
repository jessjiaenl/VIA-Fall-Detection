{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 10:55:17.818344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.955122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.955372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.992199: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 10:55:18.004453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:18.005122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:18.005675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.040463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.041029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.041456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.050547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_data = np.loadtxt('./datasets/model2_data/default.txt')\n",
    "falling_data = np.loadtxt('./datasets/model2_data/falling.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6270, 16) (6270, 16)\n"
     ]
    }
   ],
   "source": [
    "mu, sig = 0, 0.001\n",
    "copy_count = len(default_data)-len(falling_data)\n",
    "oldLen = len(falling_data)\n",
    "rowi = 0\n",
    "\n",
    "# limit = lambda x : max(min(x,1), 0)\n",
    "\n",
    "def limit(x):\n",
    "    if x > 1: return 1 - (x-1)\n",
    "    elif x < 0: return abs(x)\n",
    "    return x\n",
    "\n",
    "newLimit = np.vectorize(limit)\n",
    "\n",
    "for i in range(copy_count):\n",
    "    err = np.random.normal(mu, sig, 16)\n",
    "    scale = 1\n",
    "    row = falling_data[rowi]*scale + err\n",
    "    row = newLimit(row)\n",
    "\n",
    "    falling_data = np.vstack([falling_data, row])\n",
    "    rowi += 1\n",
    "    if rowi >= oldLen: rowi = 0\n",
    "\n",
    "print(falling_data.shape, default_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 1. 1. ... 0. 1. 1.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [1. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data)))) #ones are falling , zeros are default\n",
    "dataset_size = len(inputs)\n",
    "new_indices = np.random.permutation(dataset_size) # shuffle indices to shuffle X and y at the same time\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices]\n",
    "\n",
    "train_size = int(0.8*dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_train = tf.data.Dataset.from_tensor_slices(inputs[:train_size]).batch(128)\n",
    "y_train = tf.data.Dataset.from_tensor_slices(outputs[:train_size]).batch(128)\n",
    "X_test, y_test = inputs[train_size:], outputs[train_size:]\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "314/314 [==============================] - 2s 5ms/step - loss: 0.3758 - accuracy: 0.8552\n",
      "Epoch 2/6\n",
      "314/314 [==============================] - 2s 5ms/step - loss: 0.3751 - accuracy: 0.8564\n",
      "Epoch 3/6\n",
      "314/314 [==============================] - 2s 5ms/step - loss: 0.3736 - accuracy: 0.8566\n",
      "Epoch 4/6\n",
      "314/314 [==============================] - 2s 7ms/step - loss: 0.3744 - accuracy: 0.8576\n",
      "Epoch 5/6\n",
      "314/314 [==============================] - 2s 6ms/step - loss: 0.3723 - accuracy: 0.8586\n",
      "Epoch 6/6\n",
      "314/314 [==============================] - 2s 5ms/step - loss: 0.3715 - accuracy: 0.8583\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1dde501dc0>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=6,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 991us/step - loss: 0.3782 - accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "# model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgy6fdty2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:16:44.155686: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2023-06-07 11:16:44.445558: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 11:16:44.445580: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 11:16:44.446467: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.447205: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 11:16:44.447233: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.449735: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 11:16:44.489124: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.496765: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 50301 microseconds.\n",
      "2023-06-07 11:16:44.509424: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "### CONVERT TO TFLITE\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model2_tflite = converter.convert()\n",
    "with open('model2.tflite', 'wb') as f: f.write(model2_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-07 15:22:06.987031: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 15:22:06.987052: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 15:22:06.987225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.987830: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 15:22:06.987841: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.989672: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 15:22:07.012328: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:07.019440: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 32217 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "### QUANTIZE\n",
    "def representative_dataset():\n",
    "  for d in inputs:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('model2_quant', 'wb') as f: f.write(tflite_quant_model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.97\n"
     ]
    }
   ],
   "source": [
    "# Resize vid\n",
    "video_path = \"compilation_cut.mp4\"\n",
    "output_path = \"compilation_cut_resized.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "#resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store the frame classifications\n",
    "frame_classifications = []\n",
    "moving_prob = []\n",
    "# Loop through the frames of the video (need to change to 30 fps)\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    #just need to figure out if this is 30 fps\n",
    "\n",
    "    if not ret: # Break the loop if the video has ended\n",
    "        break\n",
    "\n",
    "    ''' Formulate Input Data (frame_rgb) '''\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Make frame input data and ensure its type matches the model\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    ''' Classify the Frame '''\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "    # keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "    # len(input) = len(output) = 1, so access the first element\n",
    "    output = interpreter.get_output_details()\n",
    "    input = interpreter.get_input_details()\n",
    "    output_index = output[0]['index']\n",
    "    input_index = input[0]['index']\n",
    "\n",
    "    # set input -> invoke -> access output\n",
    "    interpreter.set_tensor(input_index, frame_rgb)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    # If the output_data shape is (batch_size, num_classes), select the first frame\n",
    "    output_data = output_data[0]\n",
    "\n",
    "    # Convert each entry into probability\n",
    "    output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    predicted_index = np.argmax(output_data)\n",
    "\n",
    "    # Assuming you have a list of class labels corresponding to the model's output classes\n",
    "    class_labels = [\"Moving\", \"Still\"]\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = class_labels[predicted_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    # print(\"Predicted Class:\", predicted_class)\n",
    "    frame_classifications.append((predicted_class, max(output_probs.numpy())))\n",
    "    \n",
    "    prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "    if predicted_class == \"Still\":\n",
    "        \n",
    "        moving_prob.append(np.subtract(1, prob))\n",
    "    else:\n",
    "        moving_prob.append(prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   0.  ]\n",
      " [0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   1.   0.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   0.   0.   1.   1.   1.   0.   1.   1.   0.\n",
      "  1.   1.  ]\n",
      " [0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   1.   0.5\n",
      "  1.   1.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [1.   0.   0.   0.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.5  1.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.12 0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.12 0.  ]\n",
      " [0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.88 0.   0.   0.12 0.   0.88 0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   1.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   1.   0.   0.\n",
      "  0.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   0.   1.   1.   1.   0.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   0.5  1.   1.   0.88 1.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "moving_probs_trimmed = moving_prob[:-(len(moving_prob)%16)]\n",
    "model2_in = np.array(moving_probs_trimmed).reshape((len(moving_prob)//16, 16))\n",
    "print(model2_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_preds = model2.predict(model2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False False False False False False\n",
      "   True  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]]\n",
      "[[0.09045138 0.09045138 0.09045138 0.09045138 0.09045138 0.5739342\n",
      "  0.30349308 0.7925048  0.7975426  0.51072216 0.7444853  0.7925048\n",
      "  0.929349   0.929349   0.8644172  0.09045138 0.15479052 0.29671496\n",
      "  0.09045138 0.15544447 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.17014171 0.12607063 0.09045138 0.09045138 0.19399568\n",
      "  0.09045138 0.09045138 0.16418059 0.14767478 0.15647146 0.12583217\n",
      "  0.09045138 0.09045138 0.18011312 0.09045138 0.11525431 0.17111975\n",
      "  0.26666683 0.929349   0.80626893 0.929349   0.929349   0.2670536\n",
      "  0.16497837 0.09045138 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.09045138 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.09045138]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.88\n",
    "bools = vid_preds.reshape((1, len(vid_preds))) > threshold\n",
    "print(bools)\n",
    "print(vid_preds.reshape((1, len(vid_preds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
