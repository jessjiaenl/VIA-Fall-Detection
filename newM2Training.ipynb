{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 10:55:17.818344: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.955122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.955372: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:17.992199: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 10:55:18.004453: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:18.005122: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:18.005675: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.040463: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.041029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.041456: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 10:55:21.050547: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_data = np.loadtxt('defaultDiff.txt')\n",
    "falling_data = np.loadtxt('fallingDiff.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3220, 16) (3220, 16)\n"
     ]
    }
   ],
   "source": [
    "mu, sig = 0, 0.001\n",
    "copy_count = len(default_data)-len(falling_data)\n",
    "oldLen = len(falling_data)\n",
    "rowi = 0\n",
    "\n",
    "# limit = lambda x : max(min(x,1), 0)\n",
    "\n",
    "def limit(x):\n",
    "    if x > 1: return 1 - (x-1)\n",
    "    elif x < 0: return abs(x)\n",
    "    return x\n",
    "\n",
    "newLimit = np.vectorize(limit)\n",
    "\n",
    "for i in range(copy_count):\n",
    "    # err = np.random.normal(mu, sig, 16)\n",
    "    err = 0\n",
    "    scale = 1\n",
    "    row = falling_data[rowi]*scale + err\n",
    "    row = newLimit(row)\n",
    "\n",
    "    falling_data = np.vstack([falling_data, row])\n",
    "    rowi += 1\n",
    "    if rowi >= oldLen: rowi = 0\n",
    "\n",
    "print(falling_data.shape, default_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.09180733 0.09466914 0.10662924 ... 0.11981803 0.0934365  0.08545271]\n",
      " [0.03829182 0.03852374 0.04664954 ... 0.09508381 0.09578914 0.09532939]\n",
      " [0.06093381 0.05540801 0.06446455 ... 0.08758888 0.08465607 0.09008838]\n",
      " ...\n",
      " [0.34471962 0.33762761 0.22215499 ... 0.26457646 0.23173583 0.2225707 ]\n",
      " [0.05326341 0.02523982 0.0726422  ... 0.07243748 0.07156026 0.07870207]\n",
      " [0.01184504 0.01083142 0.01386972 ... 0.01039466 0.01052505 0.0111747 ]]\n"
     ]
    }
   ],
   "source": [
    "# loading data\n",
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data)))) #ones are falling , zeros are default\n",
    "dataset_size = len(inputs)\n",
    "new_indices = np.random.permutation(dataset_size) # shuffle indices to shuffle X and y at the same time\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices]\n",
    "\n",
    "train_size = int(0.8*dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_train, y_train = inputs[:train_size], outputs[:train_size]\n",
    "X_test, y_test = inputs[train_size:], outputs[train_size:]\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "145/145 [==============================] - 1s 5ms/step - loss: 0.5831 - accuracy: 0.6592 - val_loss: 0.5423 - val_accuracy: 0.6938\n",
      "Epoch 2/6\n",
      "145/145 [==============================] - 0s 1ms/step - loss: 0.5151 - accuracy: 0.7332 - val_loss: 0.5379 - val_accuracy: 0.6996\n",
      "Epoch 3/6\n",
      "145/145 [==============================] - 0s 1ms/step - loss: 0.5028 - accuracy: 0.7414 - val_loss: 0.5298 - val_accuracy: 0.7016\n",
      "Epoch 4/6\n",
      "145/145 [==============================] - 0s 1ms/step - loss: 0.4977 - accuracy: 0.7433 - val_loss: 0.5296 - val_accuracy: 0.7093\n",
      "Epoch 5/6\n",
      "145/145 [==============================] - 1s 4ms/step - loss: 0.4934 - accuracy: 0.7476 - val_loss: 0.5232 - val_accuracy: 0.7151\n",
      "Epoch 6/6\n",
      "145/145 [==============================] - 0s 1ms/step - loss: 0.4897 - accuracy: 0.7500 - val_loss: 0.5184 - val_accuracy: 0.7132\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faf646aadc0>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, validation_split=0.1, epochs=6,callbacks=[tensorboard_callback]) # if training data is np array\n",
    "# model.fit(train_data, epochs=6,callbacks=[tensorboard_callback]) # if training data is tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/IPython/core/pylabtools.py:152: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n",
      "  fig.canvas.print_figure(bytes_io, **kw)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image size of 547x134913 pixels is too large. It must be less than 2^16 in each direction.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/IPython/core/formatters.py:340\u001b[0m, in \u001b[0;36mBaseFormatter.__call__\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    338\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 340\u001b[0m     \u001b[39mreturn\u001b[39;00m printer(obj)\n\u001b[1;32m    341\u001b[0m \u001b[39m# Finally look for special method names\u001b[39;00m\n\u001b[1;32m    342\u001b[0m method \u001b[39m=\u001b[39m get_real_method(obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprint_method)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/IPython/core/pylabtools.py:152\u001b[0m, in \u001b[0;36mprint_figure\u001b[0;34m(fig, fmt, bbox_inches, base64, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mmatplotlib\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbackend_bases\u001b[39;00m \u001b[39mimport\u001b[39;00m FigureCanvasBase\n\u001b[1;32m    150\u001b[0m     FigureCanvasBase(fig)\n\u001b[0;32m--> 152\u001b[0m fig\u001b[39m.\u001b[39;49mcanvas\u001b[39m.\u001b[39;49mprint_figure(bytes_io, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    153\u001b[0m data \u001b[39m=\u001b[39m bytes_io\u001b[39m.\u001b[39mgetvalue()\n\u001b[1;32m    154\u001b[0m \u001b[39mif\u001b[39;00m fmt \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39msvg\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backend_bases.py:2255\u001b[0m, in \u001b[0;36mFigureCanvasBase.print_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2251\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   2252\u001b[0m     \u001b[39m# _get_renderer may change the figure dpi (as vector formats\u001b[39;00m\n\u001b[1;32m   2253\u001b[0m     \u001b[39m# force the figure dpi to 72), so we need to set it again here.\u001b[39;00m\n\u001b[1;32m   2254\u001b[0m     \u001b[39mwith\u001b[39;00m cbook\u001b[39m.\u001b[39m_setattr_cm(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure, dpi\u001b[39m=\u001b[39mdpi):\n\u001b[0;32m-> 2255\u001b[0m         result \u001b[39m=\u001b[39m print_method(\n\u001b[1;32m   2256\u001b[0m             filename,\n\u001b[1;32m   2257\u001b[0m             facecolor\u001b[39m=\u001b[39;49mfacecolor,\n\u001b[1;32m   2258\u001b[0m             edgecolor\u001b[39m=\u001b[39;49medgecolor,\n\u001b[1;32m   2259\u001b[0m             orientation\u001b[39m=\u001b[39;49morientation,\n\u001b[1;32m   2260\u001b[0m             bbox_inches_restore\u001b[39m=\u001b[39;49m_bbox_inches_restore,\n\u001b[1;32m   2261\u001b[0m             \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   2262\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m   2263\u001b[0m     \u001b[39mif\u001b[39;00m bbox_inches \u001b[39mand\u001b[39;00m restore_bbox:\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backend_bases.py:1669\u001b[0m, in \u001b[0;36m_check_savefig_extra_args.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1661\u001b[0m     _api\u001b[39m.\u001b[39mwarn_deprecated(\n\u001b[1;32m   1662\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m3.3\u001b[39m\u001b[39m'\u001b[39m, name\u001b[39m=\u001b[39mname,\n\u001b[1;32m   1663\u001b[0m         message\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39m%(name)s\u001b[39;00m\u001b[39m() got unexpected keyword argument \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1664\u001b[0m                 \u001b[39m+\u001b[39m arg \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m which is no longer supported as of \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1665\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%(since)s\u001b[39;00m\u001b[39m and will become an error \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m   1666\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m%(removal)s\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m   1667\u001b[0m     kwargs\u001b[39m.\u001b[39mpop(arg)\n\u001b[0;32m-> 1669\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:508\u001b[0m, in \u001b[0;36mFigureCanvasAgg.print_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs, *args)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[39m@_check_savefig_extra_args\u001b[39m\n\u001b[1;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mprint_png\u001b[39m(\u001b[39mself\u001b[39m, filename_or_obj, \u001b[39m*\u001b[39margs,\n\u001b[1;32m    461\u001b[0m               metadata\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, pil_kwargs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    462\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    463\u001b[0m \u001b[39m    Write the figure to a PNG file.\u001b[39;00m\n\u001b[1;32m    464\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[39m        *metadata*, including the default 'Software' key.\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 508\u001b[0m     FigureCanvasAgg\u001b[39m.\u001b[39;49mdraw(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    509\u001b[0m     mpl\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mimsave(\n\u001b[1;32m    510\u001b[0m         filename_or_obj, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuffer_rgba(), \u001b[39mformat\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpng\u001b[39m\u001b[39m\"\u001b[39m, origin\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mupper\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    511\u001b[0m         dpi\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfigure\u001b[39m.\u001b[39mdpi, metadata\u001b[39m=\u001b[39mmetadata, pil_kwargs\u001b[39m=\u001b[39mpil_kwargs)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:401\u001b[0m, in \u001b[0;36mFigureCanvasAgg.draw\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdraw\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    400\u001b[0m     \u001b[39m# docstring inherited\u001b[39;00m\n\u001b[0;32m--> 401\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrenderer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_renderer(cleared\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m    402\u001b[0m     \u001b[39m# Acquire a lock on the shared font cache.\u001b[39;00m\n\u001b[1;32m    403\u001b[0m     \u001b[39mwith\u001b[39;00m RendererAgg\u001b[39m.\u001b[39mlock, \\\n\u001b[1;32m    404\u001b[0m          (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\u001b[39m.\u001b[39m_wait_cursor_for_draw_cm() \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtoolbar\n\u001b[1;32m    405\u001b[0m           \u001b[39melse\u001b[39;00m nullcontext()):\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:417\u001b[0m, in \u001b[0;36mFigureCanvasAgg.get_renderer\u001b[0;34m(self, cleared)\u001b[0m\n\u001b[1;32m    414\u001b[0m reuse_renderer \u001b[39m=\u001b[39m (\u001b[39mhasattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrenderer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    415\u001b[0m                   \u001b[39mand\u001b[39;00m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m_lastKey\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m key)\n\u001b[1;32m    416\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m reuse_renderer:\n\u001b[0;32m--> 417\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrenderer \u001b[39m=\u001b[39m RendererAgg(w, h, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfigure\u001b[39m.\u001b[39;49mdpi)\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lastKey \u001b[39m=\u001b[39m key\n\u001b[1;32m    419\u001b[0m \u001b[39melif\u001b[39;00m cleared:\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/matplotlib/backends/backend_agg.py:91\u001b[0m, in \u001b[0;36mRendererAgg.__init__\u001b[0;34m(self, width, height, dpi)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m width\n\u001b[1;32m     90\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mheight \u001b[39m=\u001b[39m height\n\u001b[0;32m---> 91\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_renderer \u001b[39m=\u001b[39m _RendererAgg(\u001b[39mint\u001b[39;49m(width), \u001b[39mint\u001b[39;49m(height), dpi)\n\u001b[1;32m     92\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_filter_renderers \u001b[39m=\u001b[39m []\n\u001b[1;32m     94\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_update_methods()\n",
      "\u001b[0;31mValueError\u001b[0m: Image size of 547x134913 pixels is too large. It must be less than 2^16 in each direction."
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for r in range(len(falling_data)):\n",
    "    x = np.arange(16)\n",
    "    y = falling_data[r]\n",
    "    plt.plot(x,y,'r',label=\"falling\")\n",
    "for r in range(len(default_data)):\n",
    "    x = np.arange(16)\n",
    "    y = default_data[r]\n",
    "    plt.plot(x,y,'b',label=\"default\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 991us/step - loss: 0.3782 - accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "# model.save(\"new_model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgy6fdty2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:16:44.155686: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2023-06-07 11:16:44.445558: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 11:16:44.445580: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 11:16:44.446467: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.447205: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 11:16:44.447233: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.449735: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 11:16:44.489124: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.496765: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 50301 microseconds.\n",
      "2023-06-07 11:16:44.509424: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "### CONVERT TO TFLITE\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model2_tflite = converter.convert()\n",
    "with open('new_model2.tflite', 'wb') as f: f.write(model2_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-07 15:22:06.987031: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 15:22:06.987052: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 15:22:06.987225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.987830: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 15:22:06.987841: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.989672: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 15:22:07.012328: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:07.019440: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 32217 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "### QUANTIZE\n",
    "def representative_dataset():\n",
    "  for d in inputs:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('new_model2_quant', 'wb') as f: f.write(tflite_quant_model2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
