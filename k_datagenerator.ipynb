{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Setup\n",
    "Model 1 takes in img outputs moving probability\n",
    "Model 2 takes in a sequence of 16 moving probs outputs falling probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras import layers\n",
    "import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU limitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 15:27:49.430793: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.446544: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.446700: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.447689: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-06 15:27:49.448029: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.448159: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.448269: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.843292: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.843458: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.843568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-07-06 15:27:49.843654: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 2048 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 4362, num_label: 2, labels: Moving, still.\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader.from_folder(\"./datasets/model1_data\")\n",
    "train_data, test_data = data.split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n",
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " hub_keras_layer_v1v2 (HubKe  (None, 1280)             3413024   \n",
      " rasLayerV1V2)                                                   \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 2)                 2562      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,415,586\n",
      "Trainable params: 2,562\n",
      "Non-trainable params: 3,413,024\n",
      "_________________________________________________________________\n",
      "None\n",
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use default resize_bicubic.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Use customized resize method bilinear\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-06 15:16:30.610437: E tensorflow/stream_executor/cuda/cuda_dnn.cc:373] Could not create cudnn handle: CUDNN_STATUS_INTERNAL_ERROR\n",
      "2023-07-06 15:16:30.610488: W tensorflow/core/framework/op_kernel.cc:1745] OP_REQUIRES failed at conv_ops.cc:1120 : UNIMPLEMENTED: DNN library is not found.\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'efficientnet-lite0/model/stem/conv2d/Conv2D' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_53644/1475952295.py\", line 1, in <module>\n      model = image_classifier.create(train_data, use_augmentation=True)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 339, in create\n      image_classifier.train(train_data, validation_data, steps_per_epoch)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 160, in train\n      self.create_model()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 129, in create_model\n      module_layer = hub_loader.HubKerasLayerV1V2(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n      self._func = load_module(handle, tags, self._load_options)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n      return module_v2.load(handle, tags=tags, options=set_load_options)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/module_v2.py\", line 106, in load\n      obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\nNode: 'efficientnet-lite0/model/stem/conv2d/Conv2D'\nDNN library is not found.\n\t [[{{node efficientnet-lite0/model/stem/conv2d/Conv2D}}]] [Op:__inference_train_function_15931]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m image_classifier\u001b[39m.\u001b[39;49mcreate(train_data, use_augmentation\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py:339\u001b[0m, in \u001b[0;36mImageClassifier.create\u001b[0;34m(cls, train_data, model_spec, validation_data, batch_size, epochs, steps_per_epoch, train_whole_model, dropout_rate, learning_rate, momentum, shuffle, use_augmentation, use_hub_library, warmup_steps, model_dir, do_train)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m do_train:\n\u001b[1;32m    338\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRetraining the models...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 339\u001b[0m   image_classifier\u001b[39m.\u001b[39;49mtrain(train_data, validation_data, steps_per_epoch)\n\u001b[1;32m    340\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m   \u001b[39m# Used in evaluation.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m   image_classifier\u001b[39m.\u001b[39mcreate_model(with_loss_and_metrics\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py:191\u001b[0m, in \u001b[0;36mImageClassifier.train\u001b[0;34m(self, train_data, validation_data, hparams, steps_per_epoch)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    189\u001b[0m   train_model \u001b[39m=\u001b[39m train_image_classifier_lib\u001b[39m.\u001b[39mhub_train_model\n\u001b[0;32m--> 191\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhistory \u001b[39m=\u001b[39m train_model(\n\u001b[1;32m    192\u001b[0m     model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel,\n\u001b[1;32m    193\u001b[0m     hparams\u001b[39m=\u001b[39;49mhparams,\n\u001b[1;32m    194\u001b[0m     train_ds\u001b[39m=\u001b[39;49mtrain_ds,\n\u001b[1;32m    195\u001b[0m     validation_ds\u001b[39m=\u001b[39;49mvalidation_ds,\n\u001b[1;32m    196\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/train_image_classifier_lib.py:140\u001b[0m, in \u001b[0;36mhub_train_model\u001b[0;34m(model, hparams, train_ds, validation_ds, steps_per_epoch)\u001b[0m\n\u001b[1;32m    132\u001b[0m loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mlosses\u001b[39m.\u001b[39mCategoricalCrossentropy(\n\u001b[1;32m    133\u001b[0m     label_smoothing\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mlabel_smoothing)\n\u001b[1;32m    134\u001b[0m model\u001b[39m.\u001b[39mcompile(\n\u001b[1;32m    135\u001b[0m     optimizer\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39moptimizers\u001b[39m.\u001b[39mSGD(\n\u001b[1;32m    136\u001b[0m         learning_rate\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mlearning_rate, momentum\u001b[39m=\u001b[39mhparams\u001b[39m.\u001b[39mmomentum),\n\u001b[1;32m    137\u001b[0m     loss\u001b[39m=\u001b[39mloss,\n\u001b[1;32m    138\u001b[0m     metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39maccuracy\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m--> 140\u001b[0m \u001b[39mreturn\u001b[39;00m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m    141\u001b[0m     train_ds,\n\u001b[1;32m    142\u001b[0m     epochs\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mtrain_epochs,\n\u001b[1;32m    143\u001b[0m     steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    144\u001b[0m     validation_data\u001b[39m=\u001b[39;49mvalidation_ds)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39mTFE_Py_Execute(ctx\u001b[39m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'efficientnet-lite0/model/stem/conv2d/Conv2D' defined at (most recent call last):\n    File \"/usr/lib/python3.8/runpy.py\", line 194, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.8/runpy.py\", line 87, in _run_code\n      exec(code, run_globals)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelapp.py\", line 728, in start\n      self.io_loop.start()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 570, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.8/asyncio/base_events.py\", line 1859, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.8/asyncio/events.py\", line 81, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 516, in dispatch_queue\n      await self.process_one()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 505, in process_one\n      await dispatch(*args)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 412, in dispatch_shell\n      await result\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/kernelbase.py\", line 740, in execute_request\n      reply_content = await reply_content\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/kevin/.local/lib/python3.8/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_53644/1475952295.py\", line 1, in <module>\n      model = image_classifier.create(train_data, use_augmentation=True)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 339, in create\n      image_classifier.train(train_data, validation_data, steps_per_epoch)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 160, in train\n      self.create_model()\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py\", line 129, in create_model\n      module_layer = hub_loader.HubKerasLayerV1V2(\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 153, in __init__\n      self._func = load_module(handle, tags, self._load_options)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py\", line 449, in load_module\n      return module_v2.load(handle, tags=tags, options=set_load_options)\n    File \"/home/kevin/.local/lib/python3.8/site-packages/tensorflow_hub/module_v2.py\", line 106, in load\n      obj = tf.compat.v1.saved_model.load_v2(module_path, tags=tags)\nNode: 'efficientnet-lite0/model/stem/conv2d/Conv2D'\nDNN library is not found.\n\t [[{{node efficientnet-lite0/model/stem/conv2d/Conv2D}}]] [Op:__inference_train_function_15931]"
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data, use_augmentation=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to find data adapter that can handle input: <class 'tensorflow_examples.lite.model_maker.core.data_util.image_dataloader.ImageClassifierDataLoader'>, <class 'NoneType'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m loss, accuracy \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mevaluate(test_data)\n\u001b[1;32m      2\u001b[0m \u001b[39m# model.summary()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/keras/engine/data_adapter.py:984\u001b[0m, in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    981\u001b[0m adapter_cls \u001b[39m=\u001b[39m [\u001b[39mcls\u001b[39m \u001b[39mfor\u001b[39;00m \u001b[39mcls\u001b[39m \u001b[39min\u001b[39;00m ALL_ADAPTER_CLS \u001b[39mif\u001b[39;00m \u001b[39mcls\u001b[39m\u001b[39m.\u001b[39mcan_handle(x, y)]\n\u001b[1;32m    982\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m adapter_cls:\n\u001b[1;32m    983\u001b[0m   \u001b[39m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[39;00m\n\u001b[0;32m--> 984\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    985\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mFailed to find data adapter that can handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    986\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    987\u001b[0m           _type_name(x), _type_name(y)))\n\u001b[1;32m    988\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mlen\u001b[39m(adapter_cls) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m    989\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m    990\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mData adapters should be mutually exclusive for \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    991\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mhandling inputs. Found multiple adapters \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m to handle \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39minput: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m    993\u001b[0m           adapter_cls, _type_name(x), _type_name(y)))\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: <class 'tensorflow_examples.lite.model_maker.core.data_util.image_dataloader.ImageClassifierDataLoader'>, <class 'NoneType'>"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### QUANTIZE\n",
    "# method 1: save as savedmodel then load and quantize like model2 did\n",
    "# method 2: change datatype before saving\n",
    "'''\n",
    "def representative_dataset():\n",
    "  for d in test_data:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('model2_quant', 'wb') as f: f.write(tflite_quant_model2)\n",
    "'''"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch predicting multiple vids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_falling.mp4\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_falling.mp4\"]\n",
    "\n",
    "default_paths = [\"./datasets/model1_vids/splitted_for_model2/resized_IMG0480_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0484_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG0485_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1614_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1615_default.MOV\",\n",
    "                 \"./datasets/model1_vids/splitted_for_model2/resized_IMG1616_default.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_jump.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_pickup.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_run4.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk1.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk2.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk3.MOV\",\n",
    "                 \"./datasets/model2_vids/resized_jess_walk4.MOV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchPredict(path):\n",
    "    cap = cv2.VideoCapture(path)\n",
    "    probs = []\n",
    "    while True:\n",
    "        ret, frame = cap.read() \n",
    "        if not ret: break\n",
    "\n",
    "        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "        interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "        interpreter.allocate_tensors()\n",
    "        output = interpreter.get_output_details()\n",
    "        input = interpreter.get_input_details()\n",
    "        output_index = output[0]['index']\n",
    "        input_index = input[0]['index']\n",
    "\n",
    "        interpreter.set_tensor(input_index, frame_rgb)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_index)\n",
    "        output_data = output_data[0]\n",
    "        output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "        predicted_index = np.argmax(output_data)\n",
    "        class_labels = [\"Moving\", \"Still\"]\n",
    "        predicted_class = class_labels[predicted_index]        \n",
    "        prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "\n",
    "        if predicted_class == \"Still\": probs.append(np.subtract(1, prob))\n",
    "        else: probs.append(prob)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batchPredictMultVids(vid_path_lst):\n",
    "    record = []\n",
    "    for vid in vid_path_lst:\n",
    "        vid_probs = batchPredict(vid)\n",
    "        print(vid)\n",
    "        for i in range(len(vid_probs)-15):\n",
    "            record.append(vid_probs[i:i+16])\n",
    "            for j in range(i, i+16):\n",
    "                print(vid_probs[j], end=\" \")\n",
    "            print()\n",
    "        print()\n",
    "    return record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "falling = batchPredictMultVids(falling_paths)\n",
    "default = batchPredictMultVids(default_paths)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 0. ... 1. 1. 1.]\n",
      " [0. 0. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "default_data = np.loadtxt('./datasets/model2_data/default.txt')\n",
    "falling_data = np.loadtxt('./datasets/model2_data/falling.txt')\n",
    "\n",
    "print(falling_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6270, 16) (6270, 16)\n"
     ]
    }
   ],
   "source": [
    "mu, sig = 0, 0.001\n",
    "copy_count = len(default_data)-len(falling_data) #difference in default and length \n",
    "oldLen = len(falling_data)\n",
    "rowi = 0\n",
    "\n",
    "limit = lambda x : max(min(x,1), 0)\n",
    "\n",
    "#function to factor out abnormal vals\n",
    "def limit(x):\n",
    "    if x > 1: return 1 - (x-1)\n",
    "    elif x < 0: return abs(x)\n",
    "    return x\n",
    "\n",
    "#normalize data since the falling data's count is not as much as the normal \n",
    "newLimit = np.vectorize(limit) \n",
    "\n",
    "for i in range(copy_count):\n",
    "    row = falling_data[rowi] + np.random.normal(mu, sig, 16)\n",
    "    row = newLimit(row)\n",
    "\n",
    "    falling_data = np.vstack([falling_data, row])\n",
    "    rowi += 1\n",
    "    if rowi >= oldLen: rowi = 0 #finish one row \n",
    "\n",
    "print(falling_data.shape, default_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 0s 914us/step - loss: 0.5813 - sparse_categorical_accuracy: 0.8525\n",
      "[0.5813300013542175, 0.8524720668792725]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# loading data\n",
    "inputs = np.concatenate((default_data, falling_data))\n",
    "outputs = np.concatenate((np.zeros(len(default_data)), np.ones(len(falling_data)))) #ones are falling , zeros are default\n",
    "dataset_size = len(inputs)\n",
    "new_indices = np.random.permutation(dataset_size) # shuffle indices to shuffle X and y at the same time\n",
    "inputs, outputs = inputs[new_indices], outputs[new_indices]\n",
    "\n",
    "train_size = int(0.8*dataset_size)\n",
    "test_size = dataset_size - train_size\n",
    "\n",
    "\n",
    "# train_data = tf.data.Dataset.from_tensor_slices((inputs[:train_size], outputs[:train_size])).batch(128)\n",
    "X_train, y_train = inputs[:train_size], outputs[:train_size] #x = image, y = label\n",
    "X_test, y_test = inputs[train_size:], outputs[train_size:]\n",
    "\n",
    "def shuffle_generator(image, label, seed):\n",
    "    idx = np.arange(len(image))\n",
    "    np.random.default_rng(seed).shuffle(idx)\n",
    "    for i in idx:\n",
    "        yield image[i], label[i]\n",
    "\n",
    " \n",
    "dataset = tf.data.Dataset.from_generator(\n",
    "    shuffle_generator,\n",
    "    args=[X_train, y_train, 42],\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(16,), dtype=tf.uint8),\n",
    "        tf.TensorSpec(shape=(), dtype=tf.uint8)))\n",
    "\n",
    " \n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(100, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "\n",
    "              metrics=\"sparse_categorical_accuracy\")\n",
    "\n",
    "history = model.fit(dataset.batch(32),\n",
    "                    epochs=50,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    verbose=0)\n",
    "\n",
    " \n",
    "\n",
    "print(model.evaluate(X_test, y_test))\n",
    "\n",
    " \n",
    "\n",
    "plt.plot(history.history['val_sparse_categorical_accuracy'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(128, activation='relu', input_shape=(16,)),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorboard\n",
    "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2915 - sparse_categorical_accuracy: 0.9016\n",
      "Epoch 2/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9022\n",
      "Epoch 3/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9025\n",
      "Epoch 4/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2901 - sparse_categorical_accuracy: 0.9011\n",
      "Epoch 5/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2885 - sparse_categorical_accuracy: 0.9035\n",
      "Epoch 6/6\n",
      "314/314 [==============================] - 0s 1ms/step - loss: 0.2897 - sparse_categorical_accuracy: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f8380221e80>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=X_train, y=y_train, epochs=6,callbacks=[tensorboard_callback])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 0s 991us/step - loss: 0.3782 - accuracy: 0.8485\n"
     ]
    }
   ],
   "source": [
    "loss, acc = model.evaluate(X_test, y_test)\n",
    "# model.summary()\n",
    "# model.save(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpgy6fdty2/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-07 11:16:44.155686: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n",
      "2023-06-07 11:16:44.445558: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 11:16:44.445580: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 11:16:44.446467: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.447205: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 11:16:44.447233: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.449735: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 11:16:44.489124: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpgy6fdty2\n",
      "2023-06-07 11:16:44.496765: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 50301 microseconds.\n",
      "2023-06-07 11:16:44.509424: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:237] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "### CONVERT TO TFLITE\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "model2_tflite = converter.convert()\n",
    "with open('model2.tflite', 'wb') as f: f.write(model2_tflite)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpdvgpu7f5/assets\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-06-07 15:22:06.987031: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-06-07 15:22:06.987052: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "2023-06-07 15:22:06.987225: I tensorflow/cc/saved_model/reader.cc:43] Reading SavedModel from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.987830: I tensorflow/cc/saved_model/reader.cc:78] Reading meta graph with tags { serve }\n",
      "2023-06-07 15:22:06.987841: I tensorflow/cc/saved_model/reader.cc:119] Reading SavedModel debug info (if present) from: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:06.989672: I tensorflow/cc/saved_model/loader.cc:228] Restoring SavedModel bundle.\n",
      "2023-06-07 15:22:07.012328: I tensorflow/cc/saved_model/loader.cc:212] Running initialization op on SavedModel bundle at path: /tmp/tmpdvgpu7f5\n",
      "2023-06-07 15:22:07.019440: I tensorflow/cc/saved_model/loader.cc:301] SavedModel load for tags { serve }; Status: success: OK. Took 32217 microseconds.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    }
   ],
   "source": [
    "### QUANTIZE\n",
    "def representative_dataset():\n",
    "  for d in inputs:\n",
    "    # d = np.expand_dims(d, axis=0)\n",
    "    yield [tf.dtypes.cast(d, tf.float32)]\n",
    "\n",
    "# print(dataset.cardinality().numpy())\n",
    "# print(tf.shape(dataset))\n",
    "# model.summary()\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "converter.representative_dataset = representative_dataset\n",
    "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "converter.inference_input_type = tf.uint8\n",
    "converter.inference_output_type = tf.uint8\n",
    "tflite_quant_model2 = converter.convert()\n",
    "with open('model2_quant', 'wb') as f: f.write(tflite_quant_model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reformat Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.97\n"
     ]
    }
   ],
   "source": [
    "# Resize vid\n",
    "video_path = \"compilation_cut.mp4\"\n",
    "output_path = \"compilation_cut_resized.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "#resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store the frame classifications\n",
    "frame_classifications = []\n",
    "moving_prob = []\n",
    "# Loop through the frames of the video (need to change to 30 fps)\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    #just need to figure out if this is 30 fps\n",
    "\n",
    "    if not ret: # Break the loop if the video has ended\n",
    "        break\n",
    "\n",
    "    ''' Formulate Input Data (frame_rgb) '''\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Make frame input data and ensure its type matches the model\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    ''' Classify the Frame '''\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "    # keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "    # len(input) = len(output) = 1, so access the first element\n",
    "    output = interpreter.get_output_details()\n",
    "    input = interpreter.get_input_details()\n",
    "    output_index = output[0]['index']\n",
    "    input_index = input[0]['index']\n",
    "\n",
    "    # set input -> invoke -> access output\n",
    "    interpreter.set_tensor(input_index, frame_rgb)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    # If the output_data shape is (batch_size, num_classes), select the first frame\n",
    "    output_data = output_data[0]\n",
    "\n",
    "    # Convert each entry into probability\n",
    "    output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    predicted_index = np.argmax(output_data)\n",
    "\n",
    "    # Assuming you have a list of class labels corresponding to the model's output classes\n",
    "    class_labels = [\"Moving\", \"Still\"]\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = class_labels[predicted_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    # print(\"Predicted Class:\", predicted_class)\n",
    "    frame_classifications.append((predicted_class, max(output_probs.numpy())))\n",
    "    \n",
    "    prob = np.around(max(output_probs.numpy()), decimals = 2)\n",
    "    if predicted_class == \"Still\":\n",
    "        \n",
    "        moving_prob.append(np.subtract(1, prob))\n",
    "    else:\n",
    "        moving_prob.append(prob)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   0.  ]\n",
      " [0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   0.   1.   0.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   0.   0.   1.   1.   1.   0.   1.   1.   0.\n",
      "  1.   1.  ]\n",
      " [0.   0.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.   1.   0.5\n",
      "  1.   1.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [1.   0.   0.   0.   1.   1.   1.   1.   1.   1.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   1.   0.5  1.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.12 0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   1.   0.\n",
      "  0.12 0.  ]\n",
      " [0.5  0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   1.   1.   1.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.88 0.   0.   0.12 0.   0.88 0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [1.   1.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.5  0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   1.   1.   1.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   1.   0.   1.   1.   0.   0.\n",
      "  0.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   0.   1.   1.   1.   0.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   1.\n",
      "  1.   1.  ]\n",
      " [1.   0.5  1.   1.   0.88 1.   1.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   1.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]\n",
      " [0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      "  0.   0.  ]]\n"
     ]
    }
   ],
   "source": [
    "moving_probs_trimmed = moving_prob[:-(len(moving_prob)%16)]\n",
    "model2_in = np.array(moving_probs_trimmed).reshape((len(moving_prob)//16, 16))\n",
    "print(model2_in)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pass into Model 2 and get final prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = tf.keras.models.load_model(\"model2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_preds = model2.predict(model2_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False False False False False False\n",
      "   True  True False False False False False False False False False False\n",
      "  False False False False False False False False False False False False\n",
      "  False False False False False False False  True False  True  True False\n",
      "  False False False False False False False False False False False False\n",
      "  False False]]\n",
      "[[0.09045138 0.09045138 0.09045138 0.09045138 0.09045138 0.5739342\n",
      "  0.30349308 0.7925048  0.7975426  0.51072216 0.7444853  0.7925048\n",
      "  0.929349   0.929349   0.8644172  0.09045138 0.15479052 0.29671496\n",
      "  0.09045138 0.15544447 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.17014171 0.12607063 0.09045138 0.09045138 0.19399568\n",
      "  0.09045138 0.09045138 0.16418059 0.14767478 0.15647146 0.12583217\n",
      "  0.09045138 0.09045138 0.18011312 0.09045138 0.11525431 0.17111975\n",
      "  0.26666683 0.929349   0.80626893 0.929349   0.929349   0.2670536\n",
      "  0.16497837 0.09045138 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.09045138 0.09045138 0.09045138 0.09045138 0.09045138\n",
      "  0.09045138 0.09045138]]\n"
     ]
    }
   ],
   "source": [
    "threshold = 0.88\n",
    "bools = vid_preds.reshape((1, len(vid_preds))) > threshold\n",
    "print(bools)\n",
    "print(vid_preds.reshape((1, len(vid_preds))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
