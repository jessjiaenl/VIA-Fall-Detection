{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "assert tf.__version__.startswith('2')\n",
    "\n",
    "from tflite_model_maker import model_spec\n",
    "from tflite_model_maker import image_classifier\n",
    "from tflite_model_maker.config import ExportFormat\n",
    "from tflite_model_maker.config import QuantizationConfig\n",
    "from tflite_model_maker.image_classifier import DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 2GB of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.set_logical_device_configuration(gpus[0], [tf.config.LogicalDeviceConfiguration(memory_limit=2048)])\n",
    "        logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Load image with size: 2917, num_label: 2, labels: Moving, still.\n"
     ]
    }
   ],
   "source": [
    "data = DataLoader.from_folder(r\"./datasets/tensorflow/photos/\")\n",
    "train_data, test_data = data.split(0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Retraining the models...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[39m=\u001b[39m image_classifier\u001b[39m.\u001b[39;49mcreate(train_data)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py:339\u001b[0m, in \u001b[0;36mImageClassifier.create\u001b[0;34m(cls, train_data, model_spec, validation_data, batch_size, epochs, steps_per_epoch, train_whole_model, dropout_rate, learning_rate, momentum, shuffle, use_augmentation, use_hub_library, warmup_steps, model_dir, do_train)\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[39mif\u001b[39;00m do_train:\n\u001b[1;32m    338\u001b[0m   tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39mlogging\u001b[39m.\u001b[39minfo(\u001b[39m'\u001b[39m\u001b[39mRetraining the models...\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m--> 339\u001b[0m   image_classifier\u001b[39m.\u001b[39;49mtrain(train_data, validation_data, steps_per_epoch)\n\u001b[1;32m    340\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    341\u001b[0m   \u001b[39m# Used in evaluation.\u001b[39;00m\n\u001b[1;32m    342\u001b[0m   image_classifier\u001b[39m.\u001b[39mcreate_model(with_loss_and_metrics\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py:160\u001b[0m, in \u001b[0;36mImageClassifier.train\u001b[0;34m(self, train_data, validation_data, hparams, steps_per_epoch)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m,\n\u001b[1;32m    141\u001b[0m           train_data,\n\u001b[1;32m    142\u001b[0m           validation_data\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    143\u001b[0m           hparams\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    144\u001b[0m           steps_per_epoch\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    145\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Feeds the training data for training.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \n\u001b[1;32m    147\u001b[0m \u001b[39m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m    The tf.keras.callbacks.History object returned by tf.keras.Model.fit*().\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_model()\n\u001b[1;32m    161\u001b[0m   hparams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_hparams_or_default(hparams)\n\u001b[1;32m    163\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(train_data) \u001b[39m<\u001b[39m hparams\u001b[39m.\u001b[39mbatch_size:\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_examples/lite/model_maker/core/task/image_classifier.py:129\u001b[0m, in \u001b[0;36mImageClassifier.create_model\u001b[0;34m(self, hparams, with_loss_and_metrics)\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Creates the classifier model for retraining.\"\"\"\u001b[39;00m\n\u001b[1;32m    127\u001b[0m hparams \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_hparams_or_default(hparams)\n\u001b[0;32m--> 129\u001b[0m module_layer \u001b[39m=\u001b[39m hub_loader\u001b[39m.\u001b[39;49mHubKerasLayerV1V2(\n\u001b[1;32m    130\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel_spec\u001b[39m.\u001b[39;49muri, trainable\u001b[39m=\u001b[39;49mhparams\u001b[39m.\u001b[39;49mdo_fine_tuning)\n\u001b[1;32m    131\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel \u001b[39m=\u001b[39m hub_lib\u001b[39m.\u001b[39mbuild_model(module_layer, hparams,\n\u001b[1;32m    132\u001b[0m                                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_spec\u001b[39m.\u001b[39minput_image_shape,\n\u001b[1;32m    133\u001b[0m                                  \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes)\n\u001b[1;32m    134\u001b[0m \u001b[39mif\u001b[39;00m with_loss_and_metrics:\n\u001b[1;32m    135\u001b[0m   \u001b[39m# Adds loss and metrics in the keras model.\u001b[39;00m\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:153\u001b[0m, in \u001b[0;36mKerasLayer.__init__\u001b[0;34m(self, handle, trainable, arguments, _sentinel, tags, signature, signature_outputs_as_dict, output_key, output_shape, load_options, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_output_shape \u001b[39m=\u001b[39m data_structures\u001b[39m.\u001b[39mNoDependency(\n\u001b[1;32m    150\u001b[0m       _convert_nest_to_shapes(output_shape))\n\u001b[1;32m    152\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_load_options \u001b[39m=\u001b[39m load_options\n\u001b[0;32m--> 153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func \u001b[39m=\u001b[39m load_module(handle, tags, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_load_options)\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_training_argument \u001b[39m=\u001b[39m func_has_training_argument(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func)\n\u001b[1;32m    155\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_is_hub_module_v1 \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_func, \u001b[39m\"\u001b[39m\u001b[39m_is_hub_module_v1\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_hub/keras_layer.py:449\u001b[0m, in \u001b[0;36mload_module\u001b[0;34m(handle, tags, load_options)\u001b[0m\n\u001b[1;32m    447\u001b[0m   \u001b[39mexcept\u001b[39;00m \u001b[39mImportError\u001b[39;00m:  \u001b[39m# Expected before TF2.4.\u001b[39;00m\n\u001b[1;32m    448\u001b[0m     set_load_options \u001b[39m=\u001b[39m load_options\n\u001b[0;32m--> 449\u001b[0m \u001b[39mreturn\u001b[39;00m module_v2\u001b[39m.\u001b[39;49mload(handle, tags\u001b[39m=\u001b[39;49mtags, options\u001b[39m=\u001b[39;49mset_load_options)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow_hub/module_v2.py:106\u001b[0m, in \u001b[0;36mload\u001b[0;34m(handle, tags, options)\u001b[0m\n\u001b[1;32m    103\u001b[0m   obj \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mcompat\u001b[39m.\u001b[39mv1\u001b[39m.\u001b[39msaved_model\u001b[39m.\u001b[39mload_v2(\n\u001b[1;32m    104\u001b[0m       module_path, tags\u001b[39m=\u001b[39mtags, options\u001b[39m=\u001b[39moptions)\n\u001b[1;32m    105\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 106\u001b[0m   obj \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mcompat\u001b[39m.\u001b[39;49mv1\u001b[39m.\u001b[39;49msaved_model\u001b[39m.\u001b[39;49mload_v2(module_path, tags\u001b[39m=\u001b[39;49mtags)\n\u001b[1;32m    107\u001b[0m obj\u001b[39m.\u001b[39m_is_hub_module_v1 \u001b[39m=\u001b[39m is_hub_module_v1  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[39mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:936\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags, options)\u001b[0m\n\u001b[1;32m    845\u001b[0m \u001b[39m@tf_export\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39msaved_model.load\u001b[39m\u001b[39m\"\u001b[39m, v1\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39msaved_model.load_v2\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m    846\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(export_dir, tags\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, options\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    847\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Load a SavedModel from `export_dir`.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m \n\u001b[1;32m    849\u001b[0m \u001b[39m  Signatures associated with the SavedModel are available as functions:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    934\u001b[0m \u001b[39m    ValueError: If `tags` don't match a MetaGraph in the SavedModel.\u001b[39;00m\n\u001b[1;32m    935\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 936\u001b[0m   result \u001b[39m=\u001b[39m load_internal(export_dir, tags, options)[\u001b[39m\"\u001b[39m\u001b[39mroot\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    937\u001b[0m   \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load.py:994\u001b[0m, in \u001b[0;36mload_internal\u001b[0;34m(export_dir, tags, options, loader_cls, filters)\u001b[0m\n\u001b[1;32m    991\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mSavedModels saved from Tensorflow 1.x or Estimator (any\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    992\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39m version) cannot be loaded with node filters.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    993\u001b[0m   \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39minit_scope():\n\u001b[0;32m--> 994\u001b[0m     root \u001b[39m=\u001b[39m load_v1_in_v2\u001b[39m.\u001b[39;49mload(export_dir, tags)\n\u001b[1;32m    995\u001b[0m     root\u001b[39m.\u001b[39mgraph_debug_info \u001b[39m=\u001b[39m debug_info\n\u001b[1;32m    997\u001b[0m \u001b[39mif\u001b[39;00m filters:\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:282\u001b[0m, in \u001b[0;36mload\u001b[0;34m(export_dir, tags)\u001b[0m\n\u001b[1;32m    280\u001b[0m metrics\u001b[39m.\u001b[39mIncrementReadApi(_LOAD_V1_V2_LABEL)\n\u001b[1;32m    281\u001b[0m loader \u001b[39m=\u001b[39m _EagerSavedModelLoader(export_dir)\n\u001b[0;32m--> 282\u001b[0m result \u001b[39m=\u001b[39m loader\u001b[39m.\u001b[39;49mload(tags\u001b[39m=\u001b[39;49mtags)\n\u001b[1;32m    283\u001b[0m metrics\u001b[39m.\u001b[39mIncrementRead(write_version\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    284\u001b[0m \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:230\u001b[0m, in \u001b[0;36m_EagerSavedModelLoader.load\u001b[0;34m(self, tags)\u001b[0m\n\u001b[1;32m    228\u001b[0m saver, \u001b[39m=\u001b[39m load_graph_returns\n\u001b[1;32m    229\u001b[0m restore_from_saver \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_saver_restore(wrapped, saver)\n\u001b[0;32m--> 230\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrestore_variables(wrapped, restore_from_saver)\n\u001b[1;32m    231\u001b[0m \u001b[39mwith\u001b[39;00m wrapped\u001b[39m.\u001b[39mgraph\u001b[39m.\u001b[39mas_default():\n\u001b[1;32m    232\u001b[0m   init_op \u001b[39m=\u001b[39m loader_impl\u001b[39m.\u001b[39mget_init_op(\n\u001b[1;32m    233\u001b[0m       meta_graph_def) \u001b[39mor\u001b[39;00m monitored_session\u001b[39m.\u001b[39mScaffold\u001b[39m.\u001b[39mdefault_local_init_op()\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/saved_model/load_v1_in_v2.py:113\u001b[0m, in \u001b[0;36m_EagerSavedModelLoader.restore_variables\u001b[0;34m(self, wrapped, restore_from_saver)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Restores variables from the checkpoint.\"\"\"\u001b[39;00m\n\u001b[1;32m    112\u001b[0m \u001b[39mif\u001b[39;00m restore_from_saver \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 113\u001b[0m   initializer, _ \u001b[39m=\u001b[39m restore_from_saver(\n\u001b[1;32m    114\u001b[0m       constant_op\u001b[39m.\u001b[39;49mconstant(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_variables_path))\n\u001b[1;32m    115\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m ops\u001b[39m.\u001b[39mexecuting_eagerly_outside_functions():\n\u001b[1;32m    116\u001b[0m     \u001b[39m# Add the initialization operation to the \"saved_model_initializers\"\u001b[39;00m\n\u001b[1;32m    117\u001b[0m     \u001b[39m# collection in case we don't have any lifted variables to attach it to.\u001b[39;00m\n\u001b[1;32m    118\u001b[0m     ops\u001b[39m.\u001b[39madd_to_collection(\u001b[39m\"\u001b[39m\u001b[39msaved_model_initializers\u001b[39m\u001b[39m\"\u001b[39m, initializer)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1601\u001b[0m, in \u001b[0;36mConcreteFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m   1552\u001b[0m \u001b[39m  \u001b[39m\u001b[39m\"\"\"Executes the wrapped function.\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \n\u001b[1;32m   1554\u001b[0m \u001b[39m  ConcreteFunctions have two signatures:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1599\u001b[0m \u001b[39m    TypeError: If the arguments do not match the function's signature.\u001b[39;00m\n\u001b[1;32m   1600\u001b[0m \u001b[39m  \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1601\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(args, kwargs)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/wrap_function.py:243\u001b[0m, in \u001b[0;36mWrappedFunction._call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_flat(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcaptured_inputs)\n\u001b[1;32m    242\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 243\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m(WrappedFunction, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m_call_impl(\n\u001b[1;32m    244\u001b[0m       args, kwargs, cancellation_manager)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1619\u001b[0m, in \u001b[0;36mConcreteFunction._call_impl\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   1617\u001b[0m       \u001b[39mraise\u001b[39;00m structured_err\n\u001b[0;32m-> 1619\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_with_flat_signature(args, kwargs, cancellation_manager)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1668\u001b[0m, in \u001b[0;36mConcreteFunction._call_with_flat_signature\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1663\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[1;32m   1664\u001b[0m       arg, (ops\u001b[39m.\u001b[39mTensor, resource_variable_ops\u001b[39m.\u001b[39mBaseResourceVariable)):\n\u001b[1;32m   1665\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_flat_signature_summary()\u001b[39m}\u001b[39;00m\u001b[39m: expected argument \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1666\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m#\u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m(zero-based) to be a Tensor; \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1667\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgot \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(arg)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m (\u001b[39m\u001b[39m{\u001b[39;00marg\u001b[39m}\u001b[39;00m\u001b[39m).\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m-> 1668\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_flat(args, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcaptured_inputs, cancellation_manager)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:1853\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1849\u001b[0m possible_gradient_type \u001b[39m=\u001b[39m gradients_util\u001b[39m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1850\u001b[0m \u001b[39mif\u001b[39;00m (possible_gradient_type \u001b[39m==\u001b[39m gradients_util\u001b[39m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1851\u001b[0m     \u001b[39mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1852\u001b[0m   \u001b[39m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1853\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_call_outputs(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_inference_function\u001b[39m.\u001b[39;49mcall(\n\u001b[1;32m   1854\u001b[0m       ctx, args, cancellation_manager\u001b[39m=\u001b[39;49mcancellation_manager))\n\u001b[1;32m   1855\u001b[0m forward_backward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m     args,\n\u001b[1;32m   1857\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1858\u001b[0m     executing_eagerly)\n\u001b[1;32m   1859\u001b[0m forward_function, args_with_tangents \u001b[39m=\u001b[39m forward_backward\u001b[39m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/function.py:499\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[39mwith\u001b[39;00m _InterpolateFunctionError(\u001b[39mself\u001b[39m):\n\u001b[1;32m    498\u001b[0m   \u001b[39mif\u001b[39;00m cancellation_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 499\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39;49mexecute(\n\u001b[1;32m    500\u001b[0m         \u001b[39mstr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msignature\u001b[39m.\u001b[39;49mname),\n\u001b[1;32m    501\u001b[0m         num_outputs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_num_outputs,\n\u001b[1;32m    502\u001b[0m         inputs\u001b[39m=\u001b[39;49margs,\n\u001b[1;32m    503\u001b[0m         attrs\u001b[39m=\u001b[39;49mattrs,\n\u001b[1;32m    504\u001b[0m         ctx\u001b[39m=\u001b[39;49mctx)\n\u001b[1;32m    505\u001b[0m   \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    506\u001b[0m     outputs \u001b[39m=\u001b[39m execute\u001b[39m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    507\u001b[0m         \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39msignature\u001b[39m.\u001b[39mname),\n\u001b[1;32m    508\u001b[0m         num_outputs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    511\u001b[0m         ctx\u001b[39m=\u001b[39mctx,\n\u001b[1;32m    512\u001b[0m         cancellation_manager\u001b[39m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/python/eager/execute.py:54\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m   ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 54\u001b[0m   tensors \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_Execute(ctx\u001b[39m.\u001b[39;49m_handle, device_name, op_name,\n\u001b[1;32m     55\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     56\u001b[0m \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m     57\u001b[0m   \u001b[39mif\u001b[39;00m name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = image_classifier.create(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - 1s 45ms/step - loss: 0.2601 - accuracy: 0.9692\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-26 11:42:43.804419: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpml5_ih03/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpml5_ih03/assets\n",
      "2023-05-26 11:42:46.840100: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.840210: I tensorflow/core/grappler/devices.cc:66] Number of eligible GPUs (core count >= 8, compute capability >= 0.0): 1\n",
      "2023-05-26 11:42:46.840264: I tensorflow/core/grappler/clusters/single_machine.cc:358] Starting new session\n",
      "2023-05-26 11:42:46.840616: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.840728: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.840827: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.840959: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.841063: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:936] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2023-05-26 11:42:46.841132: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6652 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2080 SUPER, pci bus id: 0000:03:00.0, compute capability: 7.5\n",
      "2023-05-26 11:42:46.865176: I tensorflow/core/grappler/optimizers/meta_optimizer.cc:1164] Optimization results for grappler item: graph_to_optimize\n",
      "  function_optimizer: Graph size after: 913 nodes (656), 923 edges (664), time = 13.874ms.\n",
      "  function_optimizer: function_optimizer did nothing. time = 0.008ms.\n",
      "\n",
      "/home/jessica/VIA-Fall-Detection/.venv/lib/python3.8/site-packages/tensorflow/lite/python/convert.py:746: UserWarning: Statistics for quantized inputs were expected, but not specified; continuing anyway.\n",
      "  warnings.warn(\"Statistics for quantized inputs were expected, but not \"\n",
      "2023-05-26 11:42:47.401652: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:357] Ignored output_format.\n",
      "2023-05-26 11:42:47.401681: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:360] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fully_quantize: 0, inference_type: 6, input_inference_type: 3, output_inference_type: 3\n",
      "INFO:tensorflow:Label file is inside the TFLite model with metadata.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmpjhnwg859/labels.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saving labels in /tmp/tmpjhnwg859/labels.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:TensorFlow Lite model exported successfully: ./model.tflite\n"
     ]
    }
   ],
   "source": [
    "model.export(export_dir='.')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_video(input_path, output_path, width, height):\n",
    "  # Open the video file\n",
    "  video = cv2.VideoCapture(input_path)\n",
    "\n",
    "  # Get the original video's width and height\n",
    "  original_width = int(video.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "  original_height = int(video.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "  \n",
    "  # Create a VideoWriter object to save the resized video\n",
    "  fourcc = cv2.VideoWriter_fourcc(*'mp4v') # Codec for the output video\n",
    "  fps = video.get(cv2.CAP_PROP_FPS)\n",
    "  writer = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
    "  \n",
    "  while True:\n",
    "    # Read a frame from the original video\n",
    "    ret, frame = video.read()\n",
    "    if not ret: break\n",
    "    # Resize the frame to the desired width and height\n",
    "    resized_frame = cv2.resize(frame, (width, height))\n",
    "    # Write the resized frame to the output video file\n",
    "    writer.write(resized_frame)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "  video.release()\n",
    "  writer.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.0\n"
     ]
    }
   ],
   "source": [
    "# Resize vid\n",
    "video_path = \"./datasets/vid9.mp4\"\n",
    "output_path = \"resized_vid9.mp4\"\n",
    "target_width = 224\n",
    "target_height = 224\n",
    "\n",
    "# resize_video(video_path, output_path, target_width, target_height)\n",
    "\n",
    "# Load resized vid\n",
    "cap = cv2.VideoCapture(output_path)\n",
    "# frame_rate = 30\n",
    "# cap.set(cv2.CAP_PROP_FPS, frame_rate) DOESN'T WORK\n",
    "print(cap.get(cv2.CAP_PROP_FPS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store the frame classifications\n",
    "frame_classifications = []\n",
    "\n",
    "# Loop through the frames of the video (need to change to 30 fps)\n",
    "while True:\n",
    "    ret, frame = cap.read() \n",
    "    #just need to figure out if this is 30 fps\n",
    "\n",
    "    if not ret: # Break the loop if the video has ended\n",
    "        break\n",
    "\n",
    "    ''' Formulate Input Data (frame_rgb) '''\n",
    "    # Convert the frame to RGB format\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    # Make frame input data and ensure its type matches the model\n",
    "    frame_rgb = np.expand_dims(frame_rgb, axis=0)\n",
    "\n",
    "    ''' Classify the Frame '''\n",
    "    interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "    interpreter.allocate_tensors()\n",
    "\n",
    "    # get_output_details() and get_input_details() return list of dictionaries of tensor details\n",
    "    # keys: name, index, shape, shape_signature, dtype, quantization, ...\n",
    "    # len(input) = len(output) = 1, so access the first element\n",
    "    output = interpreter.get_output_details()\n",
    "    input = interpreter.get_input_details()\n",
    "    output_index = output[0]['index']\n",
    "    input_index = input[0]['index']\n",
    "\n",
    "    # set input -> invoke -> access output\n",
    "    interpreter.set_tensor(input_index, frame_rgb)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_index)\n",
    "    # If the output_data shape is (batch_size, num_classes), select the first frame\n",
    "    output_data = output_data[0]\n",
    "\n",
    "    # Convert each entry into probability\n",
    "    output_probs = tf.nn.softmax(output_data.astype(float))\n",
    "\n",
    "    # Find the index of the highest probability\n",
    "    predicted_index = np.argmax(output_data)\n",
    "\n",
    "    # Assuming you have a list of class labels corresponding to the model's output classes\n",
    "    class_labels = [\"Moving\", \"Still\"]\n",
    "\n",
    "    # Get the predicted class label\n",
    "    predicted_class = class_labels[predicted_index]\n",
    "\n",
    "    # Print the predicted class label\n",
    "    # print(\"Predicted Class:\", predicted_class)\n",
    "    frame_classifications.append((predicted_class, max(output_probs.numpy())))\n",
    "    '''\n",
    "    print(\"output: \")\n",
    "    print(output_data)\n",
    "\n",
    "    # Display the frame\n",
    "    cv2.imshow(\"Video\", frame)\n",
    "\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frame Classifications: [('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Moving', 0.9820137900379085), ('Moving', 0.9820137900379085), ('Moving', 0.9999999999999873), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 0.5), ('Moving', 0.9999999997210531), ('Moving', 0.9999999999993086), ('Still', 0.9820137900379085), ('Moving', 0.9999998874648379), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Moving', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0), ('Still', 1.0)]\n"
     ]
    }
   ],
   "source": [
    "''' SHOW CLASSIFICATION RESULTS '''\n",
    "# Convert the frame classifications to a numpy array\n",
    "#frame_classifications = np.array(predicted_class)\n",
    "# Print the frame classifications\n",
    "print(\"Frame Classifications:\", frame_classifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Release the video capture and close the OpenCV windows (Do we need this?) yes, i think thats why i took up all the resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
